{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SGRjaq1uu9U",
        "outputId": "5a5e280e-37da-43e7-e6de-f271737c5d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: accelerate==1.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: aiohappyeyeballs==2.4.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.4.3)\n",
            "Requirement already satisfied: aiohttp==3.11.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.11.2)\n",
            "Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: alabaster==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.0.19)\n",
            "Requirement already satisfied: albumentations==1.4.20 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.4.20)\n",
            "Requirement already satisfied: altair==4.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.2.2)\n",
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: anyio==3.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi==23.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (23.1.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (21.2.0)\n",
            "Requirement already satisfied: array_record==0.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.5.1)\n",
            "Requirement already satisfied: arviz==0.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.20.0)\n",
            "Requirement already satisfied: astropy==6.1.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (6.1.6)\n",
            "Requirement already satisfied: astropy-iers-data==0.2024.11.18.0.35.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (0.2024.11.18.0.35.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (1.6.3)\n",
            "Requirement already satisfied: async-timeout==4.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (4.0.3)\n",
            "Requirement already satisfied: atpublic==4.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (4.1.0)\n",
            "Requirement already satisfied: attrs==24.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (24.2.0)\n",
            "Requirement already satisfied: audioread==3.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (3.0.1)\n",
            "Requirement already satisfied: autograd==1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (1.7.0)\n",
            "Requirement already satisfied: babel==2.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (2.16.0)\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (0.2.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (4.12.3)\n",
            "Requirement already satisfied: bigframes==1.27.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (1.27.0)\n",
            "Requirement already satisfied: bigquery-magics==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.4.0)\n",
            "Requirement already satisfied: bleach==6.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (6.2.0)\n",
            "Requirement already satisfied: blinker==1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 30)) (1.9.0)\n",
            "Requirement already satisfied: blis==0.7.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 31)) (0.7.11)\n",
            "Requirement already satisfied: blosc2==2.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (2.7.1)\n",
            "Requirement already satisfied: bokeh==3.6.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 33)) (3.6.1)\n",
            "Requirement already satisfied: Bottleneck==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (1.4.2)\n",
            "Requirement already satisfied: bqplot==0.12.43 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (0.12.43)\n",
            "Requirement already satisfied: branca==0.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 36)) (0.8.0)\n",
            "Requirement already satisfied: CacheControl==0.14.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (0.14.1)\n",
            "Requirement already satisfied: cachetools==5.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (5.5.0)\n",
            "Requirement already satisfied: catalogue==2.0.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 39)) (2.0.10)\n",
            "Collecting catboost==1.2.7 (from -r requirements.txt (line 40))\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi==2024.8.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 41)) (2024.8.30)\n",
            "Requirement already satisfied: cffi==1.17.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (1.17.1)\n",
            "Requirement already satisfied: chardet==5.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 43)) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer==3.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 44)) (3.4.0)\n",
            "Requirement already satisfied: chex==0.1.87 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 45)) (0.1.87)\n",
            "Requirement already satisfied: clarabel==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 46)) (0.9.0)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 47)) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib==0.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 48)) (0.20.0)\n",
            "Requirement already satisfied: cloudpickle==3.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 49)) (3.1.0)\n",
            "Requirement already satisfied: cmake==3.30.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 50)) (3.30.5)\n",
            "Requirement already satisfied: cmdstanpy==1.2.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 51)) (1.2.4)\n",
            "Requirement already satisfied: colorcet==3.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 52)) (3.1.0)\n",
            "Requirement already satisfied: colorlover==0.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 53)) (0.3.0)\n",
            "Requirement already satisfied: colour==0.1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 54)) (0.1.5)\n",
            "Requirement already satisfied: community==1.0.0b1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 55)) (1.0.0b1)\n",
            "Requirement already satisfied: confection==0.1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 56)) (0.1.5)\n",
            "Requirement already satisfied: cons==0.4.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 57)) (0.4.6)\n",
            "Requirement already satisfied: contourpy==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 58)) (1.3.1)\n",
            "Requirement already satisfied: cryptography==43.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 59)) (43.0.3)\n",
            "Requirement already satisfied: cuda-python==12.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 60)) (12.2.1)\n",
            "Requirement already satisfied: cudf-cu12==24.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 61)) (24.10.1)\n",
            "Requirement already satisfied: cufflinks==0.17.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 62)) (0.17.3)\n",
            "Requirement already satisfied: cupy-cuda12x==12.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 63)) (12.2.0)\n",
            "Requirement already satisfied: cvxopt==1.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 64)) (1.3.2)\n",
            "Requirement already satisfied: cvxpy==1.5.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 65)) (1.5.4)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 66)) (0.12.1)\n",
            "Requirement already satisfied: cymem==2.0.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 67)) (2.0.8)\n",
            "Requirement already satisfied: Cython==3.0.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 68)) (3.0.11)\n",
            "Requirement already satisfied: dask==2024.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 69)) (2024.10.0)\n",
            "Collecting dask-expr==1.1.16 (from -r requirements.txt (line 70))\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: datascience==0.17.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 71)) (0.17.6)\n",
            "Requirement already satisfied: db-dtypes==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 72)) (1.3.1)\n",
            "Requirement already satisfied: dbus-python==1.2.18 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 73)) (1.2.18)\n",
            "Requirement already satisfied: debugpy==1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 74)) (1.8.0)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 75)) (4.4.2)\n",
            "Requirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 76)) (0.7.1)\n",
            "Requirement already satisfied: Deprecated==1.2.15 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 77)) (1.2.15)\n",
            "Requirement already satisfied: diffusers==0.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 78)) (0.31.0)\n",
            "Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 79)) (1.9.0)\n",
            "Requirement already satisfied: dlib==19.24.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 80)) (19.24.2)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 81)) (0.1.8)\n",
            "Requirement already satisfied: docker-pycreds==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 82)) (0.4.0)\n",
            "Requirement already satisfied: docstring_parser==0.16 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 83)) (0.16)\n",
            "Requirement already satisfied: docutils==0.21.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 84)) (0.21.2)\n",
            "Requirement already satisfied: dopamine_rl==4.0.9 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 85)) (4.0.9)\n",
            "Requirement already satisfied: duckdb==1.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 86)) (1.1.3)\n",
            "Requirement already satisfied: earthengine-api==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 87)) (1.2.0)\n",
            "Requirement already satisfied: easydict==1.13 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 88)) (1.13)\n",
            "Requirement already satisfied: ecos==2.0.14 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 89)) (2.0.14)\n",
            "Requirement already satisfied: editdistance==0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 90)) (0.8.1)\n",
            "Requirement already satisfied: eerepr==0.0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 91)) (0.0.4)\n",
            "Requirement already satisfied: einops==0.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 92)) (0.8.0)\n",
            "Requirement already satisfied: en-core-web-sm==3.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 93)) (3.7.1)\n",
            "Requirement already satisfied: entrypoints==0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 94)) (0.4)\n",
            "Requirement already satisfied: et_xmlfile==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 95)) (2.0.0)\n",
            "Requirement already satisfied: etils==1.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 96)) (1.10.0)\n",
            "Requirement already satisfied: etuples==0.3.9 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 97)) (0.3.9)\n",
            "Requirement already satisfied: eval_type_backport==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 98)) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 99)) (1.2.2)\n",
            "Requirement already satisfied: fastai==2.7.18 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 100)) (2.7.18)\n",
            "Requirement already satisfied: fastcore==1.7.20 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 101)) (1.7.20)\n",
            "Requirement already satisfied: fastdownload==0.0.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 102)) (0.0.7)\n",
            "Requirement already satisfied: fastjsonschema==2.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 103)) (2.20.0)\n",
            "Requirement already satisfied: fastprogress==1.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 104)) (1.0.3)\n",
            "Requirement already satisfied: fastrlock==0.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 105)) (0.8.2)\n",
            "Requirement already satisfied: filelock==3.16.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 106)) (3.16.1)\n",
            "Requirement already satisfied: firebase-admin==6.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 107)) (6.5.0)\n",
            "Requirement already satisfied: Flask==3.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 108)) (3.0.3)\n",
            "Requirement already satisfied: flatbuffers==24.3.25 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 109)) (24.3.25)\n",
            "Requirement already satisfied: flax==0.8.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 110)) (0.8.5)\n",
            "Requirement already satisfied: folium==0.18.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 111)) (0.18.0)\n",
            "Requirement already satisfied: fonttools==4.55.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 112)) (4.55.0)\n",
            "Requirement already satisfied: frozendict==2.4.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 113)) (2.4.6)\n",
            "Requirement already satisfied: frozenlist==1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 114)) (1.5.0)\n",
            "Requirement already satisfied: fsspec==2024.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 115)) (2024.10.0)\n",
            "Requirement already satisfied: future==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 116)) (1.0.0)\n",
            "Requirement already satisfied: gast==0.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 117)) (0.6.0)\n",
            "Requirement already satisfied: gcsfs==2024.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 118)) (2024.10.0)\n",
            "Requirement already satisfied: GDAL==3.6.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 119)) (3.6.4)\n",
            "Requirement already satisfied: gdown==5.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 120)) (5.2.0)\n",
            "Requirement already satisfied: geemap==0.35.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 121)) (0.35.1)\n",
            "Requirement already satisfied: gensim==4.3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 122)) (4.3.3)\n",
            "Requirement already satisfied: geocoder==1.38.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 123)) (1.38.1)\n",
            "Requirement already satisfied: geographiclib==2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 124)) (2.0)\n",
            "Requirement already satisfied: geopandas==1.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 125)) (1.0.1)\n",
            "Requirement already satisfied: geopy==2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 126)) (2.4.1)\n",
            "Requirement already satisfied: gin-config==0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 127)) (0.5.0)\n",
            "Requirement already satisfied: gitdb==4.0.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 128)) (4.0.11)\n",
            "Requirement already satisfied: GitPython==3.1.43 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 129)) (3.1.43)\n",
            "Requirement already satisfied: glob2==0.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 130)) (0.7)\n",
            "Requirement already satisfied: google==2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 131)) (2.0.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 132)) (0.6.10)\n",
            "Requirement already satisfied: google-api-core==2.19.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 133)) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client==2.151.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 134)) (2.151.0)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 135)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 136)) (0.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib==1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 137)) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-aiplatform==1.71.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 138)) (1.71.1)\n",
            "Requirement already satisfied: google-cloud-bigquery==3.25.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 139)) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-connection==1.16.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 140)) (1.16.1)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage==2.27.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 141)) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-bigtable==2.27.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 142)) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-core==2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 143)) (2.4.1)\n",
            "Requirement already satisfied: google-cloud-datastore==2.20.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 144)) (2.20.1)\n",
            "Requirement already satisfied: google-cloud-firestore==2.19.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 145)) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-functions==1.18.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 146)) (1.18.1)\n",
            "Requirement already satisfied: google-cloud-iam==2.16.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 147)) (2.16.1)\n",
            "Requirement already satisfied: google-cloud-language==2.15.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 148)) (2.15.1)\n",
            "Requirement already satisfied: google-cloud-pubsub==2.27.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 149)) (2.27.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 150)) (1.13.1)\n",
            "Requirement already satisfied: google-cloud-storage==2.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 151)) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-translate==3.17.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 152)) (3.17.0)\n",
            "Requirement already satisfied: google-colab==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 153)) (1.0.0)\n",
            "Requirement already satisfied: google-crc32c==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 154)) (1.6.0)\n",
            "Requirement already satisfied: google-generativeai==0.8.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 155)) (0.8.3)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 156)) (0.2.0)\n",
            "Requirement already satisfied: google-resumable-media==2.7.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 157)) (2.7.2)\n",
            "Requirement already satisfied: googleapis-common-protos==1.66.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 158)) (1.66.0)\n",
            "Requirement already satisfied: googledrivedownloader==0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 159)) (0.4)\n",
            "Requirement already satisfied: graphviz==0.20.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 160)) (0.20.3)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 161)) (3.1.1)\n",
            "Requirement already satisfied: grpc-google-iam-v1==0.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 162)) (0.13.1)\n",
            "Requirement already satisfied: grpcio==1.68.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 163)) (1.68.0)\n",
            "Requirement already satisfied: grpcio-status==1.62.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 164)) (1.62.3)\n",
            "Requirement already satisfied: gspread==6.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 165)) (6.0.2)\n",
            "Requirement already satisfied: gspread-dataframe==3.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 166)) (3.3.1)\n",
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 167)) (0.25.2)\n",
            "Requirement already satisfied: gym-notices==0.0.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 168)) (0.0.8)\n",
            "Requirement already satisfied: h11==0.14.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 169)) (0.14.0)\n",
            "Requirement already satisfied: h5netcdf==1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 170)) (1.4.1)\n",
            "Requirement already satisfied: h5py==3.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 171)) (3.12.1)\n",
            "Requirement already satisfied: holidays==0.61 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 172)) (0.61)\n",
            "Requirement already satisfied: holoviews==1.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 173)) (1.20.0)\n",
            "Requirement already satisfied: html5lib==1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 174)) (1.1)\n",
            "Requirement already satisfied: httpcore==1.0.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 175)) (1.0.7)\n",
            "Requirement already satisfied: httpimport==1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 176)) (1.4.0)\n",
            "Requirement already satisfied: httplib2==0.22.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 177)) (0.22.0)\n",
            "Requirement already satisfied: httpx==0.27.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 178)) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub==0.26.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 179)) (0.26.2)\n",
            "Requirement already satisfied: humanize==4.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 180)) (4.11.0)\n",
            "Requirement already satisfied: hyperopt==0.2.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 181)) (0.2.7)\n",
            "Requirement already satisfied: ibis-framework==9.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 182)) (9.2.0)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 183)) (3.10)\n",
            "Requirement already satisfied: imageio==2.36.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 184)) (2.36.0)\n",
            "Requirement already satisfied: imageio-ffmpeg==0.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 185)) (0.5.1)\n",
            "Requirement already satisfied: imagesize==1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 186)) (1.4.1)\n",
            "Requirement already satisfied: imbalanced-learn==0.12.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 187)) (0.12.4)\n",
            "Requirement already satisfied: imgaug==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 188)) (0.4.0)\n",
            "Requirement already satisfied: immutabledict==4.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 189)) (4.2.1)\n",
            "Requirement already satisfied: importlib_metadata==8.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 190)) (8.5.0)\n",
            "Requirement already satisfied: importlib_resources==6.4.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 191)) (6.4.5)\n",
            "Requirement already satisfied: imutils==0.5.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 192)) (0.5.4)\n",
            "Requirement already satisfied: inflect==7.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 193)) (7.4.0)\n",
            "Requirement already satisfied: iniconfig==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 194)) (2.0.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2025.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 195)) (2025.0.0)\n",
            "Requirement already satisfied: intel-openmp==2025.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 196)) (2025.0.0)\n",
            "Requirement already satisfied: ipyevents==2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 197)) (2.0.2)\n",
            "Requirement already satisfied: ipyfilechooser==0.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 198)) (0.6.0)\n",
            "Requirement already satisfied: ipykernel==5.5.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 199)) (5.5.6)\n",
            "Requirement already satisfied: ipyleaflet==0.19.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 200)) (0.19.2)\n",
            "Requirement already satisfied: ipyparallel==8.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 201)) (8.8.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 202)) (7.34.0)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 203)) (0.2.0)\n",
            "Requirement already satisfied: ipython-sql==0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 204)) (0.5.0)\n",
            "Requirement already satisfied: ipytree==0.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 205)) (0.2.2)\n",
            "Requirement already satisfied: ipywidgets==7.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 206)) (7.7.1)\n",
            "Requirement already satisfied: itsdangerous==2.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 207)) (2.2.0)\n",
            "Requirement already satisfied: jax==0.4.33 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 208)) (0.4.33)\n",
            "Requirement already satisfied: jax-cuda12-pjrt==0.4.33 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 209)) (0.4.33)\n",
            "Requirement already satisfied: jax-cuda12-plugin==0.4.33 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 210)) (0.4.33)\n",
            "Requirement already satisfied: jaxlib==0.4.33 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 211)) (0.4.33)\n",
            "Requirement already satisfied: jeepney==0.7.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 212)) (0.7.1)\n",
            "Requirement already satisfied: jellyfish==1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 213)) (1.1.0)\n",
            "Requirement already satisfied: jieba==0.42.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 214)) (0.42.1)\n",
            "Requirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 215)) (3.1.4)\n",
            "Requirement already satisfied: jiter==0.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 216)) (0.7.1)\n",
            "Requirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 217)) (1.4.2)\n",
            "Requirement already satisfied: jsonpatch==1.33 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 218)) (1.33)\n",
            "Requirement already satisfied: jsonpickle==4.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 219)) (4.0.0)\n",
            "Requirement already satisfied: jsonpointer==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 220)) (3.0.0)\n",
            "Requirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 221)) (4.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications==2024.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 222)) (2024.10.1)\n",
            "Requirement already satisfied: jupyter-client==6.1.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 223)) (6.1.12)\n",
            "Requirement already satisfied: jupyter-console==6.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 224)) (6.1.0)\n",
            "Requirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 225)) (5.7.2)\n",
            "Requirement already satisfied: jupyter-leaflet==0.19.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 226)) (0.19.2)\n",
            "Requirement already satisfied: jupyter-server==1.24.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 227)) (1.24.0)\n",
            "Requirement already satisfied: jupyterlab_pygments==0.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 228)) (0.3.0)\n",
            "Requirement already satisfied: jupyterlab_widgets==3.0.13 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 229)) (3.0.13)\n",
            "Requirement already satisfied: kaggle==1.6.17 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 230)) (1.6.17)\n",
            "Requirement already satisfied: kagglehub==0.3.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 231)) (0.3.4)\n",
            "Requirement already satisfied: keras==3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 232)) (3.5.0)\n",
            "Requirement already satisfied: keyring==23.5.0 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 233)) (23.5.0)\n",
            "Requirement already satisfied: kiwisolver==1.4.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 234)) (1.4.7)\n",
            "Requirement already satisfied: langchain==0.3.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 235)) (0.3.7)\n",
            "Requirement already satisfied: langchain-core==0.3.19 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 236)) (0.3.19)\n",
            "Requirement already satisfied: langchain-text-splitters==0.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 237)) (0.3.2)\n",
            "Requirement already satisfied: langcodes==3.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 238)) (3.4.1)\n",
            "Requirement already satisfied: langsmith==0.1.143 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 239)) (0.1.143)\n",
            "Requirement already satisfied: language_data==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 240)) (1.2.0)\n",
            "Requirement already satisfied: launchpadlib==1.10.16 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 241)) (1.10.16)\n",
            "Requirement already satisfied: lazr.restfulclient==0.14.4 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 242)) (0.14.4)\n",
            "Requirement already satisfied: lazr.uri==1.0.6 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 243)) (1.0.6)\n",
            "Requirement already satisfied: lazy_loader==0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 244)) (0.4)\n",
            "Collecting liac-arff==2.5.0 (from -r requirements.txt (line 245))\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: libclang==18.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 246)) (18.1.1)\n",
            "Requirement already satisfied: libcudf-cu12==24.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 247)) (24.10.1)\n",
            "Requirement already satisfied: librosa==0.10.2.post1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 248)) (0.10.2.post1)\n",
            "Requirement already satisfied: lightgbm==4.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 249)) (4.5.0)\n",
            "Requirement already satisfied: linkify-it-py==2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 250)) (2.0.3)\n",
            "Requirement already satisfied: llvmlite==0.43.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 251)) (0.43.0)\n",
            "Requirement already satisfied: locket==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 252)) (1.0.0)\n",
            "Requirement already satisfied: logical-unification==0.4.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 253)) (0.4.6)\n",
            "Requirement already satisfied: lxml==5.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 254)) (5.3.0)\n",
            "Requirement already satisfied: marisa-trie==1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 255)) (1.2.1)\n",
            "Requirement already satisfied: Markdown==3.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 256)) (3.7)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 257)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 258)) (3.0.2)\n",
            "Requirement already satisfied: matplotlib==3.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 259)) (3.8.0)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 260)) (0.1.7)\n",
            "Requirement already satisfied: matplotlib-venn==1.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 261)) (1.1.1)\n",
            "Requirement already satisfied: mdit-py-plugins==0.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 262)) (0.4.2)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 263)) (0.1.2)\n",
            "Requirement already satisfied: miniKanren==1.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 264)) (1.0.3)\n",
            "Collecting minio==7.2.12 (from -r requirements.txt (line 265))\n",
            "  Downloading minio-7.2.12-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: missingno==0.5.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 266)) (0.5.2)\n",
            "Requirement already satisfied: mistune==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 267)) (3.0.2)\n",
            "Requirement already satisfied: mizani==0.13.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 268)) (0.13.0)\n",
            "Requirement already satisfied: mkl==2025.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 269)) (2025.0.0)\n",
            "Requirement already satisfied: ml-dtypes==0.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 270)) (0.4.1)\n",
            "Requirement already satisfied: mlxtend==0.23.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 271)) (0.23.3)\n",
            "Requirement already satisfied: more-itertools==10.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 272)) (10.5.0)\n",
            "Requirement already satisfied: moviepy==1.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 273)) (1.0.3)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 274)) (1.3.0)\n",
            "Requirement already satisfied: msgpack==1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 275)) (1.1.0)\n",
            "Requirement already satisfied: multidict==6.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 276)) (6.1.0)\n",
            "Requirement already satisfied: multipledispatch==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 277)) (1.0.0)\n",
            "Requirement already satisfied: multitasking==0.0.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 278)) (0.0.11)\n",
            "Requirement already satisfied: murmurhash==1.0.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 279)) (1.0.10)\n",
            "Requirement already satisfied: music21==9.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 280)) (9.3.0)\n",
            "Requirement already satisfied: namex==0.0.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 281)) (0.0.8)\n",
            "Requirement already satisfied: natsort==8.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 282)) (8.4.0)\n",
            "Requirement already satisfied: nbclassic==1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 283)) (1.1.0)\n",
            "Requirement already satisfied: nbclient==0.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 284)) (0.10.0)\n",
            "Requirement already satisfied: nbconvert==7.16.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 285)) (7.16.4)\n",
            "Requirement already satisfied: nbformat==5.10.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 286)) (5.10.4)\n",
            "Requirement already satisfied: ndindex==1.9.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 287)) (1.9.2)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 288)) (1.6.0)\n",
            "Requirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 289)) (3.4.2)\n",
            "Requirement already satisfied: nibabel==5.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 290)) (5.3.2)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 291)) (3.9.1)\n",
            "Requirement already satisfied: notebook==6.5.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 292)) (6.5.5)\n",
            "Requirement already satisfied: notebook_shim==0.2.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 293)) (0.2.4)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 294)) (0.60.0)\n",
            "Requirement already satisfied: numexpr==2.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 295)) (2.10.1)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 296)) (1.26.4)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 297)) (12.6.3.3)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 298)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 299)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 300)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 301)) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 302)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 303)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 304)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 305)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.23.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 306)) (2.23.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 307)) (12.6.77)\n",
            "Requirement already satisfied: nvtx==0.2.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 308)) (0.2.10)\n",
            "Requirement already satisfied: nx-cugraph-cu12==24.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 309)) (24.10.0)\n",
            "Requirement already satisfied: oauth2client==4.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 310)) (4.1.3)\n",
            "Requirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 311)) (3.2.2)\n",
            "Requirement already satisfied: openai==1.54.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 312)) (1.54.4)\n",
            "Requirement already satisfied: opencv-contrib-python==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 313)) (4.10.0.84)\n",
            "Requirement already satisfied: opencv-python==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 314)) (4.10.0.84)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 315)) (4.10.0.84)\n",
            "Collecting openml==0.15.0 (from -r requirements.txt (line 316))\n",
            "  Downloading openml-0.15.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: openpyxl==3.1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 317)) (3.1.5)\n",
            "Requirement already satisfied: opentelemetry-api==1.28.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 318)) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-sdk==1.28.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 319)) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 320)) (0.49b2)\n",
            "Requirement already satisfied: opt_einsum==3.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 321)) (3.4.0)\n",
            "Requirement already satisfied: optax==0.2.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 322)) (0.2.4)\n",
            "Requirement already satisfied: optree==0.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 323)) (0.13.1)\n",
            "Requirement already satisfied: orbax-checkpoint==0.6.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 324)) (0.6.4)\n",
            "Requirement already satisfied: orjson==3.10.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 325)) (3.10.11)\n",
            "Requirement already satisfied: osqp==0.6.7.post3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 326)) (0.6.7.post3)\n",
            "Requirement already satisfied: packaging==24.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 327)) (24.2)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 328)) (2.2.2)\n",
            "Requirement already satisfied: pandas-datareader==0.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 329)) (0.10.0)\n",
            "Requirement already satisfied: pandas-gbq==0.24.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 330)) (0.24.0)\n",
            "Requirement already satisfied: pandas-stubs==2.2.2.240909 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 331)) (2.2.2.240909)\n",
            "Requirement already satisfied: pandocfilters==1.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 332)) (1.5.1)\n",
            "Requirement already satisfied: panel==1.5.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 333)) (1.5.4)\n",
            "Requirement already satisfied: param==2.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 334)) (2.1.1)\n",
            "Requirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 335)) (0.8.4)\n",
            "Requirement already satisfied: parsy==2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 336)) (2.1)\n",
            "Requirement already satisfied: partd==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 337)) (1.4.2)\n",
            "Requirement already satisfied: pathlib==1.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 338)) (1.0.1)\n",
            "Requirement already satisfied: patsy==1.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 339)) (1.0.1)\n",
            "Requirement already satisfied: peewee==3.17.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 340)) (3.17.8)\n",
            "Requirement already satisfied: peft==0.13.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 341)) (0.13.2)\n",
            "Requirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 342)) (4.9.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 343)) (0.7.5)\n",
            "Requirement already satisfied: pillow==11.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 344)) (11.0.0)\n",
            "Requirement already satisfied: pip==24.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 345)) (24.1.2)\n",
            "Requirement already satisfied: platformdirs==4.3.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 346)) (4.3.6)\n",
            "Requirement already satisfied: plotly==5.24.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 347)) (5.24.1)\n",
            "Requirement already satisfied: plotnine==0.14.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 348)) (0.14.1)\n",
            "Requirement already satisfied: pluggy==1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 349)) (1.5.0)\n",
            "Requirement already satisfied: polars==1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 350)) (1.9.0)\n",
            "Requirement already satisfied: pooch==1.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 351)) (1.8.2)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 352)) (1.5.2)\n",
            "Requirement already satisfied: preshed==3.0.9 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 353)) (3.0.9)\n",
            "Requirement already satisfied: prettytable==3.12.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 354)) (3.12.0)\n",
            "Requirement already satisfied: proglog==0.1.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 355)) (0.1.10)\n",
            "Requirement already satisfied: progressbar2==4.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 356)) (4.5.0)\n",
            "Requirement already satisfied: prometheus_client==0.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 357)) (0.21.0)\n",
            "Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 358)) (2.3)\n",
            "Requirement already satisfied: prompt_toolkit==3.0.48 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 359)) (3.0.48)\n",
            "Requirement already satisfied: propcache==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 360)) (0.2.0)\n",
            "Requirement already satisfied: prophet==1.1.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 361)) (1.1.6)\n",
            "Requirement already satisfied: proto-plus==1.25.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 362)) (1.25.0)\n",
            "Requirement already satisfied: protobuf==4.25.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 363)) (4.25.5)\n",
            "Requirement already satisfied: psutil==5.9.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 364)) (5.9.5)\n",
            "Requirement already satisfied: psycopg2==2.9.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 365)) (2.9.10)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 366)) (0.7.0)\n",
            "Requirement already satisfied: py-cpuinfo==9.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 367)) (9.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 368)) (0.10.9.7)\n",
            "Requirement already satisfied: pyarrow==17.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 369)) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix==0.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 370)) (0.6)\n",
            "Requirement already satisfied: pyasn1==0.6.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 371)) (0.6.1)\n",
            "Requirement already satisfied: pyasn1_modules==0.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 372)) (0.4.1)\n",
            "Requirement already satisfied: pycocotools==2.0.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 373)) (2.0.8)\n",
            "Requirement already satisfied: pycparser==2.22 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 374)) (2.22)\n",
            "Collecting pycryptodome==3.21.0 (from -r requirements.txt (line 375))\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pydantic==2.9.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 376)) (2.9.2)\n",
            "Requirement already satisfied: pydantic_core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 377)) (2.23.4)\n",
            "Requirement already satisfied: pydata-google-auth==1.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 378)) (1.8.2)\n",
            "Requirement already satisfied: pydot==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 379)) (3.0.2)\n",
            "Requirement already satisfied: pydotplus==2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 380)) (2.0.2)\n",
            "Requirement already satisfied: PyDrive==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 381)) (1.3.1)\n",
            "Requirement already satisfied: PyDrive2==1.21.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 382)) (1.21.1)\n",
            "Requirement already satisfied: pyerfa==2.0.1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 383)) (2.0.1.5)\n",
            "Requirement already satisfied: pygame==2.6.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 384)) (2.6.1)\n",
            "Requirement already satisfied: pygit2==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 385)) (1.16.0)\n",
            "Requirement already satisfied: Pygments==2.18.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 386)) (2.18.0)\n",
            "Requirement already satisfied: PyGObject==3.42.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 387)) (3.42.1)\n",
            "Requirement already satisfied: PyJWT==2.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 388)) (2.10.0)\n",
            "Requirement already satisfied: pylibcudf-cu12==24.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 389)) (24.10.1)\n",
            "Requirement already satisfied: pylibcugraph-cu12==24.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 390)) (24.10.0)\n",
            "Requirement already satisfied: pylibraft-cu12==24.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 391)) (24.10.0)\n",
            "Requirement already satisfied: pymc==5.18.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 392)) (5.18.2)\n",
            "Requirement already satisfied: pymystem3==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 393)) (0.2.0)\n",
            "Requirement already satisfied: pynvjitlink-cu12==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 394)) (0.4.0)\n",
            "Requirement already satisfied: pyogrio==0.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 395)) (0.10.0)\n",
            "Requirement already satisfied: PyOpenGL==3.1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 396)) (3.1.7)\n",
            "Requirement already satisfied: pyOpenSSL==24.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 397)) (24.2.1)\n",
            "Requirement already satisfied: pyparsing==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 398)) (3.2.0)\n",
            "Requirement already satisfied: pyperclip==1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 399)) (1.9.0)\n",
            "Requirement already satisfied: pyproj==3.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 400)) (3.7.0)\n",
            "Requirement already satisfied: pyshp==2.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 401)) (2.3.1)\n",
            "Requirement already satisfied: PySocks==1.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 402)) (1.7.1)\n",
            "Requirement already satisfied: pyspark==3.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 403)) (3.5.3)\n",
            "Requirement already satisfied: pytensor==2.26.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 404)) (2.26.3)\n",
            "Requirement already satisfied: pytest==8.3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 405)) (8.3.3)\n",
            "Requirement already satisfied: python-apt==0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 406)) (0.0.0)\n",
            "Requirement already satisfied: python-box==7.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 407)) (7.2.0)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 408)) (2.8.2)\n",
            "Requirement already satisfied: python-louvain==0.16 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 409)) (0.16)\n",
            "Requirement already satisfied: python-slugify==8.0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 410)) (8.0.4)\n",
            "Requirement already satisfied: python-utils==3.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 411)) (3.9.0)\n",
            "Collecting pytorch-ranger==0.1.1 (from -r requirements.txt (line 412))\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Collecting pytorch-tabnet==4.1.0 (from -r requirements.txt (line 413))\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pytz==2024.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 414)) (2024.2)\n",
            "Requirement already satisfied: pyviz_comms==3.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 415)) (3.0.3)\n",
            "Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 416)) (6.0.2)\n",
            "Requirement already satisfied: pyzmq==24.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 417)) (24.0.1)\n",
            "Requirement already satisfied: qdldl==0.1.7.post4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 418)) (0.1.7.post4)\n",
            "Requirement already satisfied: ratelim==0.1.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 419)) (0.1.6)\n",
            "Requirement already satisfied: referencing==0.35.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 420)) (0.35.1)\n",
            "Requirement already satisfied: regex==2024.9.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 421)) (2024.9.11)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 422)) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 423)) (1.3.1)\n",
            "Requirement already satisfied: requests-toolbelt==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 424)) (1.0.0)\n",
            "Requirement already satisfied: requirements-parser==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 425)) (0.9.0)\n",
            "Requirement already satisfied: rich==13.9.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 426)) (13.9.4)\n",
            "Requirement already satisfied: rmm-cu12==24.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 427)) (24.10.0)\n",
            "Requirement already satisfied: rpds-py==0.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 428)) (0.21.0)\n",
            "Requirement already satisfied: rpy2==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 429)) (3.4.2)\n",
            "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 430)) (4.9)\n",
            "Requirement already satisfied: safetensors==0.4.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 431)) (0.4.5)\n",
            "Requirement already satisfied: scikit-image==0.24.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 432)) (0.24.0)\n",
            "Requirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 433)) (1.5.2)\n",
            "Requirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 434)) (1.13.1)\n",
            "Requirement already satisfied: scooby==0.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 435)) (0.10.0)\n",
            "Requirement already satisfied: scs==3.2.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 436)) (3.2.7)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 437)) (0.13.2)\n",
            "Requirement already satisfied: SecretStorage==3.3.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 438)) (3.3.1)\n",
            "Requirement already satisfied: Send2Trash==1.8.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 439)) (1.8.3)\n",
            "Requirement already satisfied: sentence-transformers==3.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 440)) (3.2.1)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 441)) (0.2.0)\n",
            "Requirement already satisfied: sentry-sdk==2.18.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 442)) (2.18.0)\n",
            "Requirement already satisfied: setproctitle==1.3.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 443)) (1.3.4)\n",
            "Requirement already satisfied: setuptools==75.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 444)) (75.1.0)\n",
            "Requirement already satisfied: shap==0.46.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 445)) (0.46.0)\n",
            "Requirement already satisfied: shapely==2.0.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 446)) (2.0.6)\n",
            "Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 447)) (1.5.4)\n",
            "Requirement already satisfied: simple-parsing==0.1.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 448)) (0.1.6)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 449)) (1.16.0)\n",
            "Requirement already satisfied: sklearn-pandas==2.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 450)) (2.2.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 451)) (0.0.8)\n",
            "Requirement already satisfied: smart-open==7.0.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 452)) (7.0.5)\n",
            "Requirement already satisfied: smmap==5.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 453)) (5.0.1)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 454)) (1.3.1)\n",
            "Requirement already satisfied: snowballstemmer==2.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 455)) (2.2.0)\n",
            "Requirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 456)) (0.12.1)\n",
            "Requirement already satisfied: soupsieve==2.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 457)) (2.6)\n",
            "Requirement already satisfied: soxr==0.5.0.post1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 458)) (0.5.0.post1)\n",
            "Requirement already satisfied: spacy==3.7.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 459)) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy==3.0.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 460)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers==1.0.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 461)) (1.0.5)\n",
            "Requirement already satisfied: Sphinx==8.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 462)) (8.1.3)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 463)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 464)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp==2.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 465)) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath==1.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 466)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 467)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 468)) (2.0.0)\n",
            "Requirement already satisfied: SQLAlchemy==2.0.36 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 469)) (2.0.36)\n",
            "Requirement already satisfied: sqlglot==25.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 470)) (25.1.0)\n",
            "Requirement already satisfied: sqlparse==0.5.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 471)) (0.5.2)\n",
            "Requirement already satisfied: srsly==2.4.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 472)) (2.4.8)\n",
            "Requirement already satisfied: stanio==0.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 473)) (0.5.1)\n",
            "Requirement already satisfied: statsmodels==0.14.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 474)) (0.14.4)\n",
            "Requirement already satisfied: StrEnum==0.4.15 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 475)) (0.4.15)\n",
            "Requirement already satisfied: stringzilla==3.10.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 476)) (3.10.10)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 477)) (1.13.1)\n",
            "Requirement already satisfied: tables==3.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 478)) (3.10.1)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 479)) (0.9.0)\n",
            "Requirement already satisfied: tbb==2022.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 480)) (2022.0.0)\n",
            "Requirement already satisfied: tcmlib==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 481)) (1.2.0)\n",
            "Requirement already satisfied: tenacity==9.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 482)) (9.0.0)\n",
            "Requirement already satisfied: tensorboard==2.17.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 483)) (2.17.1)\n",
            "Requirement already satisfied: tensorboard-data-server==0.7.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 484)) (0.7.2)\n",
            "Requirement already satisfied: tensorflow==2.17.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 485)) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-datasets==4.9.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 486)) (4.9.7)\n",
            "Requirement already satisfied: tensorflow-hub==0.16.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 487)) (0.16.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.37.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 488)) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-metadata==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 489)) (1.13.1)\n",
            "Requirement already satisfied: tensorflow-probability==0.24.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 490)) (0.24.0)\n",
            "Requirement already satisfied: tensorstore==0.1.68 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 491)) (0.1.68)\n",
            "Requirement already satisfied: termcolor==2.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 492)) (2.5.0)\n",
            "Requirement already satisfied: terminado==0.18.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 493)) (0.18.1)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 494)) (1.3)\n",
            "Requirement already satisfied: textblob==0.17.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 495)) (0.17.1)\n",
            "Requirement already satisfied: tf_keras==2.17.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 496)) (2.17.0)\n",
            "Requirement already satisfied: tf-slim==1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 497)) (1.1.0)\n",
            "Requirement already satisfied: thinc==8.2.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 498)) (8.2.5)\n",
            "Requirement already satisfied: threadpoolctl==3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 499)) (3.5.0)\n",
            "Requirement already satisfied: tifffile==2024.9.20 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 500)) (2024.9.20)\n",
            "Requirement already satisfied: timm==1.0.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 501)) (1.0.11)\n",
            "Requirement already satisfied: tinycss2==1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 502)) (1.4.0)\n",
            "Requirement already satisfied: tokenizers==0.20.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 503)) (0.20.3)\n",
            "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 504)) (0.10.2)\n",
            "Requirement already satisfied: tomli==2.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 505)) (2.1.0)\n",
            "Requirement already satisfied: toolz==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 506)) (0.12.1)\n",
            "Requirement already satisfied: torch==2.5.1+cu121 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 507)) (2.5.1+cu121)\n",
            "Collecting torch-optimizer==0.3.0 (from -r requirements.txt (line 508))\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio==2.5.1+cu121 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 509)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 510)) (1.5.1)\n",
            "Requirement already satisfied: torchvision==0.20.1+cu121 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 511)) (0.20.1+cu121)\n",
            "Requirement already satisfied: tornado==6.3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 512)) (6.3.3)\n",
            "Requirement already satisfied: tqdm==4.66.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 513)) (4.66.6)\n",
            "Requirement already satisfied: traitlets==5.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 514)) (5.7.1)\n",
            "Requirement already satisfied: traittypes==0.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 515)) (0.2.1)\n",
            "Requirement already satisfied: transformers==4.46.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 516)) (4.46.2)\n",
            "Requirement already satisfied: tweepy==4.14.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 517)) (4.14.0)\n",
            "Requirement already satisfied: typeguard==4.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 518)) (4.4.1)\n",
            "Requirement already satisfied: typer==0.13.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 519)) (0.13.0)\n",
            "Requirement already satisfied: types-pytz==2024.2.0.20241003 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 520)) (2024.2.0.20241003)\n",
            "Requirement already satisfied: types-setuptools==75.6.0.20241126 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 521)) (75.6.0.20241126)\n",
            "Requirement already satisfied: typing_extensions==4.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 522)) (4.12.2)\n",
            "Requirement already satisfied: tzdata==2024.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 523)) (2024.2)\n",
            "Requirement already satisfied: tzlocal==5.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 524)) (5.2)\n",
            "Requirement already satisfied: uc-micro-py==1.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 525)) (1.0.3)\n",
            "Requirement already satisfied: umf==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 526)) (0.9.0)\n",
            "Requirement already satisfied: uritemplate==4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 527)) (4.1.1)\n",
            "Requirement already satisfied: urllib3==2.2.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 528)) (2.2.3)\n",
            "Requirement already satisfied: vega-datasets==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 529)) (0.9.0)\n",
            "Requirement already satisfied: wadllib==1.3.6 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 530)) (1.3.6)\n",
            "Requirement already satisfied: wandb==0.18.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 531)) (0.18.7)\n",
            "Requirement already satisfied: wasabi==1.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 532)) (1.1.3)\n",
            "Requirement already satisfied: wcwidth==0.2.13 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 533)) (0.2.13)\n",
            "Requirement already satisfied: weasel==0.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 534)) (0.4.1)\n",
            "Requirement already satisfied: webcolors==24.11.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 535)) (24.11.1)\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 536)) (0.5.1)\n",
            "Requirement already satisfied: websocket-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 537)) (1.8.0)\n",
            "Requirement already satisfied: Werkzeug==3.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 538)) (3.1.3)\n",
            "Requirement already satisfied: wheel==0.45.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 539)) (0.45.0)\n",
            "Requirement already satisfied: widgetsnbextension==3.6.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 540)) (3.6.10)\n",
            "Requirement already satisfied: wordcloud==1.9.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 541)) (1.9.4)\n",
            "Requirement already satisfied: wrapt==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 542)) (1.16.0)\n",
            "Requirement already satisfied: xarray==2024.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 543)) (2024.10.0)\n",
            "Requirement already satisfied: xarray-einstats==0.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 544)) (0.8.0)\n",
            "Requirement already satisfied: xgboost==2.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 545)) (2.1.2)\n",
            "Requirement already satisfied: xlrd==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 546)) (2.0.1)\n",
            "Collecting xmltodict==0.14.2 (from -r requirements.txt (line 547))\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: xyzservices==2024.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 548)) (2024.9.0)\n",
            "Requirement already satisfied: yarl==1.17.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 549)) (1.17.2)\n",
            "Requirement already satisfied: yellowbrick==1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 550)) (1.5)\n",
            "Requirement already satisfied: yfinance==0.2.49 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 551)) (0.2.49)\n",
            "Requirement already satisfied: zipp==3.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 552)) (3.21.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading minio-7.2.12-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m94.3/94.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openml-0.15.0-py3-none-any.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m158.0/158.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11716 sha256=7d9c54f41907fbfcc9ef17a6414a0ffafc46e424b6c26cbf6d53bbe5f0e980fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/2a/9c/3895d9617f8f49a0883ba686326d598e78a1c2f54fe3cae86d\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: catboost, xmltodict, torch-optimizer, pytorch-tabnet, pytorch-ranger, pycryptodome, openml, minio, liac-arff, dask-expr\n",
            "Successfully installed catboost-1.2.7 dask-expr-1.1.16 liac-arff-2.5.0 minio-7.2.12 openml-0.15.0 pycryptodome-3.21.0 pytorch-ranger-0.1.1 pytorch-tabnet-4.1.0 torch-optimizer-0.3.0 xmltodict-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8M5wO3_btHp"
      },
      "source": [
        "# Baselines: LR, KNN, SVM, DT, RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muSPURHdbBcZ",
        "outputId": "cc27caf9-376b-4850-f79b-18956ca52630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openml:No permission to create OpenML directory at /root/.config/openml! This can result in OpenML-Python not working properly.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "\n",
        "\n",
        "# Set OpenML configuration directory to a temporary location\n",
        "os.environ[\"OPENML_CONFIG\"] = \"/tmp/openml_config\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39x0zBfP4c-7"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U23wsqcIb8GT"
      },
      "outputs": [],
      "source": [
        "def load_preprocess_task(task_id, task_type=\"classification\", target_encode=None, cat_feature_encode=True):\n",
        "    \"\"\"\n",
        "    Load and preprocess dataset from OpenML based on task type (classification or regression).\n",
        "    Args:\n",
        "        task_id (int): OpenML task ID\n",
        "        task_type (str): Either 'classification' or 'regression'\n",
        "        target_encode (bool or None): Encode target if classification task (default: True if not regression)\n",
        "        cat_feature_encode (bool): Whether to one-hot encode categorical features (default: True)\n",
        "    Returns:\n",
        "        X_preprocessed (ndarray): Preprocessed feature set\n",
        "        y (ndarray): Target values (encoded for classification tasks)\n",
        "    \"\"\"\n",
        "    # Load task from OpenML using the task ID\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=task.target_name)\n",
        "\n",
        "    print(f\"Dataset shape: {X.shape}\")\n",
        "\n",
        "    # Infer and apply target encoding based on task type and target_encode flag\n",
        "    is_regression = (task_type == \"regression\")\n",
        "    if (target_encode is None and not is_regression) or target_encode:\n",
        "        le = LabelEncoder()\n",
        "        y = le.fit_transform(y)\n",
        "\n",
        "    # Detect and preprocess categorical features\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "    numeric_cols = X.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # Preprocessing pipelines for numeric and categorical features\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    if cat_feature_encode:\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_cols),\n",
        "                ('cat', categorical_transformer, categorical_cols)])\n",
        "    else:\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_cols)])\n",
        "\n",
        "    # Apply transformations\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    return X_preprocessed, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR9y_VVF4kAA"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZRdrbcEb-EE"
      },
      "outputs": [],
      "source": [
        "def cross_validate_model(model, X, y, task_type=\"classification\", n_folds=10):\n",
        "    \"\"\"\n",
        "    Cross-validate model based on task type (classification or regression).\n",
        "    Args:\n",
        "        model: Machine learning model to train and evaluate\n",
        "        X (ndarray): Feature matrix\n",
        "        y (ndarray): Target vector\n",
        "        task_type (str): 'classification' or 'regression'\n",
        "        n_folds (int): Number of cross-validation folds\n",
        "    Returns:\n",
        "        avg_score (float): Average cross-validated score\n",
        "    \"\"\"\n",
        "    if task_type == \"classification\":\n",
        "        cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "        scoring_func = accuracy_score\n",
        "    elif task_type == \"regression\":\n",
        "        cv = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "        scoring_func = mean_squared_error\n",
        "    else:\n",
        "        raise ValueError(\"Invalid task type. Use 'classification' or 'regression'.\")\n",
        "\n",
        "    scores = []\n",
        "    for train_index, test_index in cv.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy for classification or RMSE for regression\n",
        "        score = scoring_func(y_test, y_pred)\n",
        "        if task_type == \"regression\":\n",
        "            score = np.sqrt(score)  # RMSE\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    avg_score = np.mean(scores)\n",
        "    metric_name = \"Accuracy\" if task_type == \"classification\" else \"RMSE\"\n",
        "    print(f\"Average {metric_name}: {avg_score:.4f}\")\n",
        "\n",
        "    return avg_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8w8hT1u4oJJ"
      },
      "source": [
        "\n",
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV7NJw6mcF32"
      },
      "outputs": [],
      "source": [
        "def linear_model_trial(trial, task_type=\"classification\"):\n",
        "    if task_type == \"classification\":\n",
        "        model = LogisticRegression(n_jobs=-1)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Linear regression not implemented.\")\n",
        "    return model\n",
        "\n",
        "def run_linear_model(X, y, task_type=\"classification\"):\n",
        "    model = linear_model_trial(None, task_type)\n",
        "    scoring = 'accuracy' if task_type == \"classification\" else 'neg_root_mean_squared_error'\n",
        "    scores = cross_val_score(model, X, y, cv=10, scoring=scoring)\n",
        "    print(f\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5rP8tPf4ydh"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY2cwRyicHgi"
      },
      "outputs": [],
      "source": [
        "def get_random_knn_parameters(seed, task_type=\"classification\"):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"n_neighbors\": 1 + 2 * rs.randint(1, 21),\n",
        "        \"knn_alg\": rs.choice([\"kd_tree\", \"ball_tree\"]),\n",
        "        \"leaf_size\": rs.choice([30, 50, 70, 100, 300]),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def run_knn(X, y, seed=42, task_type=\"classification\"):\n",
        "    params = get_random_knn_parameters(seed, task_type)\n",
        "    if task_type == \"classification\":\n",
        "        knn = KNeighborsClassifier(n_neighbors=params[\"n_neighbors\"],\n",
        "                                   algorithm=params[\"knn_alg\"],\n",
        "                                   leaf_size=params[\"leaf_size\"],\n",
        "                                   n_jobs=-1)\n",
        "        scoring = 'accuracy'\n",
        "    else:\n",
        "        knn = KNeighborsRegressor(n_neighbors=params[\"n_neighbors\"],\n",
        "                                  algorithm=params[\"knn_alg\"],\n",
        "                                  leaf_size=params[\"leaf_size\"],\n",
        "                                  n_jobs=-1)\n",
        "        scoring = 'neg_root_mean_squared_error'\n",
        "\n",
        "    scores = cross_val_score(knn, X, y, cv=10, scoring=scoring)\n",
        "    print(f\"KNN with random params: {params}\")\n",
        "    print(f\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-7UPM-t41LM"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjOm2JU8cLA2"
      },
      "outputs": [],
      "source": [
        "def get_random_svm_parameters(seed: int):\n",
        "    # Generate random parameters for SVM\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\"C\": np.power(10, rs.uniform(-10, 10))}\n",
        "    return params\n",
        "\n",
        "\n",
        "def run_svm(X, y, seed=42, task_type=\"classification\"):\n",
        "    params = get_random_svm_parameters(seed)\n",
        "\n",
        "    if task_type == \"classification\":\n",
        "        svm_model = SVC(C=params[\"C\"], probability=True)\n",
        "        scoring = 'accuracy'\n",
        "    else:\n",
        "        svm_model = SVR(C=params[\"C\"])\n",
        "        scoring = 'neg_root_mean_squared_error'\n",
        "\n",
        "    scores = cross_val_score(svm_model, X, y, cv=10, scoring=scoring)\n",
        "    print(f\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZzSV4As45E-"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFDFObj-cPYk"
      },
      "outputs": [],
      "source": [
        "def get_random_decision_tree_parameters(seed, task_type=\"classification\"):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\"max_depth\": int(np.round(np.power(2, rs.uniform(1, np.log2(12)))))}\n",
        "    return params\n",
        "\n",
        "def run_decision_tree(X, y, seed=42, task_type=\"classification\"):\n",
        "    params = get_random_decision_tree_parameters(seed, task_type)\n",
        "    if task_type == \"classification\":\n",
        "        dt = DecisionTreeClassifier(max_depth=params[\"max_depth\"], random_state=seed)\n",
        "        scoring = 'accuracy'\n",
        "    else:\n",
        "        dt = DecisionTreeRegressor(max_depth=params[\"max_depth\"], random_state=seed)\n",
        "        scoring = 'neg_root_mean_squared_error'\n",
        "\n",
        "    scores = cross_val_score(dt, X, y, cv=10, scoring=scoring)\n",
        "    print(f\"Decision Tree with random params: {params}\")\n",
        "    print(f\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSJpAD2G5BLf"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0XA-ahecSkW"
      },
      "outputs": [],
      "source": [
        "def get_random_forest_parameters(seed, task_type=\"classification\"):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"max_depth\": int(np.round(np.power(2, rs.uniform(1, np.log2(12))))),\n",
        "        \"n_estimators\": int(np.round(np.power(5, rs.uniform(1, np.log2(100) / np.log2(5)))))\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def run_random_forest(X, y, seed=42, task_type=\"classification\"):\n",
        "    params = get_random_forest_parameters(seed, task_type)\n",
        "    if task_type == \"classification\":\n",
        "        rf = RandomForestClassifier(n_estimators=params[\"n_estimators\"],\n",
        "                                    max_depth=params[\"max_depth\"],\n",
        "                                    random_state=seed,\n",
        "                                    n_jobs=-1)\n",
        "        scoring = 'accuracy'\n",
        "    else:\n",
        "        rf = RandomForestRegressor(n_estimators=params[\"n_estimators\"],\n",
        "                                   max_depth=params[\"max_depth\"],\n",
        "                                   random_state=seed,\n",
        "                                   n_jobs=-1)\n",
        "        scoring = 'neg_root_mean_squared_error'\n",
        "\n",
        "    scores = cross_val_score(rf, X, y, cv=10, scoring=scoring)\n",
        "    print(f\"Random Forest with random params: {params}\")\n",
        "    print(f\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2suByRaJ5Pt6"
      },
      "source": [
        "# Task ID1: 14965"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFxOy5QFcVzJ",
        "outputId": "a0251366-9855-454b-e5e6-815c7fb1f51a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (45211, 16)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(14965)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8tDr4bwJvLM",
        "outputId": "e26559a8-b687-4413-88d8-7a60f7c0d850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-fold CV accuracy: 0.8142  0.1492\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwbCnY9mlAKS",
        "outputId": "aeac25c8-1fd9-4195-af86-01a3cdbc217c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8398  0.0424\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nX4SgHHlBzV",
        "outputId": "c74cf372-efd7-4952-df0f-f3ece72225cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-fold CV accuracy: 0.7432  0.1386\n"
          ]
        }
      ],
      "source": [
        "run_svm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxDjuF_WlDOP",
        "outputId": "dc544c3d-dcc5-49e6-974b-938384e7f0d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.8235  0.1123\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60S503d7lFB4",
        "outputId": "5329db10-6db2-48ee-ba6f-8e58322ecca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8769  0.0159\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zojzAMHO6Q_K"
      },
      "source": [
        "# Task ID2: 9977"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4af1RcN46erR",
        "outputId": "b8b5451a-5645-45d5-c83c-0187fe537b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (34465, 118)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9977)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrknfTbuCqlo",
        "outputId": "6c77ab80-9522-4269-ab07-0049659e4334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.9493\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVG0LT3_6erS",
        "outputId": "b293fd71-6bdd-4252-fb3e-42ff51a0c095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV Accuracy: 0.9274  0.0276\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H1TotA26erS",
        "outputId": "0663fbfa-52a4-4811-fe99-a361f55ec4a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-fold CV Accuracy: 0.9408  0.0296\n"
          ]
        }
      ],
      "source": [
        "run_svm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxffUZTH6erS",
        "outputId": "640460d4-e18b-488a-8438-eef29e3f0c90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV Accuracy: 0.8235  0.1123\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP4QcliE6erT",
        "outputId": "45ddd52a-0a05-4b62-8bea-2fc6c1bdedd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV Accuracy: 0.8769  0.0159\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hz-jF53KD4A"
      },
      "source": [
        "# Task ID3: 34539"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r1ojz5XKFG2",
        "outputId": "07f423b5-4eb3-4250-ac0c-868b5d4b2058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (32769, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(34539)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QEhF17lKFG3",
        "outputId": "93e1337d-9e4d-4f23-e15e-1d2e193a013e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.9472\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDpYa4HYKFG3",
        "outputId": "c8b27740-ec7c-42cd-9782-ae015470697f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV Accuracy: 0.9460  0.0018\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFFF63rFKFG3",
        "outputId": "b59ab841-f247-4a6b-99a5-54f7311b48a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-fold CV Accuracy: 0.9482  0.0011\n"
          ]
        }
      ],
      "source": [
        "run_svm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBdAAP2LKFG4",
        "outputId": "2fe494c9-75a1-460f-feb9-7ce8c51ea330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV Accuracy: 0.9437  0.0009\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFBGQcsRKFG4",
        "outputId": "58c74d48-5633-4930-d4d4-76d46c8fb33f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV Accuracy: 0.9421  0.0001\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvwXkeugnnxT"
      },
      "source": [
        "# Task ID4: 146606"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpK-BrI_nppN",
        "outputId": "498580dc-b857-4753-d82c-e7e4b57b4757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (98050, 28)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146606)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4AAgCVYnppN",
        "outputId": "00100a64-4e30-4199-894f-a9a3c0b14260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.6410\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHrS1WrznppO",
        "outputId": "02168a71-4679-4e68-c59d-dd44aa0af459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV Accuracy: 0.6434  0.0053\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9mjE-trnppO"
      },
      "outputs": [],
      "source": [
        "run_svm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEhPNj41nppP",
        "outputId": "7fd7315f-be8b-4d34-fbe8-cfab82860a3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV Accuracy: 0.6586  0.0030\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFLIFwgynppP",
        "outputId": "21983738-a9fd-4b3e-d964-cec9a952ae9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV Accuracy: 0.6645  0.0038\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agXkkvFj-uRE"
      },
      "source": [
        "# Task ID5: 7592\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qef1YPgY-uRF",
        "outputId": "01dbfb11-4975-4475-8072-f3d7723ad144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (48842, 14)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(7592)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVGfknW5-uRG",
        "outputId": "d2c2899f-0f71-416e-f4a9-c18ae7ea4850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.8516\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1zEjAoL-uRH",
        "outputId": "1bf782c2-4c08-4f38-b804-bd18ea1ae0d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV Accuracy: 0.8434  0.0033\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8wpFec_-uRI",
        "outputId": "d077752f-7730-4732-bab1-e84b1f38972a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV Accuracy: 0.8432  0.0032\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_oAxfn6-uRI",
        "outputId": "05acc519-152e-40a8-c78f-fef868eab917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV Accuracy: 0.8257  0.0040\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCGiaodFBGNK"
      },
      "source": [
        "# Task ID6: 146195\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx_GD9-LBGNL",
        "outputId": "cbfe0634-31ed-4bfa-c715-d44664a8d784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (67557, 42)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146195)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCAI04jJBGNL",
        "outputId": "fc020b10-ce6f-42e2-d294-89ca8baa3b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.7571\n"
          ]
        }
      ],
      "source": [
        "# Choose and run classification models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4CMhWolBGNL",
        "outputId": "9b1a2695-f604-49d0-bdf3-bf44d05f55e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV Accuracy: 0.6227  0.0459\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPXFjQSrBGNL",
        "outputId": "a90ea47c-502c-469f-969c-98ed75468dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV Accuracy: 0.5900  0.1039\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMAEFPv3BGNM",
        "outputId": "d6cc1ccd-83fc-473f-b26b-eabfdd4fbf37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV Accuracy: 0.6601  0.0034\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpQ09vBX6GbE"
      },
      "source": [
        "# Task ID7: 167119\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yzn3qD-6GbF",
        "outputId": "548936f0-583f-4060-9e0f-463d97c7ec75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (44819, 6)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(167119)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_tbAV3t6GbG",
        "outputId": "2b6e2dd0-a6a3-4d52-9605-de6d647b351c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-fold CV accuracy: 0.6539  0.0666\n"
          ]
        }
      ],
      "source": [
        "# Choose and run classification models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaDmWxDI6GbG",
        "outputId": "8138a557-3048-4388-9b17-850c1407a3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7219  0.0975\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arMzkO806GbH",
        "outputId": "00adf691-a0a3-4af5-acdb-fee022680257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.6412  0.0753\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAA52F7n6GbH",
        "outputId": "8d1ff078-9dba-4926-e64b-0bf8e0d7770c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7051  0.0600\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhMkxXLjFYC_"
      },
      "source": [
        "# Task ID8: 167120\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24VJEJaqFYC_",
        "outputId": "4dc055d9-c0d9-4736-85c9-f525decb6a19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (96320, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(167120)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUhH6tXgFYDA",
        "outputId": "60cce544-824b-451d-ed08-b134f9181ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.5234\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE_MHiB4FYDA",
        "outputId": "21e1c882-9587-4713-a0de-bede8a3c8b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV Accuracy: 0.5088  0.0045\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0i88aLWFYDB",
        "outputId": "abdd971d-7bdf-4220-cfe1-b6f0a1fa6cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV Accuracy: 0.5177  0.0036\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtLp4QIOFYDB",
        "outputId": "b6dababf-c251-4e39-a402-b32846a20d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV Accuracy: 0.5193  0.0034\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i_GKe1EIZUZ"
      },
      "source": [
        "# Task ID10: \t3945\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrgVeUIhIZUZ",
        "outputId": "39360c11-0b78-406d-fa75-f6f30a4e70a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (50000, 230)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3945)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1vKAQ9bIZUZ",
        "outputId": "81f62488-ee84-4e03-8bf8-365eae54451d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.9806\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfmlNiY-IZUZ",
        "outputId": "8fc69e33-ca81-4223-c2fe-bc96a0cdf9d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV Accuracy: 0.9822  0.0000\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iQYMR3BIZUa",
        "outputId": "22474f7a-0c7a-428f-fadb-23dacb997557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV Accuracy: 0.9821  0.0004\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeiFv2j_IZUa",
        "outputId": "c33e7fe9-3c2e-4920-b117-b0156dcb38c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV Accuracy: 0.9822  0.0000\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Or_hKHXYB39"
      },
      "source": [
        "# Task ID11: \t168331\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK0dIwCaYB3-",
        "outputId": "339ce63b-3d76-4ea1-a389-927a28c67049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (58310, 180)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(168331)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3Qj1fPvYB3-",
        "outputId": "047f1c33-7ebc-44e6-ffb5-1f290db4ea65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.5827\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "951p0lF3YB3-",
        "outputId": "05504bd5-e92a-479e-8110-352ee583fb6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV Accuracy: 0.6704  0.0050\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA7Xb8jRYB3-",
        "outputId": "17df7ac6-322f-4a05-c5ec-c20e8d8209c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV Accuracy: 0.4678  0.0045\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiQ1ksG8YB3-",
        "outputId": "4ca28928-bab3-4297-bc20-5fef003eb24a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV Accuracy: 0.4994  0.0040\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUMhEOrBdFAA"
      },
      "source": [
        "# Task ID12: \t168330\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li5QMmi9dFAA",
        "outputId": "24c53c40-421e-4754-bd34-29566bc2e19f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (83733, 54)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(168330)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t37NBADmdFAB",
        "outputId": "90814303-afe4-49c2-d02a-a02118c47de5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.6436\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_IfKfYPdFAB",
        "outputId": "f2c99a6b-1606-4bb0-dddd-6e1e37aa2f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV Accuracy: 0.6266  0.0041\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWCYkGSidFAB",
        "outputId": "9839fe9e-fafb-406a-97b6-6b0ebe54253a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV Accuracy: 0.6169  0.0041\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmbybi8jdFAB",
        "outputId": "0e367a53-633c-4eeb-b10c-aa86d16a52b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV Accuracy: 0.6377  0.0032\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgm3xuHO6hUr"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Task ID13: \t168335\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAI-NW5m6hUs",
        "outputId": "8aa0e685-a4d8-4721-cf87-5382cec6f1fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (130064, 50)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(168335)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX7KOn896hUs",
        "outputId": "7cea44ce-9567-4536-d258-efbb0b2e9c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-fold CV accuracy: 0.8829  0.0028\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf09kp7P6hUt",
        "outputId": "81330170-74f9-4a3c-dea7-cc56be7e98ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8966  0.0025\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSCbHKr16hUt",
        "outputId": "4f626361-caa3-4351-8e25-ef0475db384a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.8779  0.0030\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAeM3jRc6hUt",
        "outputId": "dfe64f2d-c22f-4ca7-d69f-85deed89ed9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8846  0.0039\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R-Is94K6lVs"
      },
      "source": [
        "# Task ID16: \t146212\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjWn8eK76lVs",
        "outputId": "41db2440-f03a-4a93-b800-bb67b1db6e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (58000, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146212)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIjDTKBC6lVs",
        "outputId": "f8d87f13-61f5-4dac-9123-5c23a8cd3b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-fold CV accuracy: 0.9665  0.0021\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjuMENvg6lVt",
        "outputId": "e28b9b60-e9f4-477d-e7e0-74b6050ce506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9981  0.0004\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6970gnu6lVt",
        "outputId": "2b5a725f-d805-41c6-aa33-db81e294bf7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9979  0.0004\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeXF_trD6lVt",
        "outputId": "87223d66-5e8f-4c29-c049-9a930d072e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9971  0.0003\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKBJFfbK6mI6"
      },
      "source": [
        "# Task ID19: \t168868\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrA8P1t66mI6",
        "outputId": "02b6d7f2-4603-4dc0-f8b5-3cfab4ab0767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (76000, 170)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(168868)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrJznCnc6mI7",
        "outputId": "09384c8c-154b-4992-ce3e-f955ee6dc5c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-fold CV accuracy: 0.9910  0.0014\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAokXumA6mI7",
        "outputId": "611d3231-6026-43fa-9cbd-ece6d74ca5a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9892  0.0010\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dhqlpoS6mI7",
        "outputId": "710d733b-d6bf-4a6e-c072-a8d0559442d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9888  0.0011\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R9PT_ZD6mI8",
        "outputId": "15154160-c274-403b-9c04-65096c599455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9896  0.0008\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  OpenML-CC18 Curated Classification benchmark"
      ],
      "metadata": {
        "id": "yrj97EfRui0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID20: 31"
      ],
      "metadata": {
        "id": "UkfKJeLtuxt7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b4e007-a78d-42a6-f9b9-5b2e93f97fe7",
        "id": "li-TPclsu9wd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1000, 20)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(31)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6585b56-94fc-4145-8ae9-b7978b7c36a0",
        "id": "3FxKyPB6u9we"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7500  0.0498\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513002ba-1e1a-4724-8988-3ebc8123b33f",
        "id": "IK6fVPPyu9we"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7320  0.0232\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f775ef-17b1-43f6-b129-0548a2b3e6ac",
        "id": "tlVQn8bIu9wf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7080  0.0199\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e726fd-bb5d-45a2-a05c-dc83d5124983",
        "id": "OYszrk8Ku9wf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7190  0.0255\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID21: 10101"
      ],
      "metadata": {
        "id": "u6DpJWyJu_Um"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b809ac-d04e-44cc-d3c2-528974c4baac",
        "id": "tRzCa0OJvAS0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (748, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(10101)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3724100-93bb-4892-c8c1-2fa82c768dc6",
        "id": "Q6ndHVM9vAS1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7687  0.0119\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f04555-43fc-4d6e-f932-86fdc894b906",
        "id": "DD_PNQ5evAS1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7301  0.0881\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf742f5b-3d92-4fa4-9415-91c8b156845c",
        "id": "MB3GsvHHvAS2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7488  0.1405\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba266455-d573-48ec-90bc-b5cd7db168c5",
        "id": "naVeqThHvAS2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7674  0.0698\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID22: \t3913"
      ],
      "metadata": {
        "id": "L4PMg4K1vD9b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b01068-a696-4b09-af04-c34348a86e04",
        "id": "s1fnAZHNvEcn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (522, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3913)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0048d7d4-2848-4145-fc7f-b1d633c5448b",
        "id": "G3VXNSnHvEcn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8406  0.0733\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df90349-d7d8-4f57-b313-a54282fc0bce",
        "id": "HpXhaOw5vEcn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8216  0.0625\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d99360d-d715-4f0e-fe86-8b976ffdcf23",
        "id": "hnZcUbk_vEco"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7851  0.0811\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1995c70-16e6-4d36-ebd2-4d7c74eebb65",
        "id": "rtvc6eBMvEco"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8272  0.0782\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID23: 3"
      ],
      "metadata": {
        "id": "6UVYtFS5vFdg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8092aaf5-5d11-486a-fb24-c17fcaf23e11",
        "id": "y5Ro7aC6vF76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3196, 36)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83eedb89-c1fa-467d-ceb1-6ebe787440ea",
        "id": "PTZK0BjKvF77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9393  0.0471\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9923d7a-2d9c-4f8a-d3f2-7bd25d7ea021",
        "id": "dSbPaJ-vvF77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7844  0.0701\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd5e111-8484-4c4f-c6bc-c49631f2ef1e",
        "id": "6_iM5kupvF78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9409  0.0568\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef28b5f-86f6-43f9-acb3-9a6447cb1c3c",
        "id": "KqfrhX7rvF78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9324  0.0642\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID24: 3917"
      ],
      "metadata": {
        "id": "vyRHJUzZvHFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0f693d-8f77-45bb-b1a4-cc6416903fb7",
        "id": "ltwmoLB4vIBd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2109, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3917)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46164c32-b52c-460f-8e8d-94364f669fd1",
        "id": "O-0hVteevIBe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8521  0.0241\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2918214b-ae0a-42eb-dcee-be294f6049b9",
        "id": "xdSglpS1vIBe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8379  0.0212\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b7df43-18f3-46b3-9b08-60fb37f68b81",
        "id": "D1aXt3bnvIBf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.8312  0.0342\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6428e9e-5198-437e-8366-9854566e0995",
        "id": "MsceSw67vIBf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8492  0.0202\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID25: 9957"
      ],
      "metadata": {
        "id": "dzf2O5cvvLhe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf1b7d7-1b8a-4886-b75e-2861d80f5fde",
        "id": "rWUJ6OTgvMOY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1055, 41)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9957)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca077fc-e8a2-4ac5-998c-4395b4adfeda",
        "id": "Zy8t5V15vMOZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8606  0.0387\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9ec975-d1af-4d9b-90d7-8f09993ac73f",
        "id": "XQpXe5HjvMOZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8454  0.0580\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4f26cf-cb42-4ba7-9480-9fccf21a56b4",
        "id": "gdsahFOsvMOZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7809  0.0668\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cadb4a9f-9cea-46a0-e752-a30645aa438c",
        "id": "SPgrEmugvMOZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8170  0.0715\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID26: 9946"
      ],
      "metadata": {
        "id": "xbL7uSt3vBp0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4bb872c-d9db-4af1-a46f-bfd09012c7e0",
        "id": "k6cJzLd5vCVc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (569, 30)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9946)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f5dbb9-098c-4e7b-f0b6-11164e57f2a2",
        "id": "_Ll78D2SvCVd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9807  0.0146\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c404c5-06ef-474c-98f1-89c5b740015f",
        "id": "PxjmtSAivCVd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9613  0.0233\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4404c09e-dc9f-46b1-d8f1-04a074beb56e",
        "id": "VEzfoim9vCVd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9158  0.0449\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7739eba7-d2a7-480e-c758-f244350466ce",
        "id": "uAhgRSMavCVd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9596  0.0342\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID27: 3918"
      ],
      "metadata": {
        "id": "4_3XUL5zOr31"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead80c4c-7062-4741-dd42-ea5cd24d9cb8",
        "id": "VYfNp5fcOr32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1109, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3918)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2b7cc8-bfe4-4017-a532-78997cac6267",
        "id": "GPG9Rs-lOr32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9297  0.0184\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6997c3-2116-4268-e079-54a4ef32ce13",
        "id": "jwt4BOefOr32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9270  0.0102\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e620d06-9ed6-48ee-eadb-76939a9e305d",
        "id": "nn8zOuMIOr33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9315  0.0128\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce96ea5d-0651-493b-92c7-181b0dd66019",
        "id": "gaauQmAxOr33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9297  0.0053\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID28: 3903"
      ],
      "metadata": {
        "id": "gMfLQ_viOsKc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dbfa1e2-bdd8-4499-816b-26ff2ef79682",
        "id": "5idMGXSBOsKd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1563, 37)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3903)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9139bae4-a97c-4530-a2bc-7555e2fa9451",
        "id": "VU_rzECWOsKd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9021  0.0112\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4e5469-2199-4bd4-89b3-441006704100",
        "id": "0b-gEGOSOsKe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8938  0.0109\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ab1e3c-4e9a-4fa3-b06f-fecee277998f",
        "id": "opOJBrNiOsKe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.8925  0.0080\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844cfa3b-447d-466e-d3fa-7f91b434fb5c",
        "id": "If20AUxhOsKf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8957  0.0042\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID29: 37"
      ],
      "metadata": {
        "id": "p8r0_x-wOsYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041a6071-a9ca-4592-e81b-7199f25c9c4c",
        "id": "GwDd8OA5OsYy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (768, 8)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(37)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f999719-79d6-41cf-b788-68aa1ff58d3c",
        "id": "01LeUanaOsYy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7722  0.0362\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2596e22-3319-458c-d521-1a110aee37c7",
        "id": "969FlODoOsYy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7474  0.0413\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d34608-b91d-4be3-eca3-200dcc0b173f",
        "id": "ZhOmRqzsOsYy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7226  0.0444\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8f8c72-b27f-465d-ee91-1b3e995dfb09",
        "id": "XndBjradOsYz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7643  0.0380\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID30: 9971"
      ],
      "metadata": {
        "id": "ZInQ-9WoOsu9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be713b0d-2f78-40f7-b796-5ed2addabc03",
        "id": "tYBvVcn2Osu9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (583, 10)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9971)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b95c57-8e3d-4c59-8876-66aac8dccc7f",
        "id": "BV88WfqXOsu9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7222  0.0191\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36902eba-8e8b-46f5-f2ba-9f5e5f8021ba",
        "id": "avC-hvRlOsu-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.6724  0.0427\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcfdf70-d96f-46d0-bc2f-d657ff7aa23f",
        "id": "1rf3hxwwOsu-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.6810  0.0272\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5a0ca3-cf84-44e7-85e6-70420d20f350",
        "id": "x-LreBNMOsu-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7084  0.0337\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID31: 9952"
      ],
      "metadata": {
        "id": "JXEaEKEaOtAm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85054ac2-0c94-45d7-baaa-e75be824f79a",
        "id": "-4zbJZsXOtAn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5404, 5)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9952)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152c1f9b-b852-4bb9-82b3-b6d68496e2e8",
        "id": "D8nt379pOtAn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7513  0.0092\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6efae08c-8c93-4089-a26b-723c902d0101",
        "id": "tIFO7NaIOtAn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8618  0.0150\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e0d21f-bcb7-4ee5-8262-a6169a621790",
        "id": "aUNxwF7aOtAn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7966  0.0105\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32b8b06-7868-4da0-d85f-322bd474a064",
        "id": "bk-A4adVOtAo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8168  0.0105\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID32: 3902"
      ],
      "metadata": {
        "id": "pW00w70pOtPI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff001439-28cb-45f2-e55e-a875d3e70dd6",
        "id": "Yve6Ha7AOtPJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1458, 37)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3902)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f247bc7-a413-4b33-956a-64c8bfe3bf11",
        "id": "JXWZ0Nk_OtPJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9095  0.0147\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0c0182-355d-492c-e390-c6ee236c396e",
        "id": "1xI2xPDuOtPJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8937  0.0140\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c036019e-0f4e-4cb2-a404-fa36ebe9bf03",
        "id": "W3npCrZIOtPJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.8944  0.0192\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7b8651-891c-4fc2-a0e3-57f6fcd8c61c",
        "id": "aHHQVruJOtPK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8923  0.0061\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID33: 49"
      ],
      "metadata": {
        "id": "6zZ17oDlOtbo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838e782c-89ab-4776-f609-5c138040c98c",
        "id": "3XUbfRA4Otbp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (958, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(49)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d99af2-5a97-4aaf-d871-5b5039837a38",
        "id": "PLfWlFsBOtbp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9759  0.0492\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b857d394-df52-4a2f-ba56-a64fa6520225",
        "id": "adDzFrnnOtbq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8613  0.0866\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b0c1bcb-c276-46ae-db8b-4aa0d1bdbb33",
        "id": "vkswOnb9Otbq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.6870  0.1060\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ccd913-c6f5-4129-a278-139f3ce4707e",
        "id": "Qiw8R1XJOtbq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7464  0.0594\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID34: 43"
      ],
      "metadata": {
        "id": "U5j20q0COtoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3c2a5c-fc82-4887-be46-a17dd96ecb9c",
        "id": "3OTTLs0LOtoD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (4601, 57)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(43)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de66ab33-1a80-46d7-9fc7-068c413e4db2",
        "id": "91CbDYbDOtoD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9170  0.0287\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fe6735-b059-4105-a3c6-e42e7bdbef2a",
        "id": "9Po5iGOrOtoD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8935  0.0367\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819f6f53-7b61-4a75-dfe8-7e01acd9f109",
        "id": "xvU4AcJTOtoE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.8907  0.0374\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaaf7c61-3aaa-43a8-83b5-5b5d06ad0754",
        "id": "F7q0apvbOtoE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9178  0.0314\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID35: 9978"
      ],
      "metadata": {
        "id": "I-iHZbecOt0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9598fa14-8352-4359-ae1e-b6186942d17e",
        "id": "yzuCJkqMOt0t"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2534, 72)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9978)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d43b80-f28c-4f63-a8a2-9224d9364bf7",
        "id": "HFYYRdkaOt0t"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9357  0.0217\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f46ab5d-a6fd-4777-f17b-ac83b9d2694c",
        "id": "M42BKsUoOt0u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9369  0.0058\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e2d4eb-2f05-4723-b58b-e245cd3ef7ca",
        "id": "Zn2hpG4GOt0u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9037  0.0376\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f99339-6e7a-4c72-ef8d-04c329941ccd",
        "id": "WRW8aafUOt0u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9317  0.0168\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID36: 10093"
      ],
      "metadata": {
        "id": "uysN5WV0Oxpu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd58d272-c438-4920-9c6a-7f024ac9335a",
        "id": "sAL9pAjTOxpu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1372, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(10093)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f40b6ce-8c1f-47fa-e941-3c7acb38642c",
        "id": "C0f9jKzlOxpu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9810  0.0099\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51dc9a5d-8ccb-4a49-b769-c3719f5cd038",
        "id": "XZZHysTkOxpv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9993  0.0022\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab60329-1532-42e0-c371-c8722dd40aa5",
        "id": "ucelT5N8Oxpv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9519  0.0199\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56faa110-d77d-40da-9bf1-3fef5b1a67f1",
        "id": "q5cifxeROxpv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9723  0.0112\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID37: 219"
      ],
      "metadata": {
        "id": "yGgmlLAKULw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e343b525-8e7a-4f5e-8359-1aaa0dc6fae7",
        "id": "U78e6HLYULw_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (45312, 8)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(219)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216d582d-ac13-4ba2-f897-53d8c5ea77e5",
        "id": "fxPxbG7aULw_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7427  0.0722\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a13ab7-5e5b-4f15-d45b-4cfe42508a5f",
        "id": "zdsCtfi6ULxA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7250  0.0611\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1772119-1a29-43bb-f373-8b4c67aa4ec6",
        "id": "0LvkFmBjULxA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7227  0.0958\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc05953-6df1-4150-8182-d2026f933288",
        "id": "4zQduxJRULxA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7346  0.0932\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID38: 9976"
      ],
      "metadata": {
        "id": "1SP0wCLxUMfZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee494e23-bd9c-4ca2-ee49-33f15782f6a0",
        "id": "Ys_nl9kRUMfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2600, 500)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9976)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39fd629-394b-49e6-beb0-dd802c5b4e87",
        "id": "qhmqO4iIUMfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.5519  0.0359\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecd4d17-3c9c-493b-c748-5f92e4352f6c",
        "id": "CtLyV57QUMfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.5723  0.0309\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374ec6e4-da0e-4784-9625-b1e3f576e7dd",
        "id": "Lz7nTFhkUMfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7327  0.0382\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1321c7-44b0-411d-9b63-5f682af069bb",
        "id": "JLX4B_rFUMfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.6446  0.0194\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID39: 6"
      ],
      "metadata": {
        "id": "S65JoO55UMry"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51434a37-1f06-4a1c-e8f7-7d3632b359d1",
        "id": "XIVWSoBIUMrz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (20000, 16)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(6)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49af461-8957-4e3b-c7c2-c84cca7549e6",
        "id": "JofdqbEPUMrz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7728  0.0109\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a2cd74-c332-4d3d-9509-01b0db102bc9",
        "id": "AKool9fHUMrz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9352  0.0066\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e36ac2-682d-4a9b-de27-936107a0ea4d",
        "id": "P2--bZKuUMrz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.2533  0.0039\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d546d8d3-dfb0-4852-ace9-883649bd9947",
        "id": "yaJyTg6jUMr0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.6069  0.0110\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID40: 53"
      ],
      "metadata": {
        "id": "BEsQTY0-UM3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f602e13c-8956-48b8-deee-16c859ca10e2",
        "id": "h81Iqxc1UM3x"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (846, 18)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(53)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2fd2dd-3ad7-40c1-8529-3c97a4d41d05",
        "id": "D5s6xIifUM3y"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7991  0.0267\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48dfe7f9-2043-4fe3-fb85-ac5a4dfe91fa",
        "id": "XZ3Rny_dUM3y"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7011  0.0452\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6dd6bf-97ae-40f9-d9c7-60432285f651",
        "id": "jXvhtAF9UM3y"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.6631  0.0390\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118ef103-787a-4372-b9fe-4ed32079198d",
        "id": "iLOYv1g0UM3z"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7116  0.0266\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID41: 11"
      ],
      "metadata": {
        "id": "dI13mhhGUNDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f01758-b24e-4a16-f431-26900575bde9",
        "id": "9kwzhfI9UNDx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (625, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(11)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f93f76f-5b7a-4599-ccb5-c384c8cd9c95",
        "id": "1G_p10YZUNDx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8497  0.0556\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8191b6-975b-4181-fb4e-f2b40fbb8693",
        "id": "5oIwjfb_UNDx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8369  0.0542\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2f9d45-8eba-4874-986b-4975de2a58f3",
        "id": "wAcuB__8UNDx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.6932  0.0861\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17935075-a2bf-4d35-9c20-9472f68cc105",
        "id": "RETd3EdiUNDy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.6800  0.1023\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID42: 15"
      ],
      "metadata": {
        "id": "L6e6nkQbUNPt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404c0bba-eb9e-4bb3-a0b2-5714a9dc6ceb",
        "id": "Teek6QgpUNPu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (699, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(15)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cf9dc6-f754-4afe-9adf-ccd73888b636",
        "id": "vAurp_cFUNPu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9671  0.0300\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee955401-9555-4a28-9e0e-8a702cc8e469",
        "id": "jOcOPlSUUNPu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9671  0.0293\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd9ebf4-392a-4382-a13d-84cec7e26a76",
        "id": "kefhtSHSUNPv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9428  0.0325\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71d81576-6aed-454b-f7d3-e3f68efb6c1c",
        "id": "phn9okJsUNPv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9686  0.0246\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID43: 16"
      ],
      "metadata": {
        "id": "iNhJDlrKUNat"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5cf894c-cba4-4d1d-829d-faa59b96a18d",
        "id": "KAadd-hGUNat"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 64)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(16)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360b5a8d-ca5a-4b83-8926-2963c81dc038",
        "id": "moJOCbDQUNau"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9490  0.0118\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00422924-eb37-4982-f10e-c9804b13cef9",
        "id": "tfGd1f_SUNau"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9420  0.0121\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf4245d-ad7d-4ed9-bdd7-c5b199d56ffb",
        "id": "5JBWBImXUNav"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5880  0.0308\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a215281-b834-45c5-839c-9a226e4a3c36",
        "id": "4sN1KzNWUNav"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9135  0.0204\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID44: 14"
      ],
      "metadata": {
        "id": "Ng32K-haUNl5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548e16b0-5f6e-4afe-9b25-a72869fc2671",
        "id": "xCCHn07SUNl6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 76)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(14)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41165a32-b876-41e1-dcd7-d7a6890f0a32",
        "id": "cGCBZUazUNl6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8210  0.0143\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24ef2a9-0797-4951-c9d9-11eb839d2702",
        "id": "7S1ABn2KUNl7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8040  0.0219\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8096ee3a-76a2-4a0a-c97e-508897f21a0c",
        "id": "7FqVCP9TUNl7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5405  0.0430\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e083e8b9-8000-4790-ecb7-79aef37f81c3",
        "id": "KozblJqBUNl7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7790  0.0258\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID45: 32"
      ],
      "metadata": {
        "id": "WL9ZOy4AUNws"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08bc8784-30ce-4793-9081-1865e7347254",
        "id": "eXcZc4KhUNwt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10992, 16)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(32)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd953a0e-262f-4c27-ae60-49668aba77ce",
        "id": "zQmvuGb3UNwt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9503  0.0105\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd77152-3f7e-44f2-c06b-f4403ac8f097",
        "id": "XIZMvbx7UNwu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9881  0.0039\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27820398-fac5-41e0-e58b-9fe25b9ebbdf",
        "id": "NHbw9zV9UNwv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7196  0.0143\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58661b5d-b953-4223-e4fd-e5f9446f644a",
        "id": "q_fhfg-EUNwv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8659  0.0170\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID46: 3549"
      ],
      "metadata": {
        "id": "ewtX2G2PUN7x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb154ca-1b90-43c0-b6d4-dde94e56a6cc",
        "id": "RwHzBeDmUN7x"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (841, 70)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3549)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b38461-eab2-4e4d-8cf2-6c13c5a7b2d4",
        "id": "vJWU5gLrUN7x"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9964  0.0054\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ffb7af-845a-4a72-8c2f-bf20767a02a1",
        "id": "badjm4XqUN7y"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9941  0.0079\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c813f4e1-8c90-4e56-cdcc-90da8b0ae4d8",
        "id": "0WH4n_S6UN7y"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9133  0.0492\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd0fc8f-1cbb-43a5-c627-796dca71d27f",
        "id": "wwNBekEXUN7y"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9834  0.0109\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID47: 12"
      ],
      "metadata": {
        "id": "7vNpomuQUOGc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c899360-feb9-4488-877c-a62ae2a8dd8f",
        "id": "5wKx6mlhUOGc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 216)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(12)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512e5da2-d4c5-4ecb-ebf3-8dbc0ddd7528",
        "id": "lTWBI6cOUOGd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9785  0.0078\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a75647f-f9e7-45de-9aa6-a344ab67609c",
        "id": "teGM9f5DUOGd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9600  0.0105\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd88b9a2-1a1b-4de7-c287-a083567cf55d",
        "id": "M0-BBAAzUOGe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5235  0.0166\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855702c6-236d-4685-8b64-fdc69b53d89a",
        "id": "IzZtC6huUOGe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9255  0.0175\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID48: 9981"
      ],
      "metadata": {
        "id": "fWOstqIAUORB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca64354-d0a8-4b64-b936-87fa4e066331",
        "id": "HFKFlyRgUORB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1080, 856)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9981)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66ab112-90d6-450d-ff09-82c396729bf8",
        "id": "UPMhbP9pUORC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9509  0.0227\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2df32e-0ca0-4a57-d1be-e25fe2adf22a",
        "id": "_7MUL71UUORC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8343  0.0322\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee95429-2cba-4926-9900-9f431feab4d7",
        "id": "X_1rGVvzUORC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.4278  0.0301\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95eb4ae8-bad9-420d-bf84-90697283ba6b",
        "id": "o6CnZDD5UORC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8806  0.0232\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID49: 18"
      ],
      "metadata": {
        "id": "lhXlnoGdUObZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd7d1ce-29ea-4494-9c9c-a958504f972f",
        "id": "83RPH8DlUObZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 6)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(18)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e236f390-2533-4ad7-8850-28fdc4378d2d",
        "id": "WtoK8HPUUOba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7360  0.0206\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "285b38cb-1f64-46c5-ffea-5de7ebd0e874",
        "id": "bLrOaReVUOba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7170  0.0125\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348b8dfb-f3a9-4fca-a712-d97f10195528",
        "id": "HJ7rsCBvUObb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.6355  0.0584\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf76e50-f733-493a-a6b5-7eb386443898",
        "id": "OSAuzasgUObb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7040  0.0211\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID50: 28"
      ],
      "metadata": {
        "id": "WJJ0ZiGDUOnY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7718a34-ce43-47ec-a203-a3fde1b3cf3d",
        "id": "cFjmeURAUOnY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5620, 64)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(28)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3724a971-e399-4cd6-8076-279e2760e442",
        "id": "EVhqfAvQUOnY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9635  0.0065\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762613bc-62c8-42cc-e756-dc59212fef98",
        "id": "e5obhRhLUOnY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9676  0.0054\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92951b1a-1739-4c09-d611-765a4cf700ea",
        "id": "II6HqrTOUOnY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5235  0.0325\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe4f491-5bc7-4512-f9cd-c28efa07daa9",
        "id": "bGV-CarTUOnY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9034  0.0192\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID51: 2074"
      ],
      "metadata": {
        "id": "cDW6wxhbUOzO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f48b94-7863-4736-ab5e-4ae99a30a965",
        "id": "sVFN742eUOzO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (6430, 36)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(2074)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3a7652-4553-452d-bc17-afb67e8b94cc",
        "id": "4CXyz3TXUOzO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8575  0.0111\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e43194-c2b6-4eb4-dbf9-5701a21654d9",
        "id": "e7wBxyITUOzP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8994  0.0089\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2ba197-4951-461c-c19a-e87c2dff8205",
        "id": "RAI_FuNvUOzP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7876  0.0145\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "084b7ed8-24ae-45a3-92be-cbe83c1f02a9",
        "id": "uy7Cmp1mUOzP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8375  0.0076\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID52: 29"
      ],
      "metadata": {
        "id": "x8l_UBrFUO-k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2c5363a-5151-4df6-8aeb-5dfb94024f55",
        "id": "AMpHfpUHUO-l"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (690, 15)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(29)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999cfb36-a55e-4300-ab12-6f1a58ce70d8",
        "id": "OntSrVakUO-l"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8420  0.1574\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f68818-e6bb-47d7-e1e8-1fc1c9727606",
        "id": "CFhsGv_mUO-l"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8406  0.1126\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e235ce8d-9a3b-47ea-e1d0-edbf90bcb44e",
        "id": "Kg7LFmXbUO-m"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.8377  0.1391\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a9f214-5f5a-4075-9069-e968c499b2a3",
        "id": "NzDoIs23UO-m"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8536  0.1334\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID53: 45"
      ],
      "metadata": {
        "id": "K8UnhnOzUPJ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc45ed0-d076-42f6-94b9-b8cf1bf4839a",
        "id": "iGcVYixEUPJ_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3190, 60)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(45)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437c6f53-1e1c-49f8-e2ad-42f7b88f92cc",
        "id": "HncC23hAUPJ_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9411  0.0174\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d3c880-e2a8-4630-89d3-18493bd7a98c",
        "id": "vsi-EUY0UPKA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8379  0.0260\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914a44a0-0ebe-440c-ddd1-40326da9fee0",
        "id": "qSJl5L9-UPKA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9210  0.0135\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4dbdb39-d542-40c0-94ca-29dd4c685b6f",
        "id": "gVBcDU6LUPKA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9088  0.0332\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID54: 125922"
      ],
      "metadata": {
        "id": "FIX9XbrkUPVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "647ffb26-bc31-4d6a-e469-df9e3dcfe430",
        "id": "q2hyutzkUPVf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5500, 40)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(125922)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd5d25c-3aa2-421b-d0ce-413329b2a318",
        "id": "qEG3eS9pUPVg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9955  0.0017\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131c7300-1120-4815-9324-ca407c370d26",
        "id": "EuCcIdi8UPVg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9735  0.0068\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dbd9b51-a68e-4f33-cfba-115cb9eceadb",
        "id": "RWTDNAW-UPVg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5316  0.0084\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49f1d2f-036b-4286-f6ad-f8b7edf4636e",
        "id": "0oa-c_CsUPVg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8138  0.0123\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID55: 9960"
      ],
      "metadata": {
        "id": "KbGl87-KUPhg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a308d7-ab26-4d27-ff3a-f6dcfd15a5fa",
        "id": "1QpnLvtsUPhg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5456, 24)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9960)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a26e172-bff0-4366-cb06-7ca8812b484d",
        "id": "tNrXecmKUPhg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.6692  0.0426\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e6cbf8-d5c2-4a32-e2a8-6f7abc662a55",
        "id": "a5d6z77NUPhh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7572  0.0626\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b74b73-fa7b-4ae1-9558-2132347f47bf",
        "id": "MqM7acfCUPhh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9062  0.0403\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2caf19-7371-4798-8d92-2570faefd021",
        "id": "bi_SyeHjUPhh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9256  0.0394\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID56: 9964"
      ],
      "metadata": {
        "id": "XykMaPm-UPuA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a1088f6-172d-4af5-e51d-b3c4a082594f",
        "id": "R0qJ6nc3UPuB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1593, 256)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9964)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff58808d-0499-47e8-bc01-55beac4794f4",
        "id": "Zw4IHAXLUPuB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9178  0.0191\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18087998-b93f-48b2-807c-b1ba3c39f0e5",
        "id": "ekYc_tH1UPuB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9021  0.0295\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d2d699-6136-413e-c69a-2f2f3bb73ac3",
        "id": "W0y0luWBUPuC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5776  0.0403\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc87fcca-8383-47ef-bb88-30809702371e",
        "id": "xsVAFiZMUPuC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8173  0.0345\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID57: 22"
      ],
      "metadata": {
        "id": "sgLsaY6mUP5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b007b6-c3e8-4734-f9ab-e2d18a9655e0",
        "id": "xftL2aHgUP5o"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 47)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(22)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c3a7ac-16ff-48ac-be46-88fd1e4364f7",
        "id": "n_s8O1EVUP5o"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8245  0.0199\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25c670f-df3b-43fd-c104-3f29e730a50e",
        "id": "IbgwN7KFUP5p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8040  0.0237\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d4fbb2-7e4d-4380-b472-184835520ae4",
        "id": "B2pbig5HUP5p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.4935  0.0535\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffdfedb4-ea89-4930-c079-eff1e36e0bd6",
        "id": "sFF-FUrIUP5p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.6935  0.0265\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID58: 2079"
      ],
      "metadata": {
        "id": "iPjBiyKOUQGo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e511b5-b490-46bd-c233-2674c2995bed",
        "id": "y6e0JqgQUQGo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (736, 19)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(2079)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d062b4-fcc6-405b-9f14-22d988dd8065",
        "id": "j-ak376_UQGo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.4828  0.0909\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a5ce97-2742-475c-84af-4753be9c5526",
        "id": "EX_QRNylUQGp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.4245  0.1145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2001d065-1cbf-4dba-93f9-868fe31d0fa3",
        "id": "CppY-gm5UQGp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5655  0.0622\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b99b26-8b93-419a-c9ee-9508c99879f5",
        "id": "Xah1yGttUQGp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.4557  0.0925\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID59: 14969"
      ],
      "metadata": {
        "id": "DCBskzEeUQTb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd47a77-c0be-4ad2-a642-c7b2b3ed3697",
        "id": "ENN7Xyj3UQTc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (9873, 32)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(14969)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6a9a4f-98ac-4fe9-9d7f-a36125eafa2f",
        "id": "7rNTeMXDUQTc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.4598  0.0498\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ee8645-d8ee-47c2-e464-be89cb7c947e",
        "id": "RGViU4B_UQTc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.4234  0.0623\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39436fba-f253-4802-b87a-3e3cc21bfcf3",
        "id": "WCHGfmyNUQTc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.4444  0.0496\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a308e5-89b8-4606-ac08-2bcd8c920576",
        "id": "lnAQanxWUQTd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.4620  0.0466\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID60: 3560"
      ],
      "metadata": {
        "id": "3scdUqaAUQfh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7376361e-af75-46b1-f483-2fdab1ac3536",
        "id": "xzD4eIsYUQfh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (797, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3560)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55b72fb-f08b-4955-9a11-b09d03bc6e5e",
        "id": "svq9woTQUQfh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.2034  0.0436\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe47b20a-835c-4500-d62f-5a76d2370eef",
        "id": "ihcmL5SWUQfi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.1895  0.0194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d70fd3-3c9e-4fff-9308-4a88b2a56ac2",
        "id": "XKezc-ojUQfi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.1969  0.0485\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b48413f-8823-46dc-9bb4-9620b34ff182",
        "id": "SGzQCDA9UQfi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.1997  0.0395\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID61: 14952"
      ],
      "metadata": {
        "id": "cyp2ScRKUQtY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43adb4f4-01f0-4806-dbf2-ac49a6931ceb",
        "id": "sP66dD9aUQtY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (11055, 30)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(14952)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5faa7ace-63ef-4b1b-91f6-bfe8b5396997",
        "id": "rURKT_31UQtY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9372  0.0068\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ec729c-6bdf-4942-ba74-f2dce6df2537",
        "id": "Hld-mJFtUQtZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9438  0.0119\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7582c39-d9df-4808-fe07-b3f3056981c8",
        "id": "XnxI6DfgUQtZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9174  0.0077\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c174775d-232e-4015-96c1-1fc2124921b0",
        "id": "UACEMX0qUQtZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9233  0.0099\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID62: 125920"
      ],
      "metadata": {
        "id": "wmm8YmtYdYqs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef419e35-a41b-45b7-9b1c-db2b39d04b67",
        "id": "Y19KhC5LdYqs"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (500, 12)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(125920)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76edf004-56fe-40f6-8a5f-cc689bc56bf5",
        "id": "H_AeNQhkdYqt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.6060  0.0664\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352539db-02ec-44f3-be6e-1273c4ab4e78",
        "id": "MDKek7WjdYqt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.5900  0.0694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dea6b19-4d34-4445-9d22-cc9bbb84397b",
        "id": "2QpJ6l9xdYqu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5840  0.0578\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2685c0b-40f7-4788-ad9d-e9cd64a4448b",
        "id": "5-6JXzQtdYqu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.6120  0.0421\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID63: 23"
      ],
      "metadata": {
        "id": "OOeFMqFydZr3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d98d861-be52-487f-ec59-90dc4acae25b",
        "id": "D7dc8ePKdZr3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1473, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(23)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef957195-ba1e-4e6b-f85b-065af46eecc1",
        "id": "XgwIbMNCdZr3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.5098  0.0369\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c6210e-ed1c-41f0-98bc-b885af01da25",
        "id": "it6jMgoydZr3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.5153  0.0181\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd1d9ea-a4d5-4717-c916-87c9aee14404",
        "id": "Rxok3cKLdZr4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5520  0.0307\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46747753-466e-4a37-fad4-95f6e3a5ab84",
        "id": "p8JHz6TDdZr4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.5268  0.0260\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID64: 3904"
      ],
      "metadata": {
        "id": "j0ZiCc7_dZ5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af1e22d-c16a-4d13-d493-d013e2404a97",
        "id": "4LPIV1WHdZ5O"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10885, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3904)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8171bc-0fca-4a26-9583-ffb3746979e9",
        "id": "yfA6zAL8dZ5O"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8100  0.0211\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64523e6e-76a8-4e6b-bf72-0af47e523247",
        "id": "bZoz65-zdZ5O"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7984  0.0167\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ddc11d5-dfe3-48b2-c5c4-1b3050faaae5",
        "id": "LJW-v28WdZ5P"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7962  0.0164\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7213869-8fff-4164-a100-abf5971005b5",
        "id": "htil_oQBdZ5P"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8084  0.0126\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID65: 3022"
      ],
      "metadata": {
        "id": "JgmWq_NbdaFn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2404a5-440f-4e1c-b607-c2d58a16ae85",
        "id": "mKRq8_F3daFn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (990, 12)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3022)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46a4f83-6fa7-406a-d16f-885c3374e43f",
        "id": "AavhMTPodaFo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.5495  0.0663\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c6407f-91b1-4007-a5d2-70add76bf7e9",
        "id": "dEA_g3yFdaFo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.6071  0.0481\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a65b56-6e0b-4989-e389-b0b0cfb2cd77",
        "id": "eQmgXt63daFo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.4222  0.0952\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d7ad0e-09da-4b65-c6f0-f72810489522",
        "id": "Sb7IqJOIdaFo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.5808  0.0713\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID66: 9985"
      ],
      "metadata": {
        "id": "k1saJ1SSdaTj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0259e80b-b273-4d21-9402-02b17718077f",
        "id": "JYN-CbMYdaTk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (6118, 51)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9985)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72002a4-7955-4d31-b710-a10001328ff9",
        "id": "PzZdmE2MdaTk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.4580  0.0622\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc2eb17-11f9-4d68-c6c4-3360733c5672",
        "id": "ok9DoeZvdaTl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.5031  0.0568\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a3b7ff-abe5-48e7-8e7d-a914f5b2d800",
        "id": "NFiWjeqvdaTl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.4333  0.0421\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc15e773-902e-4fb5-f5d5-d124a568471f",
        "id": "OLQclOfpdaTl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.4706  0.0492\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID67: 9910"
      ],
      "metadata": {
        "id": "NmZiZ5TQdagj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6153cd6a-1815-49d1-e6fc-3582ad4056aa",
        "id": "rT2WDeXkdagj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3751, 1776)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(9910)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7501c8-e0c9-46a5-d314-3b8462b7ceec",
        "id": "O2xzH3EDdagj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7337  0.0228\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104a349c-a461-4458-cb6e-642fb0231b19",
        "id": "blxUpW6kdagj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7438  0.0349\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e743ee9c-b2fd-4547-cb58-5401dd2fed4e",
        "id": "81mJSqqOdagk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7643  0.0167\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed13358-bb4a-43bd-e82a-4718c22ce3c6",
        "id": "2gskN_UJdagk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7254  0.0181\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID68: 14970"
      ],
      "metadata": {
        "id": "hbFnAU79das-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd683f7-8bc8-4d98-ea75-f7a94cfe6a0d",
        "id": "gQB-rgNLdas_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10299, 561)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(14970)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18a6466-365a-448c-e2e6-03baca555c7e",
        "id": "2IYssSKbdas_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9581  0.0266\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a9739c-f50a-406c-e00f-c2cac4056b83",
        "id": "Mp2DjPrGdas_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9054  0.0171\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518dc1c6-a03e-44e2-bc98-731cbb41ecc9",
        "id": "Cri60Kw_datA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.8490  0.0343\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9fbeb90-d50a-44bc-d4d7-e3663c4d8b35",
        "id": "SNspgwOidatA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8851  0.0166\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID69: 3021"
      ],
      "metadata": {
        "id": "HXGqTwEwda5r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51a90f5-1ae9-46c3-bd98-9764f8cb775b",
        "id": "8ArWzc5dda5r"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3772, 29)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3021)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75338af7-9101-4327-889b-2aee7d30d474",
        "id": "pEg9alWhda5r"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9669  0.0057\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1003730d-e534-4b4c-8a66-f0618252a234",
        "id": "LvQtS3h2da5r"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9642  0.0101\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42891e0b-e9bb-4a9a-f0f1-4454b9df63dc",
        "id": "gA5BBMNZda5r"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9849  0.0063\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcb8524-b741-44dc-aa68-f85087a78cea",
        "id": "YTTtb9Fzda5s"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9443  0.0060\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID70: 3481"
      ],
      "metadata": {
        "id": "_SGwxPNTdbG5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5c389b-46a1-4c73-9de7-e3f3d883bba2",
        "id": "6OdJMITXdbG5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (7797, 617)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3481)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6419ab21-4098-4217-ea34-f9687c4a9427",
        "id": "hIlTZqRvdbG5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9556  0.0138\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43801cdd-962e-41c5-87c9-3e4fea1089a6",
        "id": "VPDYZoYTdbG5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9151  0.0279\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ac1740-3870-43d5-97a7-a99fa2a1767b",
        "id": "bzsOFxOXdbG6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.3926  0.0204\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146b2098-1561-45e2-f150-f176b834c5a3",
        "id": "dd20l2NtdbG6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7816  0.0239\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID71: 3573"
      ],
      "metadata": {
        "id": "pRv7Cn0GdbT-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31943aa4-2c4e-470a-a9bf-3f947fd66193",
        "id": "poPjuFN4dbT-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (70000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(3573)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b331316-cdc9-45db-c8f2-f370fed9db90",
        "id": "U2pi4yQOdbT-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9173  0.0069\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-_peKTZdbT-"
      },
      "outputs": [],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImF6qfEtdbT-"
      },
      "outputs": [],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeSrsnAadbT_"
      },
      "outputs": [],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID72: 146824"
      ],
      "metadata": {
        "id": "yyid9hePdbi9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1706a9ba-953c-4f45-e5d3-f0eb5a0d94b5",
        "id": "AiexN3l2dbi9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 240)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146824)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbce53c0-9157-4285-ee4d-92fbd32601f2",
        "id": "Qj_3R6UIdbi9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9640  0.0080\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff240e1-0714-4d93-bf64-25ff91b3fd2c",
        "id": "mgQnb1Uldbi-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9695  0.0123\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da06e10-aca6-4b38-fce3-2b548672dbec",
        "id": "MgNQugtrdbi-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7735  0.0281\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9b2788-211d-4ea4-c980-b473832c35d0",
        "id": "Ei6OoX1kdbi-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9405  0.0149\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID73: 146820"
      ],
      "metadata": {
        "id": "ZErycBTkjcTT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e07eff5-a088-4068-d474-cb4baeda61bc",
        "id": "gyjNJFyGjcTT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (4839, 5)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146820)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0796826e-dddf-42fd-92c5-605e4cb4ebd2",
        "id": "gx47w5KPjcTU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9465  0.0321\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0dffeb-c211-4a52-ac0a-c1d8555c3840",
        "id": "V9HQFemijcTU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9599  0.0096\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d87327-e91a-4f08-8e1f-36f7979a1edb",
        "id": "NzNd03JljcTU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9777  0.0107\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e690bbff-735a-4ad0-8696-694c5d27e5d1",
        "id": "kf7aOhwWjcTU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9702  0.0088\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID74: 146822"
      ],
      "metadata": {
        "id": "ApnHqxGOjclf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d66a735-dcad-453c-d031-4c5b3b400f2f",
        "id": "P9OYX0R-jclg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2310, 16)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146822)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172ff906-0137-4f3a-8894-9ff4e0aaef9e",
        "id": "AvmJ0MeAjclg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8745  0.0099\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d0711f-7065-4f5e-cd4a-bca294c3a3f2",
        "id": "oScScjvzjclh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8831  0.0190\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016c8c4d-ffdb-473a-ee8f-7e36f21198bc",
        "id": "2mRWod0djclh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7558  0.0116\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bfb539-bd2d-4724-b319-f34396d4cea1",
        "id": "PoYzxkoMjclh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8485  0.0130\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID75: 146195"
      ],
      "metadata": {
        "id": "Ybce50ysjc0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622ed1e3-ebfe-4005-9f39-8605cc9bac0a",
        "id": "Vgh-XdZIjc0t"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (67557, 42)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146195)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663de838-9aa7-4ebe-85f8-d24f2ac23910",
        "id": "_zLXK9dEjc0t"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.7244  0.0440\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7f16f5-1094-4346-b784-bb894733f46e",
        "id": "GwW_FF3xjc0u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.6227  0.0459\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b822b30e-a013-4bc3-ff33-d06c86ec9b86",
        "id": "a4Qpx-o4jc0u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5900  0.1039\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44859c7-9086-422f-c6b1-071e7b7f0034",
        "id": "B1HFpxCOjc0u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.6601  0.0034\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID76: 146800"
      ],
      "metadata": {
        "id": "hS7jGUhzjdCn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7f5f31-c067-4618-d4c5-b52e1fb66af1",
        "id": "P1ngLKfMjdCn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1080, 77)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146800)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d788d1d8-15d6-4276-a54a-91b241249d60",
        "id": "yPSZWzVTjdCn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8407  0.0767\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c8ae49-16e2-40a5-c987-f7febcc0744e",
        "id": "gTeBF3MSjdCn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.5741  0.1253\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1634258-be12-45c5-e062-ac9e9a07ee22",
        "id": "QWoHhkI9jdCo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5139  0.1086\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fc223d-247d-4198-efa9-ca3466eede48",
        "id": "q5-95nb_jdCo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.6426  0.1076\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID77: 146817"
      ],
      "metadata": {
        "id": "Cp2KgzM5jdPc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb443c7-2fd3-4e0b-da03-044c6105bfd2",
        "id": "fbiJFjQgjdPd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1941, 27)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146817)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3cf9f28-f221-44a3-c735-99f8539377e5",
        "id": "w9prgyupjdPd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.6322  0.0939\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85060bf9-3df0-4a4d-bac3-de0092a19d71",
        "id": "OPwojqT8jdPd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.6132  0.1021\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e23e71c-26df-4410-ccb8-b76402b45d61",
        "id": "qwEfSsYojdPe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.5534  0.0802\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f59866c-93dd-4901-a2bc-d8f0ad6f6601",
        "id": "VAOiohrojdPe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.5931  0.0807\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID78: 146819"
      ],
      "metadata": {
        "id": "yXXQpbjwjdb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aafb5e3-7f79-49ca-f0d8-58bf3d8f57ec",
        "id": "cF43RL0Ojdb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (540, 18)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146819)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ed85c6-d0bb-4723-8a6e-50a9a4135349",
        "id": "ML3GSIxejdb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9611  0.0175\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8947edaf-355c-464b-ea5d-3ea9a18a4062",
        "id": "E7T1v-Dhjdb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9148  0.0091\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb3bb17-52b6-4117-ff99-34a667816a1a",
        "id": "Y31gaWi1jdb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9259  0.0166\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0950b0d6-9c6c-44bf-c607-28656b9c7bc3",
        "id": "i4nfwoIljdb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9148  0.0091\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID79: 146821"
      ],
      "metadata": {
        "id": "FQWP6NUVjdoM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "349f99ff-eaed-4cf3-fd7f-139ee79340ed",
        "id": "-hQrP6MhjdoN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1728, 6)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(146821)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6733b94-4fd4-4614-d33b-be92ffa1ffc9",
        "id": "sFXoIdRzjdoN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8327  0.0780\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b62db7-b5b3-4956-eea4-bb3115ad8d4b",
        "id": "tEajdBYAjdoN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.7617  0.0899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48338a68-e35d-4a15-ff94-f568358e2009",
        "id": "35tdCAhkjdoO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.7326  0.0674\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588f746e-bcd8-4db2-9113-607e10715cd2",
        "id": "UMANDVmZjdoO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.7344  0.0587\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID80: 14954"
      ],
      "metadata": {
        "id": "R1dUVRBFjd0n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec8e461-35dc-42a1-b9c8-beacffd87153",
        "id": "9zyTHIiUjd0o"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (540, 37)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(14954)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93cd4220-280e-437e-e5c6-4b6162e906d7",
        "id": "QlBm-f6Bjd0o"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.6019  0.1229\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5532c7-56f5-445f-e6b0-93801f985d4a",
        "id": "SD-N2zC2jd0p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.6278  0.0770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:583: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aae9d1d-c8a5-48b4-ad89-d6afc8c699cf",
        "id": "K0pNh8_Gjd0p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.6111  0.1356\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a5a2cd-1432-4eb1-a8ea-3948a3258146",
        "id": "Fs01Vl-Xjd0p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.6444  0.1239\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID81: 167141"
      ],
      "metadata": {
        "id": "hZqVKYjojeA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003e5160-f91a-422b-fdd9-763bd39bdd86",
        "id": "51YGj8FDjeA6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5000, 20)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(167141)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0458a196-72da-4328-e495-4380fa50819e",
        "id": "zp8OvRxHjeA6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.8546  0.0073\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9f0ddd-d4c9-4abc-bf85-a8018b734d69",
        "id": "gnBYAlGMjeA7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8778  0.0050\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60b5fce-e106-483b-f542-74241ab71020",
        "id": "mZ31VbeAjeA7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9264  0.0088\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edb4d4f-392e-4278-849b-d0bbf08db290",
        "id": "fQEYnMIajeA7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8892  0.0077\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID82: 167140"
      ],
      "metadata": {
        "id": "S19sCYYBjeNI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa13bfc8-0dea-49ff-818f-ba13aca7452d",
        "id": "Q-_iDOv_jeNI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3186, 180)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(167140)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f649cb-eacd-4233-e15a-d32db5cb9b2e",
        "id": "bQdkO5fUjeNI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9466  0.0115\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "242b838c-df2a-4b85-958c-c0aec9878960",
        "id": "0x1hUDOAjeNJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.8528  0.0167\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f641d5b-425b-4edc-d4b2-a9950a64a735",
        "id": "MFTetWJyjeNJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.8939  0.0155\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224b9424-8660-4252-f907-6aa043401292",
        "id": "RG5lBL_WjeNJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.9005  0.0119\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID83: 167125"
      ],
      "metadata": {
        "id": "LDgS_tZ8jeZ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64dca9c7-dc8f-41cb-d12b-3cb443df965d",
        "id": "Y-N90ndKjeZ7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3279, 1558)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(167125)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372ffa8d-ee65-4371-df47-681847f0e4a7",
        "id": "6mgwgGWUjeZ7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold CV accuracy: 0.9634  0.0248\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2703af-2daa-4434-d8f0-0a212075a22f",
        "id": "vUqm-BdxjeZ7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with random params: {'n_neighbors': 15, 'knn_alg': 'ball_tree', 'leaf_size': 300}\n",
            "10-fold CV accuracy: 0.9341  0.0289\n"
          ]
        }
      ],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4c0b29-3749-4007-a381-e0ce7c943e72",
        "id": "Vt1nadnXjeZ7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with random params: {'max_depth': 4}\n",
            "10-fold CV accuracy: 0.9558  0.0231\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92651a50-7eb7-4b6c-f4eb-cfa11f422f70",
        "id": "rJCI97UzjeZ7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with random params: {'max_depth': 4, 'n_estimators': 86}\n",
            "10-fold CV accuracy: 0.8990  0.0204\n"
          ]
        }
      ],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID84: 167124"
      ],
      "metadata": {
        "id": "y5x3ad8gjenJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58dd0746-3367-444e-ebae-0689791967b7",
        "id": "sXTULHs4jenK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (60000, 3072)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(167124)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JBLKuaZjenK"
      },
      "outputs": [],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ80Zdk9jenK"
      },
      "outputs": [],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5tNmqr3jenK"
      },
      "outputs": [],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGH_E9UdjenL"
      },
      "outputs": [],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID85: 167121"
      ],
      "metadata": {
        "id": "Q82DhhLtjezZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3dba311-b828-454a-c101-3472ac3a5604",
        "id": "aKG4coGDjezZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (92000, 1024)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y = load_preprocess_task(167121)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvkcLSxQjeza"
      },
      "outputs": [],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_linear_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgjE7i6Ijezb"
      },
      "outputs": [],
      "source": [
        "run_knn(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_84I0LEUjezc"
      },
      "outputs": [],
      "source": [
        "run_decision_tree(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGesQ2hujezc"
      },
      "outputs": [],
      "source": [
        "run_random_forest(X, y, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpybL6mAmBGw"
      },
      "source": [
        "# GBDTs: XGBoost, CatBoost and LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59ezRz2mxsQw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmUB8qBjmBGz"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP3G5Va_mBGz"
      },
      "outputs": [],
      "source": [
        "def load_preprocess_task(task_id, model_type=\"xgboost\", task_type=\"classification\", target_encode=None):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=task.target_name)\n",
        "\n",
        "    print(f\"Dataset shape: {X.shape}\")\n",
        "\n",
        "    # Encode target variable if classification\n",
        "    if target_encode or (target_encode is None and task_type == \"classification\"):\n",
        "        le = LabelEncoder()\n",
        "        y = le.fit_transform(y)\n",
        "\n",
        "    # Define feature columns\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "    numeric_cols = X.select_dtypes(include=['number']).columns\n",
        "    cat_features = []\n",
        "\n",
        "    # Define transformers for numeric and categorical columns\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Configure encoders based on model type\n",
        "    if model_type == \"xgboost\":\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "    elif model_type == \"catboost\":\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('ordinal', OrdinalEncoder())\n",
        "        ])\n",
        "        cat_features = [X.columns.get_loc(col) for col in categorical_cols]\n",
        "    elif model_type == \"lightgbm\":\n",
        "        # Ensure one-hot encoding for LightGBM as it doesn't handle categorical strings natively\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "        cat_features = [X.columns.get_loc(col) for col in categorical_cols]\n",
        "\n",
        "    # Create preprocessor and preprocess data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    return X_preprocessed, y, cat_features if model_type in [\"catboost\", \"lightgbm\"] else None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk_LRgQ5mBG0"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqUBgN8dmBG0"
      },
      "outputs": [],
      "source": [
        "def cross_validate_model(model, X, y, task_type=\"classification\", n_folds=10):\n",
        "    if task_type == \"classification\":\n",
        "        cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "        scoring_func = accuracy_score\n",
        "    elif task_type == \"regression\":\n",
        "        cv = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "        scoring_func = mean_squared_error\n",
        "    else:\n",
        "        raise ValueError(\"Invalid task type. Use 'classification' or 'regression'.\")\n",
        "\n",
        "    scores = []\n",
        "    for train_index, test_index in cv.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        score = scoring_func(y_test, y_pred)\n",
        "        if task_type == \"regression\":\n",
        "            score = np.sqrt(score)  # RMSE\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    avg_score = np.mean(scores)\n",
        "    metric_name = \"Accuracy\" if task_type == \"classification\" else \"RMSE\"\n",
        "    print(f\"Average {metric_name}: {avg_score:.4f}\")\n",
        "\n",
        "    return avg_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSQH5bBumBG0"
      },
      "source": [
        "\n",
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzTzGg3XmBG0"
      },
      "outputs": [],
      "source": [
        "def get_random_xgboost_parameters(seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"max_depth\": int(np.round(np.power(2, rs.uniform(1, np.log2(12))))),\n",
        "        \"alpha\": np.power(10, rs.uniform(-8, 0)),\n",
        "        \"lambda\": np.power(10, rs.uniform(-8, 0)),\n",
        "        \"eta\": 3.0 * np.power(10, rs.uniform(-2, -1)),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def run_xgboost(X, y, seed=42, task_type=\"classification\"):\n",
        "    params = get_random_xgboost_parameters(seed)\n",
        "    model = XGBClassifier(**params) if task_type == \"classification\" else XGBRegressor(**params)\n",
        "    scoring = 'accuracy' if task_type == \"classification\" else 'neg_root_mean_squared_error'\n",
        "\n",
        "    scores = cross_val_score(model, X, y, cv=10, scoring=scoring)\n",
        "    print(f\"XGBoost with random params: {params}\")\n",
        "    print(f\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiq-0pUJmBG0"
      },
      "source": [
        "# CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9XMFkbbmBG0"
      },
      "outputs": [],
      "source": [
        "def get_random_catboost_parameters(seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"learning_rate\": 3.0 * np.power(10, rs.uniform(-2, -1)),\n",
        "        \"max_depth\": int(np.round(np.power(2, rs.uniform(1, np.log2(12))))),\n",
        "        \"l2_leaf_reg\": 0.5 * np.power(60, rs.uniform(0, 1)),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def run_catboost(X, y, seed=42, task_type=\"classification\", num_classes=None):\n",
        "    params = get_random_catboost_parameters(seed)  # Removed task_type\n",
        "\n",
        "    if task_type == \"regression\":\n",
        "        model = CatBoostRegressor(iterations=1000, verbose=0, **params)\n",
        "        scoring = 'neg_root_mean_squared_error'\n",
        "    elif task_type == \"classification\":\n",
        "        model = CatBoostClassifier(iterations=1000, classes_count=num_classes, verbose=0, **params)\n",
        "        scoring = 'accuracy'\n",
        "\n",
        "    # Perform cross-validation with suppressed verbosity\n",
        "    scores = cross_val_score(model, X, y, cv=10, scoring=scoring)\n",
        "    print(f\"CatBoost with random params: {params}\")\n",
        "    print(f\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h01qWXfZmBG1"
      },
      "source": [
        "# LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWRPrD5jmBG1"
      },
      "outputs": [],
      "source": [
        "def get_random_lightgbm_parameters(seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"num_leaves\": int(np.round(np.power(2, rs.uniform(1, 12)))),\n",
        "        \"lambda_l1\": np.power(10, rs.uniform(-8, 1)),\n",
        "        \"lambda_l2\": np.power(10, rs.uniform(-8, 1)),\n",
        "        \"learning_rate\": 3.0 * np.power(10, rs.uniform(-2, 1)),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "\n",
        "def run_lightgbm(X, y, seed=42, task_type=\"classification\"):\n",
        "    params = get_random_lightgbm_parameters(seed)\n",
        "\n",
        "    # Determine num_classes based on unique target values for LightGBM compatibility\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    if task_type == \"regression\":\n",
        "        params[\"objective\"] = \"regression\"\n",
        "        params[\"metric\"] = \"mse\"\n",
        "        model = LGBMRegressor(**params, verbose=-1)\n",
        "        scoring = 'neg_root_mean_squared_error'\n",
        "    elif task_type == \"classification\":\n",
        "        params[\"objective\"] = \"multiclass\" if num_classes > 2 else \"binary\"\n",
        "        params[\"num_class\"] = num_classes if num_classes > 2 else None\n",
        "        model = LGBMClassifier(**params, verbose=-1)\n",
        "        scoring = 'accuracy'\n",
        "\n",
        "    # Perform cross-validation\n",
        "    scores = cross_val_score(model, X, y, cv=10, scoring=scoring)\n",
        "    print(f\"LightGBM with random params: {params}\")\n",
        "    print(f\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DnR_bJYmBG1"
      },
      "source": [
        "# Task ID1: 14965"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56022e9-345e-46dd-b6fb-48929008c8ee",
        "id": "TkhWIe34mBG1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (45211, 16)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(14965)  # Replace with an actual task ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a45707c-8924-4708-a76c-b25d115aa62f",
        "id": "zXRME7sJmBG1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7360  0.1495\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0b1305-4392-440a-da93-5aea9eda3679",
        "id": "jXKX4mYcmBG1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatBoost with random params: {'learning_rate': 0.07106591851092234, 'max_depth': 11, 'l2_leaf_reg': 10.013039911391246}\n",
            "10-fold CV accuracy: 0.6491  0.1581\n"
          ]
        }
      ],
      "source": [
        "run_catboost(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6291eb9e-9742-41f9-e551-29b5dbd6fffb",
        "id": "RuZRQCAVmBG1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.7733  0.1460\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtgEW3WTmBG1"
      },
      "source": [
        "# Task ID2: 9977"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46be3c36-3f79-4781-abfb-18e6e8eb2b0c",
        "id": "-5vqpKDSmBG1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (34465, 118)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9977)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18AyWYMx1UZF",
        "outputId": "a121a57d-3f90-4995-e866-b83abd74bd51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9488  0.0254\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jfiUfQhV1UZF",
        "outputId": "2cdcc62d-1dd4-4e34-932e-b5a276af4ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatBoost with random params: {'learning_rate': 0.07106591851092234, 'max_depth': 11, 'l2_leaf_reg': 10.013039911391246}\n",
            "10-fold CV accuracy: 0.9531  0.0224\n"
          ]
        }
      ],
      "source": [
        "run_catboost(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5kfny9L1UZG",
        "outputId": "8366ca25-bbf0-4290-87f0-c875daead95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8318  0.0689\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdYLmjsBmBG2"
      },
      "source": [
        "# Task ID3: 34539"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c1773d-ba00-44bf-d3ce-7c43a628b29b",
        "id": "xUlk2wUUmBG2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (32769, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(34539)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSvKKzGg1amu",
        "outputId": "069e41a3-6a85-49c5-c7d8-553ffbeb7c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9431  0.0006\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQXL7f0_1amv",
        "outputId": "49ea2005-8241-43c2-e6d3-d25b9c11dd19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9131  0.0140\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ1lM1Bv99qs"
      },
      "source": [
        "# Task ID4: 146606"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ceb317-5ddd-4e2b-fcc4-bff25f18e5e4",
        "id": "ui3L3MPF99qs"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (98050, 28)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146606)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a755eab-f0f6-42df-9206-35af1cdca8d9",
        "id": "FOnTkvME99qt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7192  0.0041\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca0bdcd-2d3a-44f4-f94f-1949f276e75d",
        "id": "JPN6qBuK99qt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.6664  0.0094\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku2GvatBmBG3"
      },
      "source": [
        "# Task ID5: 7592\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a537486-ccc5-4ce6-efbc-1f1488282991",
        "id": "ViR09heQmBG3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (48842, 14)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(7592)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-aK1ln91knH",
        "outputId": "c99b04ec-b22b-4f16-8c35-b22576316ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8721  0.0031\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP6IdKl-1knI",
        "outputId": "3b8b00ea-deb8-48d1-bbef-485e8aa04ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.6680  0.1565\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qB-NZ-t-wyd"
      },
      "source": [
        "# Task ID6: 146195\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187ab132-8819-49e7-a48b-9094451fd5c5",
        "id": "_40sjzCV-wyd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (67557, 42)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146195)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e3cc9a-b20e-4f81-ab8d-f2a7a244d832",
        "id": "SeQA8b2Q-wye"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.6903  0.0501\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f014a04-4a42-4c12-ddda-c4df1dbdf1ca",
        "id": "B5QrRkLb-wye"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 3}\n",
            "10-fold CV accuracy: 0.4384  0.1732\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-6lP2ED-yYw"
      },
      "source": [
        "# Task ID7: 167119\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac43499-a67d-4241-b815-a353ca58021d",
        "id": "r5NMtRXx-yYx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (44819, 6)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(167119)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8821a7-603e-4d78-e56c-0b1e7239190a",
        "id": "J8GzbspK-yYx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7006  0.0713\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad89393-6f8f-4070-a3ba-0ad3fb57e460",
        "id": "FggJ5o9L-yYx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 3}\n",
            "10-fold CV accuracy: 0.4532  0.1107\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqc8I91XmBG4"
      },
      "source": [
        "# Task ID8: 167120\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8189b0cc-6861-4ae4-e825-cb0a9928eecf",
        "id": "ITqjmJabmBG4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (96320, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(167120)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbRdyLRy1w3u",
        "outputId": "e96228a6-b20f-4442-8a36-60ff6cfe020a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.5177  0.0038\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbKWZR2t1w3v",
        "outputId": "c02f0c4a-8072-4fb3-9e5a-8e4b7a258051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.5047  0.0052\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VekBUwoM9Dk"
      },
      "source": [
        "# Task ID9: 146825\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754010a6-6f2f-41eb-a341-14012c490daa",
        "id": "KRqNK-SYM9Dk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (70000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146825)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "fbd925da-a957-4a10-9c04-8ec95257f01f",
        "id": "jxQyzJsjM9Dl"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-14973b8f02ee>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Choose and run models with 10-fold cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_xgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-0a1506be2bd3>\u001b[0m in \u001b[0;36mrun_xgboost\u001b[0;34m(X, y, seed, task_type)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtask_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classification\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'neg_root_mean_squared_error'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"XGBoost with random params: {params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    424\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             )\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1532\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02f0c4a-8072-4fb3-9e5a-8e4b7a258051",
        "id": "J7H59ICmM9Dl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.5047  0.0052\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFf0q3EqmBG5"
      },
      "source": [
        "# Task ID10: \t3945\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdbc6a1d-636a-40ab-f888-2219089e4c65",
        "id": "f5NOiruimBG5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (50000, 230)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3945)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFqDP4-x1zZy",
        "outputId": "44e1897a-f4b4-4880-f76a-226aa980f0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9821  0.0001\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fcbm98To1zZz",
        "outputId": "07279ff0-781f-4a38-ff6a-f9f4b1545461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9682  0.0092\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCt8ZJ-imBG5"
      },
      "source": [
        "# Task ID11: \t168331\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547e55e7-9afe-4dff-cf9b-0319203bb912",
        "id": "mko6BRr5mBG5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (58310, 180)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(168331)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_wH6X9e11aK",
        "outputId": "2c4f1aba-28d4-40f9-f400-f28fc74212b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.6448  0.0050\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqD5kF4J11aL",
        "outputId": "e58e47f1-b6bf-46d2-acc8-cf372ae9240f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.1990  0.0474\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPNCH9MSmBG6"
      },
      "source": [
        "# Task ID12: \t168330\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf38737-f46a-45fd-8c58-4d680cb10d05",
        "id": "QaqB2ugKmBG6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (83733, 54)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(168330)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzRk9K7U13pC",
        "outputId": "66326f96-1bdf-405e-d4a0-74ab035aac9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7014  0.0031\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erRKEytN13pD",
        "outputId": "1730f05a-ba0a-4508-a15e-ce90185f4d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 4}\n",
            "10-fold CV accuracy: 0.4279  0.0751\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3yrnlEGULX7"
      },
      "source": [
        "# Task ID13: \t168335\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103542df-4090-4fee-8459-9a6814667dfc",
        "id": "Ay2Mr7mdULX8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (130064, 50)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(168335)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a490b705-e59a-4ffd-ca6a-ff9f4f13e584",
        "id": "jLBda3O4ULX8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9355  0.0020\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c29709-1bef-49d3-d399-f17fe430cb09",
        "id": "VNlOvKXjULX9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8926  0.0137\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2dfRKyWUMWc"
      },
      "source": [
        "# Task ID14: \t168332\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ead5a2e-bb15-4581-e1fd-c1e7dbc02eec",
        "id": "cqS6ba8QUMWd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10000, 7200)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(168332)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "3812e2ba-6210-4aa6-f19d-b958de84d39e",
        "id": "ihiqQwXxUMWd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-14973b8f02ee>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Choose and run models with 10-fold cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_xgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-0a1506be2bd3>\u001b[0m in \u001b[0;36mrun_xgboost\u001b[0;34m(X, y, seed, task_type)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtask_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classification\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'neg_root_mean_squared_error'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"XGBoost with random params: {params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    424\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             )\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1532\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1730f05a-ba0a-4508-a15e-ce90185f4d3c",
        "id": "7HenuCLoUMWd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 4}\n",
            "10-fold CV accuracy: 0.4279  0.0751\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp8aKwahUNE1"
      },
      "source": [
        "# Task ID15: \t168337\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ccd49b-cc14-434d-9ef9-67bb70ef3375",
        "id": "y7_owBk7UNE1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (20000, 4296)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(168337)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7edcde1-eaec-4383-fecd-3b5e732603dc",
        "id": "f60XHp_lUNE2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x781c108b4400>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 582, in _next_wrapper\n",
            "    def _next_wrapper(self, this: None) -> int:  # pylint: disable=unused-argument\n",
            "KeyboardInterrupt: \n",
            "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x781c108b4a00>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 582, in _next_wrapper\n",
            "    def _next_wrapper(self, this: None) -> int:  # pylint: disable=unused-argument\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1730f05a-ba0a-4508-a15e-ce90185f4d3c",
        "id": "D8zN2Xq5UNE2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 4}\n",
            "10-fold CV accuracy: 0.4279  0.0751\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "065Jhn3bUN3E"
      },
      "source": [
        "\n",
        "# Task ID16: \t146212\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f8d81b-37d6-4a97-b5f5-2c0cab1bd115",
        "id": "4rzPoKR9UN3F"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (58000, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146212)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e085ecad-33b4-4cfe-f59e-1158f9133828",
        "id": "gFEQaVoKUN3F"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9999  0.0001\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282654e6-8c33-40f1-9364-74d67f7b5987",
        "id": "i3cfpiDUUN3F"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 7}\n",
            "10-fold CV accuracy: 0.8166  0.1577\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El_-VFlJUO4I"
      },
      "source": [
        "# Task ID17: \t168329\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6196754b-0fab-47bb-81c6-f63e2f4b508f",
        "id": "h-YYvD2zUO4I"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (65196, 27)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(168329)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "ae1d6724-8e03-42dc-acfb-cb84e8f1e531",
        "id": "PU0jRVc8UO4J"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-14973b8f02ee>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Choose and run models with 10-fold cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_xgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-0a1506be2bd3>\u001b[0m in \u001b[0;36mrun_xgboost\u001b[0;34m(X, y, seed, task_type)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtask_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classification\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'neg_root_mean_squared_error'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"XGBoost with random params: {params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    424\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             )\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1532\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1730f05a-ba0a-4508-a15e-ce90185f4d3c",
        "id": "m54_bMpsUO4J"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 4}\n",
            "10-fold CV accuracy: 0.4279  0.0751\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHX_XapBUPmH"
      },
      "source": [
        "# Task ID18: \t168338\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c7b8f4-9314-488b-aec3-04731068cde2",
        "id": "k1T-LlGFUPmH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (20000, 4296)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(168338)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "2e669eef-d98a-4b25-869b-fab86be50f25",
        "id": "OO8zhzG8UPmI"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-14973b8f02ee>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Choose and run models with 10-fold cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_xgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-0a1506be2bd3>\u001b[0m in \u001b[0;36mrun_xgboost\u001b[0;34m(X, y, seed, task_type)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtask_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classification\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'neg_root_mean_squared_error'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"XGBoost with random params: {params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"10-fold CV {scoring}: {scores.mean():.4f}  {scores.std():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    424\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             )\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1532\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1730f05a-ba0a-4508-a15e-ce90185f4d3c",
        "id": "EskmbU0CUPmI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 4}\n",
            "10-fold CV accuracy: 0.4279  0.0751\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPP_6tsjUQVe"
      },
      "source": [
        "# Task ID19: \t168868\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "705b3fd7-ca13-41f1-86db-6b6c822ac1c3",
        "id": "dok7f3StUQVe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (76000, 170)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(168868)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13688946-de08-4068-90a0-78eb4871fe7c",
        "id": "gkMr7IkiUQVf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9944  0.0007\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "205bf18c-ad57-45a3-a3da-2ad8d6265b92",
        "id": "7E4ogYCoUQVf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9757  0.0072\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID20: 31"
      ],
      "metadata": {
        "id": "O-_L6JbmmBG-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e6c8b2-3039-403e-93a9-454b0276a739",
        "id": "8TZ8xFhm5apg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1000, 20)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(31)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a15c2dc-92ca-410e-c809-abf1e8ce46bc",
        "id": "s9Bq8YQy5aph"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7560  0.0310\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4e40b2-1a45-4a5f-e91c-bd442d291e63",
        "id": "qt8az3t05api"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.7030  0.0650\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID21: 10101"
      ],
      "metadata": {
        "id": "pz87BwJOmBG_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0210667b-bdb5-410b-d648-433eb2314686",
        "id": "yR8sUbo35baK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (748, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(10101)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeeff68f-0bc6-4949-9f6a-b1c29b5f7b5a",
        "id": "ZdE8RlL_5baL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7127  0.1362\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab23e316-3683-4e8d-a7c8-28a80a37381d",
        "id": "jnxsHb3d5baL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.6781  0.1610\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID22: \t3913"
      ],
      "metadata": {
        "id": "9Bpvk5sxmBHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "293b0559-e618-47c3-b548-7c373203da75",
        "id": "e00vfw6v5cLi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (522, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3913)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed4b8e2-6c63-4613-d0d9-a0fef5381ba9",
        "id": "jWhbFMkE5cLi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7966  0.0775\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc892866-d77a-426a-8286-ce1045166f81",
        "id": "YEj4gf5i5cLj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.7910  0.1004\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID23: 3"
      ],
      "metadata": {
        "id": "oiMkFgpfmBHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e4279d-5b3c-4c64-ba9d-d064f83310be",
        "id": "3alW-Yl45c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3196, 36)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7839c94-1813-487a-9b53-b2a4eea70d23",
        "id": "NilvDT0z5c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9687  0.0289\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f07410-fb23-4284-b4bb-5158e0cf8549",
        "id": "nKwbYrAi5c8-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8393  0.1783\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID24: 3917"
      ],
      "metadata": {
        "id": "FEifXxy8mBHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858a2ee9-a0b3-4267-a9a8-872bd8d7a41e",
        "id": "6xtOEagi5dnT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2109, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3917)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b86fd7-a409-42c5-c2ea-8b7c18e78305",
        "id": "7oRmZvhE5dnT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8308  0.0354\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a774c973-1f64-40cd-912d-434a6a92a872",
        "id": "spi5eDKh5dnU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.7942  0.0312\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID25: 9957"
      ],
      "metadata": {
        "id": "Xn3zAw8EmBHB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c052b5c1-f9e2-4f7b-8b1b-bed3c6a5e8e5",
        "id": "bMHDvpYj5eSD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1055, 41)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9957)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e287a3-bae9-4609-b789-5368a4ecb6c0",
        "id": "Sif5HJXI5eSE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8511  0.0545\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933478a0-ccdb-49bb-cad8-a1a396c7cefe",
        "id": "76eAyHlf5eSF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8019  0.0565\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID26: 9946"
      ],
      "metadata": {
        "id": "5rgMGgZ9mBHB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18c70c0-7fe7-464c-fe21-f0b3b5ccd523",
        "id": "GBcVuKGe5e-z"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (569, 30)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9946)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d62ed6f-182b-450f-b6fa-1819fce2f4c0",
        "id": "QKBkoCKh5e-0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9701  0.0208\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9525e4f8-6e5b-4638-b9a3-ea462720e9b8",
        "id": "LILm3PV65e-0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9526  0.0272\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID27: 3918"
      ],
      "metadata": {
        "id": "FbvRMHbYmBHB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ab832b-4872-43c3-b734-02eae58189eb",
        "id": "ExKdD8euwWUW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1109, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3918)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb38218-d8c9-422d-e83e-78e6ea1ea684",
        "id": "Sp6lTzp1wWUX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9297  0.0311\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fb2e6a-e20a-4aed-e9ce-06b14d4bbd97",
        "id": "o7PUeOslwWUX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8765  0.0353\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID28: 3903"
      ],
      "metadata": {
        "id": "OdlGguSsmBHB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab2ac09-7110-4a4e-e9b1-3f230b6a6bc2",
        "id": "U6URxEk1wXxA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1563, 37)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3903)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a6fd6b-fed8-46ac-e473-ebc144bb9edb",
        "id": "66dura9DwXxB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8925  0.0144\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc981c9-ed59-46ef-e7f5-fc78eeabca4d",
        "id": "-rnzYjY8wXxB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8421  0.0472\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID29: 37"
      ],
      "metadata": {
        "id": "gATHwsDVmBHC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831a1d40-137e-4cda-859a-1368e2de7acb",
        "id": "B8frlh8bwYd_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (768, 8)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(37)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4d8a89-9ec0-430a-aa92-30ba89b1c040",
        "id": "0l2KHSODwYeA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7578  0.0416\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f86080-fa1b-4c1c-93a8-7e5648badf94",
        "id": "FE6cj0rhwYeA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.7005  0.0261\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID30: 9971"
      ],
      "metadata": {
        "id": "JwiID8vDmBHC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346aa640-ffbb-480a-d073-3b50314edb06",
        "id": "Vqw71yzKwZDS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (583, 10)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9971)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f67e7f-416d-45ed-8cae-1c992fb9d9e9",
        "id": "h7v27OKIwZDT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.6793  0.0824\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033af2f2-0358-4d5e-ea43-c2c3d4064f94",
        "id": "F8cH0ysuwZDU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.6211  0.0865\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID31: 9952"
      ],
      "metadata": {
        "id": "EGn0RDNtmBHD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58a1579-a841-425a-c201-0691be98605c",
        "id": "tgDTgNHjwZsv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5404, 5)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9952)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa48e24-15bd-41c7-8a9c-be1b9a9e2355",
        "id": "6-LOOwCYwZsw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8769  0.0154\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7a2be6-9cbf-482b-bdfc-9d1c79653bf5",
        "id": "hbJN0QLxwZsw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.7089  0.0997\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID32: 3902"
      ],
      "metadata": {
        "id": "slnLEXhcmBHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e15cc5-735b-40ee-a9d0-2c9cf52487cb",
        "id": "Zw8e2qiYwaQ_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1458, 37)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3902)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d4f5c4-a209-4bd2-c61d-875bcef879cd",
        "id": "Y70V_cyLwaRA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9108  0.0126\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6be22f-79c5-4024-fa91-6143fedde00b",
        "id": "rRpDJSZawaRA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8560  0.0283\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID33: 49"
      ],
      "metadata": {
        "id": "6KjH3nJAmBHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509f422d-bd4f-497f-da5a-ad16de263ae6",
        "id": "GBfch12dwayo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (958, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(49)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89be119-8d2c-4823-94aa-a8689e09fc3d",
        "id": "fTfa7ekmwayp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8779  0.1370\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ec3f82-add7-464d-e9e2-105659a125db",
        "id": "WNX4bvxkwayp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8237  0.1105\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID34: 43"
      ],
      "metadata": {
        "id": "LU0S3n4smBHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df03dd47-7468-4c9d-a4e6-4c99e1f8120c",
        "id": "rokKEK3kwbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (4601, 57)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(43)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbf3619-b99c-4200-881a-f475fb081b3c",
        "id": "GrmM4Uhywbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9422  0.0324\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1fa0582-4bc4-4a08-d08a-bef3fc9ec9e3",
        "id": "2sWLswB6wbb-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8111  0.0454\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID35: 9978"
      ],
      "metadata": {
        "id": "hj-rIRL2mBHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a6d675-ef87-40d5-bc75-c7f182113c85",
        "id": "Jgc3KmWrwcJS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2534, 72)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9978)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ba43d4-f6ab-4a45-de0f-73d0b1ecc06b",
        "id": "iMuWm-LFwcJT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9325  0.0174\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc8b8cc-68c6-4a78-9f1b-00d9b1d24549",
        "id": "h1CGxB-gwcJT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9069  0.0168\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID36: 10093"
      ],
      "metadata": {
        "id": "-JwSG_1umBHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17843b26-8e63-4b41-9a21-722fd7fd8f71",
        "id": "GNtXEDfqwcwA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1372, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(10093)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096b8b55-0338-488a-f4b5-06b4d21248fd",
        "id": "kZkktAudwcwA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9956  0.0048\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b322808-4bbf-467c-c32c-e6434f2ba69e",
        "id": "foMATdh-wcwA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9818  0.0188\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID37: 219"
      ],
      "metadata": {
        "id": "hjiZ3dJemBHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afdd88ce-5117-43ca-d952-664f8b5ff907",
        "id": "-eSJSjXdwdPp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (45312, 8)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(219)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9b19cd-07bd-4797-8ef8-354838a3b8b4",
        "id": "jMzLsea_wdPq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7299  0.0614\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738fce51-5882-4e0e-9006-0bdfae925db3",
        "id": "jgRLLEeYwdPr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.7130  0.0965\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID38: 9976"
      ],
      "metadata": {
        "id": "EcoJTJdumBHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e00e1a2-92ad-498e-dd42-04340062b661",
        "id": "K19VviThwdvH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2600, 500)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9976)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b1f13c-1e32-4bc2-d67b-440b98cddca9",
        "id": "WNPNKzk5wdvI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7835  0.0221\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aca81a7-67f0-4b69-bc15-027653fdda14",
        "id": "uBvk8AguwdvI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.6935  0.0315\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID39: 6"
      ],
      "metadata": {
        "id": "K5IeY4InmBHG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a9df55-260d-45b7-e222-43f741e7cdb0",
        "id": "o1ZJHYegweW6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (20000, 16)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(6)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8f3157-63ea-4e97-d64e-689404e48429",
        "id": "u6EXgMb0weW7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9321  0.0028\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4404f0f7-ffc8-4cbc-c868-d22681114337",
        "id": "lyaq7HMgweW7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 26}\n",
            "10-fold CV accuracy: 0.1738  0.0415\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID40: 53"
      ],
      "metadata": {
        "id": "GUDpZSyimBHG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98afd84b-5973-4c69-9a1d-f3d28c0b2a68",
        "id": "Sm36nZQ2we-2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (846, 18)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(53)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0fd8810-b710-4773-b64e-56813fb36f83",
        "id": "mm9NLYgowe-2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7696  0.0277\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf75e8e-f86b-4d28-9014-12b4bcc64dd4",
        "id": "_9ZT6NG-we-3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 4}\n",
            "10-fold CV accuracy: 0.4054  0.0701\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID41: 11"
      ],
      "metadata": {
        "id": "ORJetoWwmBHH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca97dfe-d3de-4895-b7d2-4ffa00a5b718",
        "id": "5PQ4t6sLwfpH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (625, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(11)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8898f16a-ddd4-4d65-db5d-ea3c0a462883",
        "id": "5YoCp-A_wfpH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7426  0.1043\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d67e7c-bd8b-415b-d306-29ea5104d134",
        "id": "LE13K2GawfpH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 3}\n",
            "10-fold CV accuracy: 0.8131  0.0642\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID42: 15"
      ],
      "metadata": {
        "id": "3r0v1nHtmBHH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a8c923-3eb3-4866-9bf2-7068e2089ec2",
        "id": "Ti-DYZpOwgPU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (699, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(15)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21236a52-ea0a-42b7-95b1-62c98a81de33",
        "id": "zo7TbQN9wgPV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9586  0.0322\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21eb785-af01-43bf-c8d7-69fc449d7e32",
        "id": "0hKDRKTYwgPV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9529  0.0313\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID43: 16"
      ],
      "metadata": {
        "id": "F_sPWD7VmBHH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265d7a69-0422-4555-bed6-0e29c0f2167e",
        "id": "588Qkb3Cwg1g"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 64)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(16)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848abe62-a9ef-466e-eb06-757e8b8e2807",
        "id": "5qBRBTazwg1h"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9460  0.0118\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60de92f-281f-467c-aa55-62b3d3f54b60",
        "id": "yU0JVWhPwg1h"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.3185  0.1023\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID44: 14"
      ],
      "metadata": {
        "id": "BWKd79aMmBHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69158e1d-f821-4a53-8c80-431b8a2e748c",
        "id": "AVVbmLBPwha_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 76)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(14)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4c9ee7-3d3e-44db-da08-0f5ec9e56b41",
        "id": "qxTFYXPqwhbA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8380  0.0176\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351a8cbd-ecf3-4a8e-af28-59572b51379c",
        "id": "VTLdZiWtwhbA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.2800  0.0680\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID45: 32"
      ],
      "metadata": {
        "id": "wWcMGCecmBHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9b40c8-df55-4dcf-b8f7-0b827fe0db26",
        "id": "5BBqmmc-wh8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10992, 16)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(32)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b395952-dbd3-4577-b48c-536d7871a5b3",
        "id": "fQaW_5wPwh8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9889  0.0031\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1fc40f0-9a22-4175-bd5d-d0f844e918c5",
        "id": "bGbJ9_9vwh8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.3439  0.1041\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID46: 3549"
      ],
      "metadata": {
        "id": "T2P3HSBRmBHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39bcb263-cb95-4d4d-a57d-0e8a3bd6bd54",
        "id": "TCysmMaQwinB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (841, 70)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3549)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fecd0f3b-fcb6-41e6-d9d2-359c1fa62edd",
        "id": "Tv2T-NO7winB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9857  0.0139\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5dbd03e-3f96-4abe-8f8b-cf99cf3fe3e2",
        "id": "VFRzCgtiwinB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 4}\n",
            "10-fold CV accuracy: 0.9013  0.0547\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID47: 12"
      ],
      "metadata": {
        "id": "4AFJCI0GmBHJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef71881d-d50d-40a0-f9dd-6cc4dbc0bde7",
        "id": "m8F2p8RawjM3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 216)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(12)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca26dfc7-fae0-4b09-b956-88f9a3b3e031",
        "id": "I52Xc-c5wjM4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9600  0.0140\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289ab9c7-0842-49a8-bedf-f618bee6a8c7",
        "id": "w6b8tBqrwjM4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.4495  0.1933\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID48: 9981"
      ],
      "metadata": {
        "id": "hFq5AolmmBHJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d5b0906-ead9-4651-f50c-5c065445fea6",
        "id": "mcO-_Y11wj0r"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1080, 856)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9981)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ee6e7b-f714-4588-de65-edfe4450c3e2",
        "id": "_DUJvLtRwj0s"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9176  0.0308\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b412a966-2b23-4154-8ac1-e2d25561f2f2",
        "id": "LuU4a3jpwj0s"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 9}\n",
            "10-fold CV accuracy: 0.1278  0.0522\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID49: 18"
      ],
      "metadata": {
        "id": "ZFdnAmLumBHJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734639c1-12a6-4dbf-e605-f4a2cd7080ad",
        "id": "eYwoVyKwwkaH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 6)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(18)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a25ac5-a181-4963-9d3b-3f32b7680881",
        "id": "hi5IYsBUwkaH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7065  0.0166\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74489192-5a9e-46b1-9c14-6b78d32ebab3",
        "id": "8Zb4FXMTwkaH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.2690  0.0521\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID50: 28"
      ],
      "metadata": {
        "id": "jaby8dPEmBHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da871c7-1614-4eb6-d923-5a1d918c6669",
        "id": "c-ku6pjQwlB2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5620, 64)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(28)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99990e9c-889f-4f80-b315-e2ba0cb6608a",
        "id": "YKPGOXH_wlB2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9733  0.0092\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ce79b2-dc7d-464f-a6a2-e0fca31477d4",
        "id": "wrAfVCgcwlB3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.3534  0.1022\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID51: 2074"
      ],
      "metadata": {
        "id": "3s7rOJSCmBHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb1e4f7-effe-4639-f246-179e1b065961",
        "id": "B9JLM74dwllp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (6430, 36)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(2074)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c5be77-5462-4674-e3bc-903eeaa58239",
        "id": "Tg3GT4Azwllp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9106  0.0123\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b372be-aceb-45fc-b189-c4a17a33d116",
        "id": "ru725ecPwllq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 6}\n",
            "10-fold CV accuracy: 0.3664  0.0641\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID52: 29"
      ],
      "metadata": {
        "id": "FopEarjwmBHL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d467b2cd-0b2e-4e0f-c1e6-45812290c046",
        "id": "ac0IWRbDwmGC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (690, 15)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(29)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede0ec79-6fd6-4732-d791-0446910430c6",
        "id": "FEXyhZstwmGD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8391  0.1284\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b5c6bf-5fa7-459c-bbea-07049275dedc",
        "id": "tDTSM8SswmGD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8087  0.1184\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID53: 45"
      ],
      "metadata": {
        "id": "EpFLrNEsmBHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cde46e-f7c4-4abe-b98f-1f58331716bf",
        "id": "bx1pupt5wmui"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3190, 60)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(45)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0123aee-9c89-4358-9208-47d0e14681ea",
        "id": "x7KXC_4gwmui"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9583  0.0141\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d49e8b-9f1d-45f3-ec7e-ed13a1cee658",
        "id": "jFr5f6NZwmui"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 3}\n",
            "10-fold CV accuracy: 0.5639  0.0946\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID54: 125922"
      ],
      "metadata": {
        "id": "3UmvBQKfmBHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9a7885-85db-4c8c-9bb5-1f11dcfc3968",
        "id": "Pl-YLqElwnZ3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5500, 40)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(125922)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf59a5e-fec3-489c-a903-08c5f0272b3d",
        "id": "FlytFkuqwnZ3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9842  0.0056\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d784d0fe-66f4-4a54-83d7-7b04caa703db",
        "id": "5-l6SN4qwnZ4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 11}\n",
            "10-fold CV accuracy: 0.4633  0.1052\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID55: 9960"
      ],
      "metadata": {
        "id": "LV_w0h47mBHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bd3114-49b8-44d9-de3c-aec3c2443b87",
        "id": "0WhWg2u6woDO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5456, 24)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9960)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ee8a29-57c8-4ada-ebba-2f520ea49004",
        "id": "BCgSiCSswoDP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9941  0.0086\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ff7089-8c63-49de-aa97-0f5fc5392228",
        "id": "d3xRFQDUwoDP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 4}\n",
            "10-fold CV accuracy: 0.7197  0.2254\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID56: 9964"
      ],
      "metadata": {
        "id": "OXWTMs1fmBHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59f3937-3976-4532-d2a5-3f0193d23374",
        "id": "16Ji3y5xwoo5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1593, 256)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9964)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3cc47f-e155-4b63-c8c4-74e1e74a3a38",
        "id": "12YFos41woo5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9272  0.0229\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ddc21b-bbde-4fe0-bc2c-223601fd2932",
        "id": "RcUxp526woo6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.3132  0.0912\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID57: 22"
      ],
      "metadata": {
        "id": "Vy4T7mNdmBHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab3b6f5-a202-4819-c041-4af7e7302d9f",
        "id": "SYxJ8YhawpPX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 47)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(22)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56971c03-8de2-451c-e819-b0bf0eb9588b",
        "id": "8k_L_gpOwpPY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7790  0.0221\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401dc1b7-9b66-4234-d741-df330888f6a8",
        "id": "ayhca6s_wpPY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.3195  0.0914\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID58: 2079"
      ],
      "metadata": {
        "id": "n4bdxcPDmBHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5a78bf-27da-4f79-a3a9-28ff65539b67",
        "id": "7Y2Wc13cwqA2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (736, 19)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(2079)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c9a0fa-dc40-4025-bf42-89acdd637af9",
        "id": "S6O-NA4RwqA3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.5601  0.0555\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74c9da6-0db0-44d2-eb52-8eaab1fe3812",
        "id": "HR9arIImwqA3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 5}\n",
            "10-fold CV accuracy: 0.2785  0.0715\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID59: 14969"
      ],
      "metadata": {
        "id": "cLJAY-VHmBHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd371f92-d95f-47cc-d36b-0c2ccd272d52",
        "id": "19zVLj0Vwqo4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (9873, 32)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(14969)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ca7a4a-0230-45b7-d2bf-ecd07f38c5cb",
        "id": "dlZZlpnAwqo4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.4783  0.0596\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc788762-e435-4729-f9e8-bcf2e4cc2af2",
        "id": "8gZbPycfwqo5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 5}\n",
            "10-fold CV accuracy: 0.2969  0.0562\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID60: 3560"
      ],
      "metadata": {
        "id": "_USMYh_ymBHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5b0053-4e6f-4c66-e4ae-c077b50ca1f9",
        "id": "Je8UjsuTwrNo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (797, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3560)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fddfe171-bdaf-46a2-de1e-e24a82ba009c",
        "id": "QefOaEESwrNp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.1896  0.0429\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3049ae63-dbe1-4799-fac1-f48e35a01767",
        "id": "XtaTmRiEwrNp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 6}\n",
            "10-fold CV accuracy: 0.1595  0.0293\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID61: 14952"
      ],
      "metadata": {
        "id": "igtVIt8xmBHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2735c9f-cb4c-43cd-e637-0a46d74fc433",
        "id": "er6TghQQwr0A"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (11055, 30)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(14952)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ef551f-e92c-4498-9d9b-0cbe1b78a83b",
        "id": "ASWylEDQwr0B"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9557  0.0059\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ae82fd-35fb-4801-cb78-9dc58fa01ee7",
        "id": "L64Twxjrwr0B"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8177  0.1445\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID62: 125920"
      ],
      "metadata": {
        "id": "xvQQw9zSmBHP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74d22b9-3950-406c-fc63-ad5e69f2df23",
        "id": "giuN-fahwsbP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (500, 12)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(125920)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fb5ccd-b3ea-4f6e-9265-7609c709d2d5",
        "id": "HLRB_5r3wsbQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.6020  0.0433\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb966ca3-8e06-4ac6-860e-150791d4a8c4",
        "id": "X4xmdFt1wsbQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.5620  0.0745\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID63: 23"
      ],
      "metadata": {
        "id": "qJt1tbHSmBHP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5964ba6d-ef69-494a-df29-73473d150653",
        "id": "arW315JiwtDM"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1473, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(23)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3929d6fc-555c-4465-c1ca-7cfba95dcb4e",
        "id": "dDfjziIUwtDM"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.5601  0.0400\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd167895-82d6-42f2-dd1b-fbaca83d9e5c",
        "id": "6sz56LbywtDM"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 3}\n",
            "10-fold CV accuracy: 0.4351  0.0485\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID64: 3904"
      ],
      "metadata": {
        "id": "565cLTQYmBHP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6e12ec-d609-4e77-db90-fff762b30049",
        "id": "dM4q1B_iwtoQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10885, 21)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3904)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef73b85-ebe9-45f6-b93f-fe0e9c2fda15",
        "id": "nfXVxgF0wtoR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8066  0.0187\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be932f0e-fda0-4604-f2cb-b2960847ee3c",
        "id": "KoveAJJHwtoR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.7150  0.0319\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID65: 3022"
      ],
      "metadata": {
        "id": "lgNGHeQTmBHP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bdb683-64e3-47da-bc46-9e6fe51217a5",
        "id": "5ZvMq4cQwuPC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (990, 12)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3022)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4f94d0-67b2-416b-afc1-87db0d3b1174",
        "id": "Pw80bvxgwuPE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.6545  0.0697\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb9830c-6579-4252-ede5-c07a9acd4163",
        "id": "SqMJcl7hwuPE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 11}\n",
            "10-fold CV accuracy: 0.1808  0.0672\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID66: 9985"
      ],
      "metadata": {
        "id": "xN6To_72mBHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3970c3-cc0b-414c-e51c-b005c30f78ef",
        "id": "LAsdLz4Bwu1T"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (6118, 51)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9985)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a07571-4878-42cb-f835-20a2c38304ea",
        "id": "9B73g_XCwu1T"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.5414  0.0602\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47dd207f-a65a-4ebf-a770-ea9a515117bf",
        "id": "6hm1bCHTwu1U"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 6}\n",
            "10-fold CV accuracy: 0.2517  0.1198\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID67: 9910"
      ],
      "metadata": {
        "id": "Xy_Hb65JmBHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07aad0e9-2c36-4fac-ba40-7f12f43fa11a",
        "id": "jr1oz4QNwvcx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3751, 1776)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(9910)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff125fac-87e6-4c17-d98d-ea3e06d50101",
        "id": "kGB2ptIbwvcy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7974  0.0180\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f18751-1f19-46ad-a11b-791a1e19c53a",
        "id": "RvYnP5V9wvcy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.5692  0.0653\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID68: 14970"
      ],
      "metadata": {
        "id": "VpwVoqHWmBHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29e0ee1-eb72-48fc-a76e-7745a190686a",
        "id": "Z_bmTKsxwwCh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10299, 561)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(14970)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cbdfb5-ac8b-4705-c6ca-7ef53026fa44",
        "id": "NwVj_39SwwCi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9555  0.0293\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c03a1da-a67a-43cb-c44d-d098f1993af2",
        "id": "bT4vQfZ0wwCj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 6}\n",
            "10-fold CV accuracy: 0.4214  0.1325\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID69: 3021"
      ],
      "metadata": {
        "id": "fRAZSw9SmBHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc9eb01-fb2e-4a0b-e515-5a80806ae9bf",
        "id": "iJYTupjFwwrf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3772, 29)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3021)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7ac5a4-2faa-421f-cb40-e1f081a1ba9e",
        "id": "neVS0pO-wwrg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9913  0.0052\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf963bd-3b94-4fd6-c14b-3145a1f575b3",
        "id": "84JP_G7Ewwrg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9327  0.0411\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID70: 3481"
      ],
      "metadata": {
        "id": "tPMAoExOmBHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5171fb-ae2b-44ae-c914-b551331d6095",
        "id": "8bJjCfChwxTf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (7797, 617)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3481)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41426e2b-475a-4128-d884-ad4763fa67ea",
        "id": "x8bakb4DwxTg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9478  0.0121\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d95ee52-7676-4bee-a770-17eaa2bf8c55",
        "id": "yAT5c_ohwxTg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 26}\n",
            "10-fold CV accuracy: 0.2706  0.0419\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID71: 3573"
      ],
      "metadata": {
        "id": "1BPz9w1fmBHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a537eb-bf81-4a51-97dd-b18020017c45",
        "id": "wPg4qvabwx5B"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (70000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(3573)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwZBBM71wx5C"
      },
      "outputs": [],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB4Tp1pYwx5C"
      },
      "outputs": [],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID72: 146824"
      ],
      "metadata": {
        "id": "ilhYkYHhmBHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb36eef-ca47-4cca-86e4-a1203142dfae",
        "id": "S5-VFQB2wyZ6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 240)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146824)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af5972f-36d6-474a-dfa8-a4d94785a265",
        "id": "2_6SKWadwyZ7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9630  0.0131\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c623e1d2-7278-4574-af2f-3fe0eda42f58",
        "id": "M4uysn3DwyZ7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 10}\n",
            "10-fold CV accuracy: 0.3550  0.0862\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID73: 146820"
      ],
      "metadata": {
        "id": "gAe_DvSymBHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7daa2fc-34cc-45a8-c6ca-a8df0a80c04d",
        "id": "YwEshIVDwzAn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (4839, 5)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146820)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797b2d57-dbb8-4798-8faf-7f062c08bea4",
        "id": "HEgczZf1wzAn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9845  0.0111\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8615fdea-0a39-45c1-a003-f6949c913f08",
        "id": "guPLAkWNwzAn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9653  0.0158\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID74: 146822"
      ],
      "metadata": {
        "id": "otn0k5RLmBHS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9008809d-1e27-4742-848b-86558998b9ce",
        "id": "s3R6BoikwzlY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2310, 16)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146822)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d160be2-93e6-4e2b-c7d3-8aab34e67736",
        "id": "GkNXjqjIwzlY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9342  0.0116\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a62bf34-a54c-4030-f817-9a9830dc8ec9",
        "id": "L-yrz8uCwzlZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 7}\n",
            "10-fold CV accuracy: 0.4701  0.1103\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID75: 146195"
      ],
      "metadata": {
        "id": "Iw8o4k8ymBHS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3420e2-0723-4814-90d3-8072d6f89136",
        "id": "SjW-_fUrw0I6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (67557, 42)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146195)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee08cbb-c8fd-4eeb-b7a5-b7cb80aac9ce",
        "id": "7T-C3S6Zw0I7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.6903  0.0501\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9c4527-c75b-42d6-dfe6-9db896b669cd",
        "id": "wJemZFFcw0I7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 3}\n",
            "10-fold CV accuracy: 0.4384  0.1732\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID76: 146800"
      ],
      "metadata": {
        "id": "1G3oYlqFmBHS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0207fb8e-de78-4e1f-eacf-06b00ea4275a",
        "id": "scMY-T4iw0us"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1080, 77)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146800)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11d8fc2-85b8-4501-8ce9-e2474fbe0228",
        "id": "s3v2AWWyw0us"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.7093  0.1207\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff84c7f5-98b9-4cf9-9329-07dc37cb12c3",
        "id": "DzdkGHRaw0ut"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 8}\n",
            "10-fold CV accuracy: 0.3481  0.1663\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID77: 146817"
      ],
      "metadata": {
        "id": "tqXwVXIsmBHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030eb24a-092a-4006-b89a-8403151333e0",
        "id": "or7Ji_uEw1Tv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1941, 27)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146817)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a607144-e047-4eef-c938-e219d22a22fb",
        "id": "3NzX5mQOw1Tv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.6353  0.0864\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f661dbb-dd83-4c19-ddd1-5b51ec2a3948",
        "id": "2fVYk7lIw1Tv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 7}\n",
            "10-fold CV accuracy: 0.2788  0.0751\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID78: 146819"
      ],
      "metadata": {
        "id": "NB1riqQEmBHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe925d8-fd05-4bcc-91d6-f2ff75200898",
        "id": "6FI3x_PZw15A"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (540, 18)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146819)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50707ba6-f880-471b-d617-37a6ce380dd4",
        "id": "mxXMrxIDw15B"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9463  0.0255\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55995d5c-9ddc-4506-c5d4-84a525442675",
        "id": "0ZmLl4ycw15B"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.9370  0.0343\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID79: 146821"
      ],
      "metadata": {
        "id": "-YCoSNrTmBHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f658884b-35b3-4279-88c2-cd2e115295b1",
        "id": "c-LC61L5w2YK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1728, 6)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(146821)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3497db5-1516-44b6-f066-f7d83993bb1d",
        "id": "FYbLXGyGw2YL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.8773  0.0711\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7c4697-0b6b-45ce-b47f-47aa27f45758",
        "id": "c6XmLTnqw2YM"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 4}\n",
            "10-fold CV accuracy: 0.5679  0.1789\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID80: 14954"
      ],
      "metadata": {
        "id": "lqGNYrKZmBHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7041eba0-7d54-4968-f46c-4228eed0a07d",
        "id": "nw85z10Lw229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (540, 37)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(14954)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46cd57c2-ffc9-446e-e194-80700775e077",
        "id": "a7EUcggsw22-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.6241  0.0837\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453b7e3d-b625-471c-98ee-3d2a1025183f",
        "id": "g8Xk2V3tw22-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.5648  0.1022\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID81: 167141"
      ],
      "metadata": {
        "id": "QwQ6aFMZmBHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704a6bc2-6333-42a0-fdaf-195da15d8843",
        "id": "QqUNfuKQw3Y9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (5000, 20)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(167141)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220729de-7d30-45b1-d9b7-6f3bc8624480",
        "id": "6zKr1hamw3Y9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9518  0.0099\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9689c3-6285-40de-ae71-e4ae30a801eb",
        "id": "m9u9r1zcw3Y9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.7800  0.0865\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID82: 167140"
      ],
      "metadata": {
        "id": "wSXbi1pgmBHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50196c9a-4955-4ea2-da3e-93bb28dba409",
        "id": "kWy_7jtnw3-y"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3186, 180)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(167140)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74ef2e9-443b-417f-a6c0-caed8ccd05db",
        "id": "sAi6SOSfw3-z"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9601  0.0113\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb7a042-0b1e-4043-a86f-a42145be2ee7",
        "id": "XgAGzL6Sw3-z"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'multiclass', 'num_class': 3}\n",
            "10-fold CV accuracy: 0.5577  0.1720\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID83: 167125"
      ],
      "metadata": {
        "id": "Hys7nMbjmBHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c916ecdd-242f-4293-8e10-39bcec810e2e",
        "id": "NYTLpW5ew4hv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3279, 1558)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(167125)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e96e5c3b-610b-46b2-9a71-aa6e3325c2e7",
        "id": "BNB8II8Bw4hw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with random params: {'max_depth': 4, 'alpha': 0.4033800832600378, 'lambda': 0.00717714192799201, 'eta': 0.11906379991333114}\n",
            "10-fold CV accuracy: 0.9695  0.0201\n"
          ]
        }
      ],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9fca700-aa2a-487a-fbb6-30ad88dc68f3",
        "id": "QSeUVCi8w4hw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM with random params: {'num_leaves': 35, 'lambda_l1': 3.6010467344475314, 'lambda_l2': 0.038720902953704145, 'learning_rate': 1.8754120723565242, 'objective': 'binary', 'num_class': None}\n",
            "10-fold CV accuracy: 0.8750  0.0508\n"
          ]
        }
      ],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID84: 167124"
      ],
      "metadata": {
        "id": "IUQ2FlaOmBHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b147fc81-a519-4042-b01e-b39fc02f35e1",
        "id": "bYKPnPIxw5BK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (60000, 3072)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(167124)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjs0BIRJw5BL"
      },
      "outputs": [],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKlOYgLAw5BL"
      },
      "outputs": [],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID85: 167121"
      ],
      "metadata": {
        "id": "ZuFFDV-umBHV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e02dd7e-618e-457f-eb78-041a1bf83293",
        "id": "bUW9qrI9w5gS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (92000, 1024)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data using OpenML Task ID\n",
        "X, y, cat_features = load_preprocess_task(167121)  # Replace with an actual task ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhqvYFrgw5gT"
      },
      "outputs": [],
      "source": [
        "# Choose and run models with 10-fold cross-validation\n",
        "run_xgboost(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFpslBasw5gT"
      },
      "outputs": [],
      "source": [
        "run_lightgbm(X, y, seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "TExf50FwwwrX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYUT4Lc1PdJI",
        "outputId": "e2770aac-60f3-471d-e853-fc4859d82328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import openml\n",
        "import pandas as pd\n",
        "\n",
        "# Function for random parameter generation\n",
        "def get_random_parameters(seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"hidden_dim\": rs.randint(10, 101),\n",
        "        \"n_layers\": rs.randint(2, 6),\n",
        "        \"learning_rate\": rs.uniform(0.00005, 0.0005),  # Adjusted lower learning rate range\n",
        "    }\n",
        "    return params\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load data from OpenML using task ID\n",
        "def load_openml_data(task_id):\n",
        "\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Convert categorical features to numeric using one-hot encoding\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    # Fill NaN values with column means and Inf with large finite values\n",
        "    X = X.fillna(X.mean())\n",
        "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    X = X.dropna()  # Optionally drop remaining NaNs if any\n",
        "\n",
        "    # Convert to numpy\n",
        "    X = X.to_numpy().astype(np.float32)\n",
        "    y = pd.factorize(y)[0]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Define MLP model\n",
        "def initialize_mlp(n_layers, input_dim, hidden_dim, output_dim, objective):\n",
        "    layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
        "    layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU()] * (n_layers - 1))\n",
        "    layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "    model = nn.Sequential(*layers)\n",
        "    return model\n",
        "\n",
        "# Training function with gradient clipping\n",
        "def train_mlp(model, X_train, y_train, X_val, y_val, args, params):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=params[\"learning_rate\"])\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=args[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
        "    val_loader = DataLoader(dataset=val_dataset, batch_size=args[\"val_batch_size\"], shuffle=False)\n",
        "\n",
        "    for epoch in range(args[\"epochs\"]):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_X)\n",
        "            loss = loss_func(output, batch_y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Apply gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                output = model(batch_X)\n",
        "                val_loss += loss_func(output, batch_y).item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Prediction function\n",
        "def predict_mlp(model, X, args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(X_tensor)\n",
        "        predictions = output.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Set hyperparameters and arguments\n",
        "args = {\n",
        "    \"objective\": \"classification\",\n",
        "    \"batch_size\": 64,\n",
        "    \"val_batch_size\": 64,\n",
        "    \"epochs\": 10,\n",
        "    \"num_features\": None,\n",
        "    \"num_classes\": None,\n",
        "}\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID1: 14965"
      ],
      "metadata": {
        "id": "Av3Ht0glwhhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=14965)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V55mursFwb6w",
        "outputId": "221f0d29-136a-418b-8e99-5ca219109328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.2617\n",
            "Epoch 2, Validation Loss: 0.2339\n",
            "Epoch 3, Validation Loss: 0.2280\n",
            "Epoch 4, Validation Loss: 0.2242\n",
            "Epoch 5, Validation Loss: 0.2222\n",
            "Epoch 6, Validation Loss: 0.2206\n",
            "Epoch 7, Validation Loss: 0.2194\n",
            "Epoch 8, Validation Loss: 0.2180\n",
            "Epoch 9, Validation Loss: 0.2175\n",
            "Epoch 10, Validation Loss: 0.2166\n",
            "Validation Accuracy: 0.9035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID2: 9977"
      ],
      "metadata": {
        "id": "_bEEFrVDxJk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9977)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLAu_7R90MQg",
        "outputId": "531f1e55-0a5e-4a50-8fb2-c7036bd38a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.1591\n",
            "Epoch 2, Validation Loss: 0.1377\n",
            "Epoch 3, Validation Loss: 0.1296\n",
            "Epoch 4, Validation Loss: 0.1253\n",
            "Epoch 5, Validation Loss: 0.1221\n",
            "Epoch 6, Validation Loss: 0.1208\n",
            "Epoch 7, Validation Loss: 0.1183\n",
            "Epoch 8, Validation Loss: 0.1173\n",
            "Epoch 9, Validation Loss: 0.1144\n",
            "Epoch 10, Validation Loss: 0.1163\n",
            "Validation Accuracy: 0.9542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5pXuF7DnyGL"
      },
      "source": [
        "# Task ID3: 34539"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=34539)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RVpMSL60hAW",
        "outputId": "dbeb7a0e-3f8c-42ad-c0e2-ffb069fa3d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.2228\n",
            "Epoch 2, Validation Loss: 0.1846\n",
            "Epoch 3, Validation Loss: 0.1945\n",
            "Epoch 4, Validation Loss: 0.2289\n",
            "Epoch 5, Validation Loss: 0.2576\n",
            "Epoch 6, Validation Loss: 0.2801\n",
            "Epoch 7, Validation Loss: 0.3096\n",
            "Epoch 8, Validation Loss: 0.3476\n",
            "Epoch 9, Validation Loss: 0.3596\n",
            "Epoch 10, Validation Loss: 0.4040\n",
            "Validation Accuracy: 0.9362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1O7j_s4nyGL"
      },
      "source": [
        "# Task ID4: 146606"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146606)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0odrZK6405zj",
        "outputId": "9781db26-8823-4787-a2a3-77b280be74a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6401\n",
            "Epoch 2, Validation Loss: 0.6242\n",
            "Epoch 3, Validation Loss: 0.6142\n",
            "Epoch 4, Validation Loss: 0.6077\n",
            "Epoch 5, Validation Loss: 0.6024\n",
            "Epoch 6, Validation Loss: 0.5982\n",
            "Epoch 7, Validation Loss: 0.5935\n",
            "Epoch 8, Validation Loss: 0.5907\n",
            "Epoch 9, Validation Loss: 0.5875\n",
            "Epoch 10, Validation Loss: 0.5851\n",
            "Validation Accuracy: 0.6917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_B_NNsZnyGM"
      },
      "source": [
        "# Task ID5: 7592\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=7592)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glpuATaK0--V",
        "outputId": "48b90bae-9439-4062-af78-0a4d683f09b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.3434\n",
            "Epoch 2, Validation Loss: 0.3242\n",
            "Epoch 3, Validation Loss: 0.3154\n",
            "Epoch 4, Validation Loss: 0.3106\n",
            "Epoch 5, Validation Loss: 0.3086\n",
            "Epoch 6, Validation Loss: 0.3070\n",
            "Epoch 7, Validation Loss: 0.3053\n",
            "Epoch 8, Validation Loss: 0.3050\n",
            "Epoch 9, Validation Loss: 0.3043\n",
            "Epoch 10, Validation Loss: 0.3035\n",
            "Validation Accuracy: 0.8590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_83XrwnnyGM"
      },
      "source": [
        "# Task ID6: 146195\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146195)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liv01-xj1Aib",
        "outputId": "64a23a26-4c47-402a-f501-c8d540fab7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6165\n",
            "Epoch 2, Validation Loss: 0.5790\n",
            "Epoch 3, Validation Loss: 0.5577\n",
            "Epoch 4, Validation Loss: 0.5400\n",
            "Epoch 5, Validation Loss: 0.5217\n",
            "Epoch 6, Validation Loss: 0.5079\n",
            "Epoch 7, Validation Loss: 0.4953\n",
            "Epoch 8, Validation Loss: 0.4868\n",
            "Epoch 9, Validation Loss: 0.4793\n",
            "Epoch 10, Validation Loss: 0.4734\n",
            "Validation Accuracy: 0.8105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMSQgBTonyGM"
      },
      "source": [
        "# Task ID7: 167119\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=167119)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8NttQDk1BiC",
        "outputId": "629bc654-2596-4dee-fd9e-e22fa24abc59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.7152\n",
            "Epoch 2, Validation Loss: 0.6600\n",
            "Epoch 3, Validation Loss: 0.6155\n",
            "Epoch 4, Validation Loss: 0.5792\n",
            "Epoch 5, Validation Loss: 0.5527\n",
            "Epoch 6, Validation Loss: 0.5334\n",
            "Epoch 7, Validation Loss: 0.5176\n",
            "Epoch 8, Validation Loss: 0.5044\n",
            "Epoch 9, Validation Loss: 0.4946\n",
            "Epoch 10, Validation Loss: 0.4844\n",
            "Validation Accuracy: 0.8017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG_LgJ4JnyGM"
      },
      "source": [
        "# Task ID8: 167120\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=167120)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT6fd-hO1CrH",
        "outputId": "4122eaf1-6f2f-4f7d-bda6-30a791c5653b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6932\n",
            "Epoch 2, Validation Loss: 0.6927\n",
            "Epoch 3, Validation Loss: 0.6927\n",
            "Epoch 4, Validation Loss: 0.6926\n",
            "Epoch 5, Validation Loss: 0.6926\n",
            "Epoch 6, Validation Loss: 0.6927\n",
            "Epoch 7, Validation Loss: 0.6926\n",
            "Epoch 8, Validation Loss: 0.6925\n",
            "Epoch 9, Validation Loss: 0.6927\n",
            "Epoch 10, Validation Loss: 0.6928\n",
            "Validation Accuracy: 0.5166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I5y3Q25nyGM"
      },
      "source": [
        "# Task ID11: \t168331\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=168331)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNZovAQU1FRh",
        "outputId": "7b344cbb-1887-45be-d7f0-b7a0dd1ab930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.3411\n",
            "Epoch 2, Validation Loss: 1.2322\n",
            "Epoch 3, Validation Loss: 1.1965\n",
            "Epoch 4, Validation Loss: 1.1750\n",
            "Epoch 5, Validation Loss: 1.1582\n",
            "Epoch 6, Validation Loss: 1.1426\n",
            "Epoch 7, Validation Loss: 1.1332\n",
            "Epoch 8, Validation Loss: 1.1247\n",
            "Epoch 9, Validation Loss: 1.1153\n",
            "Epoch 10, Validation Loss: 1.1086\n",
            "Validation Accuracy: 0.6020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYGqmSMbnyGN"
      },
      "source": [
        "# Task ID12: \t168330\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=168330)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDFx0vtX1Go6",
        "outputId": "13556f82-df25-41b8-d391-ac288f7315c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.8471\n",
            "Epoch 2, Validation Loss: 0.8239\n",
            "Epoch 3, Validation Loss: 0.8099\n",
            "Epoch 4, Validation Loss: 0.7987\n",
            "Epoch 5, Validation Loss: 0.7902\n",
            "Epoch 6, Validation Loss: 0.7818\n",
            "Epoch 7, Validation Loss: 0.7741\n",
            "Epoch 8, Validation Loss: 0.7683\n",
            "Epoch 9, Validation Loss: 0.7628\n",
            "Epoch 10, Validation Loss: 0.7589\n",
            "Validation Accuracy: 0.6781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZj_gNCEnyGN"
      },
      "source": [
        "# Task ID13: \t168335\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=168335)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiObocx31Hr3",
        "outputId": "fae3efa1-8205-4466-c7a4-e05735b75f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.2862\n",
            "Epoch 2, Validation Loss: 0.2583\n",
            "Epoch 3, Validation Loss: 0.2466\n",
            "Epoch 4, Validation Loss: 0.2383\n",
            "Epoch 5, Validation Loss: 0.2342\n",
            "Epoch 6, Validation Loss: 0.2281\n",
            "Epoch 7, Validation Loss: 0.2243\n",
            "Epoch 8, Validation Loss: 0.2217\n",
            "Epoch 9, Validation Loss: 0.2180\n",
            "Epoch 10, Validation Loss: 0.2138\n",
            "Validation Accuracy: 0.9114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPepY1khnyGN"
      },
      "source": [
        "\n",
        "# Task ID16: \t146212\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146212)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfYhh9pY1Ibv",
        "outputId": "f162b000-c7aa-4a1a-bb76-68191882e03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.2034\n",
            "Epoch 2, Validation Loss: 0.1067\n",
            "Epoch 3, Validation Loss: 0.0710\n",
            "Epoch 4, Validation Loss: 0.0523\n",
            "Epoch 5, Validation Loss: 0.0421\n",
            "Epoch 6, Validation Loss: 0.0360\n",
            "Epoch 7, Validation Loss: 0.0320\n",
            "Epoch 8, Validation Loss: 0.0293\n",
            "Epoch 9, Validation Loss: 0.0272\n",
            "Epoch 10, Validation Loss: 0.0261\n",
            "Validation Accuracy: 0.9962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyNxYJtYnyGN"
      },
      "source": [
        "# Task ID19: \t168868\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=168868)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mObmSiH1JPP",
        "outputId": "4c6bbfcd-e666-44fd-89a8-063dfec73f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.0426\n",
            "Epoch 2, Validation Loss: 0.0381\n",
            "Epoch 3, Validation Loss: 0.0379\n",
            "Epoch 4, Validation Loss: 0.0369\n",
            "Epoch 5, Validation Loss: 0.0378\n",
            "Epoch 6, Validation Loss: 0.0383\n",
            "Epoch 7, Validation Loss: 0.0405\n",
            "Epoch 8, Validation Loss: 0.0446\n",
            "Epoch 9, Validation Loss: 0.0424\n",
            "Epoch 10, Validation Loss: 0.0426\n",
            "Validation Accuracy: 0.9909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID20: 31"
      ],
      "metadata": {
        "id": "bBSe6j0EnyGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=31)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO_X-I5z8SpY",
        "outputId": "f7d808f7-d5fd-428e-d259-aec6f5402143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6796\n",
            "Epoch 2, Validation Loss: 0.6699\n",
            "Epoch 3, Validation Loss: 0.6615\n",
            "Epoch 4, Validation Loss: 0.6542\n",
            "Epoch 5, Validation Loss: 0.6480\n",
            "Epoch 6, Validation Loss: 0.6422\n",
            "Epoch 7, Validation Loss: 0.6373\n",
            "Epoch 8, Validation Loss: 0.6326\n",
            "Epoch 9, Validation Loss: 0.6285\n",
            "Epoch 10, Validation Loss: 0.6248\n",
            "Validation Accuracy: 0.7050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID21: 10101"
      ],
      "metadata": {
        "id": "lK6WpkBRnyGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=10101)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMDHBSTD8Tv7",
        "outputId": "f22b232d-decd-427e-df85-2eadeb870022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6583\n",
            "Epoch 2, Validation Loss: 0.6488\n",
            "Epoch 3, Validation Loss: 0.6396\n",
            "Epoch 4, Validation Loss: 0.6304\n",
            "Epoch 5, Validation Loss: 0.6215\n",
            "Epoch 6, Validation Loss: 0.6122\n",
            "Epoch 7, Validation Loss: 0.6034\n",
            "Epoch 8, Validation Loss: 0.5944\n",
            "Epoch 9, Validation Loss: 0.5860\n",
            "Epoch 10, Validation Loss: 0.5780\n",
            "Validation Accuracy: 0.7533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID22: \t3913"
      ],
      "metadata": {
        "id": "IDVsIcfqnyGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3913)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgQwz31Z8Uk_",
        "outputId": "253ae6f5-9115-45ce-bbe8-11c21128ab4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6336\n",
            "Epoch 2, Validation Loss: 0.6247\n",
            "Epoch 3, Validation Loss: 0.6157\n",
            "Epoch 4, Validation Loss: 0.6071\n",
            "Epoch 5, Validation Loss: 0.5979\n",
            "Epoch 6, Validation Loss: 0.5885\n",
            "Epoch 7, Validation Loss: 0.5789\n",
            "Epoch 8, Validation Loss: 0.5693\n",
            "Epoch 9, Validation Loss: 0.5597\n",
            "Epoch 10, Validation Loss: 0.5496\n",
            "Validation Accuracy: 0.8095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID23: 3"
      ],
      "metadata": {
        "id": "BryxMbWGnyGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc-CqG0o8Vll",
        "outputId": "b4c3a23f-7a7e-4a9f-def2-7aaa7c2976d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6705\n",
            "Epoch 2, Validation Loss: 0.6494\n",
            "Epoch 3, Validation Loss: 0.6219\n",
            "Epoch 4, Validation Loss: 0.5850\n",
            "Epoch 5, Validation Loss: 0.5380\n",
            "Epoch 6, Validation Loss: 0.4850\n",
            "Epoch 7, Validation Loss: 0.4317\n",
            "Epoch 8, Validation Loss: 0.3815\n",
            "Epoch 9, Validation Loss: 0.3370\n",
            "Epoch 10, Validation Loss: 0.2992\n",
            "Validation Accuracy: 0.9062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID24: 3917"
      ],
      "metadata": {
        "id": "uJkeVVgqnyGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3917)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Xc4AOZ8WS-",
        "outputId": "d9dc8efc-291b-46c2-a229-795a63572489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6958\n",
            "Epoch 2, Validation Loss: 0.6509\n",
            "Epoch 3, Validation Loss: 0.6055\n",
            "Epoch 4, Validation Loss: 0.5575\n",
            "Epoch 5, Validation Loss: 0.5046\n",
            "Epoch 6, Validation Loss: 0.4568\n",
            "Epoch 7, Validation Loss: 0.4231\n",
            "Epoch 8, Validation Loss: 0.4041\n",
            "Epoch 9, Validation Loss: 0.3947\n",
            "Epoch 10, Validation Loss: 0.3909\n",
            "Validation Accuracy: 0.8412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID25: 9957"
      ],
      "metadata": {
        "id": "4KNteK5ynyGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9957)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKS2wRZ88XFX",
        "outputId": "fba80e3b-c410-4441-e734-44353a277153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6828\n",
            "Epoch 2, Validation Loss: 0.6638\n",
            "Epoch 3, Validation Loss: 0.6472\n",
            "Epoch 4, Validation Loss: 0.6314\n",
            "Epoch 5, Validation Loss: 0.6163\n",
            "Epoch 6, Validation Loss: 0.6010\n",
            "Epoch 7, Validation Loss: 0.5852\n",
            "Epoch 8, Validation Loss: 0.5687\n",
            "Epoch 9, Validation Loss: 0.5516\n",
            "Epoch 10, Validation Loss: 0.5351\n",
            "Validation Accuracy: 0.7630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID26: 9946"
      ],
      "metadata": {
        "id": "J42792rXnyGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9946)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJv04mNj8X7o",
        "outputId": "248a217b-dcbf-478b-a97f-26a1af46e02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6439\n",
            "Epoch 2, Validation Loss: 0.6249\n",
            "Epoch 3, Validation Loss: 0.6068\n",
            "Epoch 4, Validation Loss: 0.5884\n",
            "Epoch 5, Validation Loss: 0.5695\n",
            "Epoch 6, Validation Loss: 0.5504\n",
            "Epoch 7, Validation Loss: 0.5311\n",
            "Epoch 8, Validation Loss: 0.5119\n",
            "Epoch 9, Validation Loss: 0.4920\n",
            "Epoch 10, Validation Loss: 0.4710\n",
            "Validation Accuracy: 0.9474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID27: 3918"
      ],
      "metadata": {
        "id": "lyDKJv1tnyGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3918)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98LvNGkTfar4",
        "outputId": "00d12069-90a8-43ea-8430-d9b48fecc0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6102\n",
            "Epoch 2, Validation Loss: 0.5890\n",
            "Epoch 3, Validation Loss: 0.5686\n",
            "Epoch 4, Validation Loss: 0.5492\n",
            "Epoch 5, Validation Loss: 0.5296\n",
            "Epoch 6, Validation Loss: 0.5104\n",
            "Epoch 7, Validation Loss: 0.4907\n",
            "Epoch 8, Validation Loss: 0.4704\n",
            "Epoch 9, Validation Loss: 0.4500\n",
            "Epoch 10, Validation Loss: 0.4292\n",
            "Validation Accuracy: 0.9144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID28: 3903"
      ],
      "metadata": {
        "id": "evExChIFnyGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3903)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opQFzrCafbc4",
        "outputId": "6a9dc7e5-60aa-4e7a-8115-ebeb84219f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6419\n",
            "Epoch 2, Validation Loss: 0.6060\n",
            "Epoch 3, Validation Loss: 0.5697\n",
            "Epoch 4, Validation Loss: 0.5309\n",
            "Epoch 5, Validation Loss: 0.4889\n",
            "Epoch 6, Validation Loss: 0.4463\n",
            "Epoch 7, Validation Loss: 0.4095\n",
            "Epoch 8, Validation Loss: 0.3783\n",
            "Epoch 9, Validation Loss: 0.3557\n",
            "Epoch 10, Validation Loss: 0.3417\n",
            "Validation Accuracy: 0.8978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID29: 37"
      ],
      "metadata": {
        "id": "ch6wwrQRnyGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=37)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDO--wuKfcgx",
        "outputId": "6682a562-8d3d-47b4-a209-fdb39b0dffb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6940\n",
            "Epoch 2, Validation Loss: 0.6859\n",
            "Epoch 3, Validation Loss: 0.6777\n",
            "Epoch 4, Validation Loss: 0.6699\n",
            "Epoch 5, Validation Loss: 0.6616\n",
            "Epoch 6, Validation Loss: 0.6538\n",
            "Epoch 7, Validation Loss: 0.6454\n",
            "Epoch 8, Validation Loss: 0.6369\n",
            "Epoch 9, Validation Loss: 0.6284\n",
            "Epoch 10, Validation Loss: 0.6191\n",
            "Validation Accuracy: 0.7143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID30: 9971"
      ],
      "metadata": {
        "id": "BtY3waYdnyGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9971)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkWxY27FfdTf",
        "outputId": "bc07dd50-0686-45ee-8726-e817af126a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6758\n",
            "Epoch 2, Validation Loss: 0.6643\n",
            "Epoch 3, Validation Loss: 0.6537\n",
            "Epoch 4, Validation Loss: 0.6440\n",
            "Epoch 5, Validation Loss: 0.6347\n",
            "Epoch 6, Validation Loss: 0.6262\n",
            "Epoch 7, Validation Loss: 0.6183\n",
            "Epoch 8, Validation Loss: 0.6105\n",
            "Epoch 9, Validation Loss: 0.6031\n",
            "Epoch 10, Validation Loss: 0.5962\n",
            "Validation Accuracy: 0.7436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID31: 9952"
      ],
      "metadata": {
        "id": "e3iQDTmMnyGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9952)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywb0ug_6feQL",
        "outputId": "9e354041-bb94-4be1-92a6-b7bbb46a691d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.5894\n",
            "Epoch 2, Validation Loss: 0.5326\n",
            "Epoch 3, Validation Loss: 0.4930\n",
            "Epoch 4, Validation Loss: 0.4664\n",
            "Epoch 5, Validation Loss: 0.4503\n",
            "Epoch 6, Validation Loss: 0.4404\n",
            "Epoch 7, Validation Loss: 0.4341\n",
            "Epoch 8, Validation Loss: 0.4290\n",
            "Epoch 9, Validation Loss: 0.4250\n",
            "Epoch 10, Validation Loss: 0.4212\n",
            "Validation Accuracy: 0.7928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID32: 3902"
      ],
      "metadata": {
        "id": "qcOMp9ECnyGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3902)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtZwdPldfe8g",
        "outputId": "cccdad0f-4152-4dee-9e61-70bfa0b82a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6374\n",
            "Epoch 2, Validation Loss: 0.6065\n",
            "Epoch 3, Validation Loss: 0.5770\n",
            "Epoch 4, Validation Loss: 0.5469\n",
            "Epoch 5, Validation Loss: 0.5155\n",
            "Epoch 6, Validation Loss: 0.4833\n",
            "Epoch 7, Validation Loss: 0.4533\n",
            "Epoch 8, Validation Loss: 0.4270\n",
            "Epoch 9, Validation Loss: 0.4056\n",
            "Epoch 10, Validation Loss: 0.3884\n",
            "Validation Accuracy: 0.8562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID33: 49"
      ],
      "metadata": {
        "id": "RiteirMDnyGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=49)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjOP7H5dfgGg",
        "outputId": "c4cc3bcc-e777-427f-acee-d8c5d9c3bdb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6989\n",
            "Epoch 2, Validation Loss: 0.6880\n",
            "Epoch 3, Validation Loss: 0.6777\n",
            "Epoch 4, Validation Loss: 0.6678\n",
            "Epoch 5, Validation Loss: 0.6581\n",
            "Epoch 6, Validation Loss: 0.6489\n",
            "Epoch 7, Validation Loss: 0.6397\n",
            "Epoch 8, Validation Loss: 0.6311\n",
            "Epoch 9, Validation Loss: 0.6227\n",
            "Epoch 10, Validation Loss: 0.6157\n",
            "Validation Accuracy: 0.6510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID34: 43"
      ],
      "metadata": {
        "id": "trrcuQNBnyGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=43)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CJI0cVSfhCl",
        "outputId": "1c91c6f3-c084-4a48-9a59-40106807ff17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6400\n",
            "Epoch 2, Validation Loss: 0.5789\n",
            "Epoch 3, Validation Loss: 0.4994\n",
            "Epoch 4, Validation Loss: 0.4110\n",
            "Epoch 5, Validation Loss: 0.3360\n",
            "Epoch 6, Validation Loss: 0.2877\n",
            "Epoch 7, Validation Loss: 0.2573\n",
            "Epoch 8, Validation Loss: 0.2394\n",
            "Epoch 9, Validation Loss: 0.2264\n",
            "Epoch 10, Validation Loss: 0.2177\n",
            "Validation Accuracy: 0.9327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID35: 9978"
      ],
      "metadata": {
        "id": "lsBOwq-NnyGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9978)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL7WsQRVfhr8",
        "outputId": "6c890de5-bd2f-461c-803d-eefb8a116358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.5707\n",
            "Epoch 2, Validation Loss: 0.4667\n",
            "Epoch 3, Validation Loss: 0.3744\n",
            "Epoch 4, Validation Loss: 0.3052\n",
            "Epoch 5, Validation Loss: 0.2637\n",
            "Epoch 6, Validation Loss: 0.2391\n",
            "Epoch 7, Validation Loss: 0.2235\n",
            "Epoch 8, Validation Loss: 0.2130\n",
            "Epoch 9, Validation Loss: 0.2048\n",
            "Epoch 10, Validation Loss: 0.1986\n",
            "Validation Accuracy: 0.9349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID36: 10093"
      ],
      "metadata": {
        "id": "qOzuSEEMnyGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=10093)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rAAOIIXfiVZ",
        "outputId": "c27b5a90-0a2f-448b-a62a-53b2ba500815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6556\n",
            "Epoch 2, Validation Loss: 0.6368\n",
            "Epoch 3, Validation Loss: 0.6176\n",
            "Epoch 4, Validation Loss: 0.5975\n",
            "Epoch 5, Validation Loss: 0.5765\n",
            "Epoch 6, Validation Loss: 0.5538\n",
            "Epoch 7, Validation Loss: 0.5295\n",
            "Epoch 8, Validation Loss: 0.5030\n",
            "Epoch 9, Validation Loss: 0.4755\n",
            "Epoch 10, Validation Loss: 0.4461\n",
            "Validation Accuracy: 0.8618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID37: 219"
      ],
      "metadata": {
        "id": "DPxUrCy5nyGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=219)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZkusjKlfjGT",
        "outputId": "8f2d9a2a-60b5-4f7b-a8e2-108f70809671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.5211\n",
            "Epoch 2, Validation Loss: 0.4844\n",
            "Epoch 3, Validation Loss: 0.4739\n",
            "Epoch 4, Validation Loss: 0.4685\n",
            "Epoch 5, Validation Loss: 0.4635\n",
            "Epoch 6, Validation Loss: 0.4606\n",
            "Epoch 7, Validation Loss: 0.4582\n",
            "Epoch 8, Validation Loss: 0.4547\n",
            "Epoch 9, Validation Loss: 0.4522\n",
            "Epoch 10, Validation Loss: 0.4505\n",
            "Validation Accuracy: 0.7888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID38: 9976"
      ],
      "metadata": {
        "id": "alCTo-Y5nyGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9976)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt13BQ3zfj5u",
        "outputId": "6f4d8a7f-1670-438e-9b4e-6276b6274bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6944\n",
            "Epoch 2, Validation Loss: 0.6916\n",
            "Epoch 3, Validation Loss: 0.6885\n",
            "Epoch 4, Validation Loss: 0.6854\n",
            "Epoch 5, Validation Loss: 0.6819\n",
            "Epoch 6, Validation Loss: 0.6773\n",
            "Epoch 7, Validation Loss: 0.6732\n",
            "Epoch 8, Validation Loss: 0.6692\n",
            "Epoch 9, Validation Loss: 0.6670\n",
            "Epoch 10, Validation Loss: 0.6687\n",
            "Validation Accuracy: 0.5423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID39: 6"
      ],
      "metadata": {
        "id": "n7-rsf27nyGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=6)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE_pIlvwfk7m",
        "outputId": "d3418f6f-59ff-480c-c67d-cb700aaa842a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 3.0598\n",
            "Epoch 2, Validation Loss: 2.5152\n",
            "Epoch 3, Validation Loss: 1.9552\n",
            "Epoch 4, Validation Loss: 1.6047\n",
            "Epoch 5, Validation Loss: 1.3912\n",
            "Epoch 6, Validation Loss: 1.2533\n",
            "Epoch 7, Validation Loss: 1.1598\n",
            "Epoch 8, Validation Loss: 1.0897\n",
            "Epoch 9, Validation Loss: 1.0362\n",
            "Epoch 10, Validation Loss: 0.9918\n",
            "Validation Accuracy: 0.7130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID40: 53"
      ],
      "metadata": {
        "id": "oF_J6GuYnyGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=53)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2FPDp9cflwI",
        "outputId": "415ec3e4-b0bc-4253-9a99-87af257d405a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.3758\n",
            "Epoch 2, Validation Loss: 1.3669\n",
            "Epoch 3, Validation Loss: 1.3582\n",
            "Epoch 4, Validation Loss: 1.3493\n",
            "Epoch 5, Validation Loss: 1.3408\n",
            "Epoch 6, Validation Loss: 1.3316\n",
            "Epoch 7, Validation Loss: 1.3224\n",
            "Epoch 8, Validation Loss: 1.3135\n",
            "Epoch 9, Validation Loss: 1.3037\n",
            "Epoch 10, Validation Loss: 1.2932\n",
            "Validation Accuracy: 0.4471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID41: 11"
      ],
      "metadata": {
        "id": "O95KUOopnyGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=11)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PD52XlSfmbZ",
        "outputId": "6b68492d-aeb4-4022-b926-8d732a3b47d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.0777\n",
            "Epoch 2, Validation Loss: 1.0609\n",
            "Epoch 3, Validation Loss: 1.0445\n",
            "Epoch 4, Validation Loss: 1.0284\n",
            "Epoch 5, Validation Loss: 1.0130\n",
            "Epoch 6, Validation Loss: 0.9978\n",
            "Epoch 7, Validation Loss: 0.9828\n",
            "Epoch 8, Validation Loss: 0.9681\n",
            "Epoch 9, Validation Loss: 0.9534\n",
            "Epoch 10, Validation Loss: 0.9387\n",
            "Validation Accuracy: 0.5760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID42: 15"
      ],
      "metadata": {
        "id": "TYCOso1YnyGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=15)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSXlIWdrfnLR",
        "outputId": "8cf2b814-4bcd-4ec5-b511-8f0a451f74a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6640\n",
            "Epoch 2, Validation Loss: 0.6463\n",
            "Epoch 3, Validation Loss: 0.6286\n",
            "Epoch 4, Validation Loss: 0.6108\n",
            "Epoch 5, Validation Loss: 0.5924\n",
            "Epoch 6, Validation Loss: 0.5734\n",
            "Epoch 7, Validation Loss: 0.5538\n",
            "Epoch 8, Validation Loss: 0.5335\n",
            "Epoch 9, Validation Loss: 0.5123\n",
            "Epoch 10, Validation Loss: 0.4902\n",
            "Validation Accuracy: 0.9643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID43: 16"
      ],
      "metadata": {
        "id": "6iXhjjGxnyGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=16)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrPSbPdTfoQS",
        "outputId": "7b23ddfb-cee0-4650-86d2-b81c34e7dfa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.2873\n",
            "Epoch 2, Validation Loss: 2.2541\n",
            "Epoch 3, Validation Loss: 2.2198\n",
            "Epoch 4, Validation Loss: 2.1813\n",
            "Epoch 5, Validation Loss: 2.1356\n",
            "Epoch 6, Validation Loss: 2.0804\n",
            "Epoch 7, Validation Loss: 2.0135\n",
            "Epoch 8, Validation Loss: 1.9330\n",
            "Epoch 9, Validation Loss: 1.8391\n",
            "Epoch 10, Validation Loss: 1.7345\n",
            "Validation Accuracy: 0.7475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID44: 14"
      ],
      "metadata": {
        "id": "wx_ZgBj6nyGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=14)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA9Uhf9_fpPG",
        "outputId": "9994d1cd-606c-4f2b-8be0-3a293ab8e000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.2667\n",
            "Epoch 2, Validation Loss: 2.2367\n",
            "Epoch 3, Validation Loss: 2.2022\n",
            "Epoch 4, Validation Loss: 2.1601\n",
            "Epoch 5, Validation Loss: 2.1078\n",
            "Epoch 6, Validation Loss: 2.0448\n",
            "Epoch 7, Validation Loss: 1.9664\n",
            "Epoch 8, Validation Loss: 1.8777\n",
            "Epoch 9, Validation Loss: 1.7794\n",
            "Epoch 10, Validation Loss: 1.6748\n",
            "Validation Accuracy: 0.6650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID45: 32"
      ],
      "metadata": {
        "id": "KSrWTnj1nyGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=32)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw5DeGUqfqA4",
        "outputId": "63b014ba-24e5-4dd8-d40a-ee247b20e014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.0543\n",
            "Epoch 2, Validation Loss: 1.5984\n",
            "Epoch 3, Validation Loss: 1.0985\n",
            "Epoch 4, Validation Loss: 0.7730\n",
            "Epoch 5, Validation Loss: 0.5953\n",
            "Epoch 6, Validation Loss: 0.4847\n",
            "Epoch 7, Validation Loss: 0.4060\n",
            "Epoch 8, Validation Loss: 0.3460\n",
            "Epoch 9, Validation Loss: 0.2996\n",
            "Epoch 10, Validation Loss: 0.2613\n",
            "Validation Accuracy: 0.9395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID46: 3549"
      ],
      "metadata": {
        "id": "3-cyox8NnyGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3549)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FKfi0lefqxz",
        "outputId": "bf3d2e3f-babe-4213-be5d-d413218f7b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.3434\n",
            "Epoch 2, Validation Loss: 1.3177\n",
            "Epoch 3, Validation Loss: 1.2914\n",
            "Epoch 4, Validation Loss: 1.2630\n",
            "Epoch 5, Validation Loss: 1.2317\n",
            "Epoch 6, Validation Loss: 1.1964\n",
            "Epoch 7, Validation Loss: 1.1564\n",
            "Epoch 8, Validation Loss: 1.1114\n",
            "Epoch 9, Validation Loss: 1.0612\n",
            "Epoch 10, Validation Loss: 1.0068\n",
            "Validation Accuracy: 0.8698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID47: 12"
      ],
      "metadata": {
        "id": "Imfyr9fbnyGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=12)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orx8-spjfrxJ",
        "outputId": "9487dc23-38dd-4c92-e9fb-6a098348e20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.2062\n",
            "Epoch 2, Validation Loss: 2.0821\n",
            "Epoch 3, Validation Loss: 1.9274\n",
            "Epoch 4, Validation Loss: 1.7305\n",
            "Epoch 5, Validation Loss: 1.5005\n",
            "Epoch 6, Validation Loss: 1.2659\n",
            "Epoch 7, Validation Loss: 1.0485\n",
            "Epoch 8, Validation Loss: 0.8616\n",
            "Epoch 9, Validation Loss: 0.7098\n",
            "Epoch 10, Validation Loss: 0.5956\n",
            "Validation Accuracy: 0.9050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID48: 9981"
      ],
      "metadata": {
        "id": "qY_f1strnyGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9981)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb-vWWQ4fstR",
        "outputId": "0a1444e2-08f4-4522-caa4-7a7d501db6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.1943\n",
            "Epoch 2, Validation Loss: 2.1784\n",
            "Epoch 3, Validation Loss: 2.1623\n",
            "Epoch 4, Validation Loss: 2.1441\n",
            "Epoch 5, Validation Loss: 2.1234\n",
            "Epoch 6, Validation Loss: 2.0991\n",
            "Epoch 7, Validation Loss: 2.0694\n",
            "Epoch 8, Validation Loss: 2.0333\n",
            "Epoch 9, Validation Loss: 1.9903\n",
            "Epoch 10, Validation Loss: 1.9388\n",
            "Validation Accuracy: 0.6343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID49: 18"
      ],
      "metadata": {
        "id": "2cLrARs7nyGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=18)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isnf2gjNftr1",
        "outputId": "a1db7985-4554-442f-afaa-ace0a6325ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.2746\n",
            "Epoch 2, Validation Loss: 2.2305\n",
            "Epoch 3, Validation Loss: 2.1823\n",
            "Epoch 4, Validation Loss: 2.1290\n",
            "Epoch 5, Validation Loss: 2.0712\n",
            "Epoch 6, Validation Loss: 2.0048\n",
            "Epoch 7, Validation Loss: 1.9350\n",
            "Epoch 8, Validation Loss: 1.8620\n",
            "Epoch 9, Validation Loss: 1.7887\n",
            "Epoch 10, Validation Loss: 1.7161\n",
            "Validation Accuracy: 0.4850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID50: 28"
      ],
      "metadata": {
        "id": "k0_-2FgEnyGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=28)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCjw4WvSfuph",
        "outputId": "6839defe-4a25-4163-ce1b-f3cd684e3ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.1857\n",
            "Epoch 2, Validation Loss: 2.0054\n",
            "Epoch 3, Validation Loss: 1.7261\n",
            "Epoch 4, Validation Loss: 1.3772\n",
            "Epoch 5, Validation Loss: 1.0364\n",
            "Epoch 6, Validation Loss: 0.7710\n",
            "Epoch 7, Validation Loss: 0.5834\n",
            "Epoch 8, Validation Loss: 0.4555\n",
            "Epoch 9, Validation Loss: 0.3731\n",
            "Epoch 10, Validation Loss: 0.3176\n",
            "Validation Accuracy: 0.9262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID51: 2074"
      ],
      "metadata": {
        "id": "l89sq5aOnyGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=2074)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzSMZlfEfvVy",
        "outputId": "1b3d71e5-8aec-45f9-b119-6d88055b396d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.4955\n",
            "Epoch 2, Validation Loss: 1.1203\n",
            "Epoch 3, Validation Loss: 0.8331\n",
            "Epoch 4, Validation Loss: 0.6529\n",
            "Epoch 5, Validation Loss: 0.5382\n",
            "Epoch 6, Validation Loss: 0.4693\n",
            "Epoch 7, Validation Loss: 0.4304\n",
            "Epoch 8, Validation Loss: 0.4069\n",
            "Epoch 9, Validation Loss: 0.3908\n",
            "Epoch 10, Validation Loss: 0.3777\n",
            "Validation Accuracy: 0.8429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID52: 29"
      ],
      "metadata": {
        "id": "rOi6eN_EnyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=29)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAK6-4Hhfv4S",
        "outputId": "f0ecb65e-57c8-4d43-f0d6-34419c66037b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.7111\n",
            "Epoch 2, Validation Loss: 0.7075\n",
            "Epoch 3, Validation Loss: 0.7037\n",
            "Epoch 4, Validation Loss: 0.7000\n",
            "Epoch 5, Validation Loss: 0.6961\n",
            "Epoch 6, Validation Loss: 0.6926\n",
            "Epoch 7, Validation Loss: 0.6879\n",
            "Epoch 8, Validation Loss: 0.6830\n",
            "Epoch 9, Validation Loss: 0.6778\n",
            "Epoch 10, Validation Loss: 0.6720\n",
            "Validation Accuracy: 0.5290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID53: 45"
      ],
      "metadata": {
        "id": "HDtOCnN3nyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=45)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oGPNoM6fwq3",
        "outputId": "524635f4-d058-41bd-c56e-6c246f838fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.0605\n",
            "Epoch 2, Validation Loss: 1.0213\n",
            "Epoch 3, Validation Loss: 0.9760\n",
            "Epoch 4, Validation Loss: 0.9206\n",
            "Epoch 5, Validation Loss: 0.8491\n",
            "Epoch 6, Validation Loss: 0.7591\n",
            "Epoch 7, Validation Loss: 0.6581\n",
            "Epoch 8, Validation Loss: 0.5572\n",
            "Epoch 9, Validation Loss: 0.4657\n",
            "Epoch 10, Validation Loss: 0.3918\n",
            "Validation Accuracy: 0.8997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID54: 125922"
      ],
      "metadata": {
        "id": "gJXSos7DnyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=125922)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pOstywjfxjT",
        "outputId": "f6522911-897f-4d03-f8eb-804679894a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.2950\n",
            "Epoch 2, Validation Loss: 2.1239\n",
            "Epoch 3, Validation Loss: 1.8690\n",
            "Epoch 4, Validation Loss: 1.6383\n",
            "Epoch 5, Validation Loss: 1.4472\n",
            "Epoch 6, Validation Loss: 1.2707\n",
            "Epoch 7, Validation Loss: 1.1165\n",
            "Epoch 8, Validation Loss: 0.9866\n",
            "Epoch 9, Validation Loss: 0.8817\n",
            "Epoch 10, Validation Loss: 0.7872\n",
            "Validation Accuracy: 0.7736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID55: 9960"
      ],
      "metadata": {
        "id": "U2dVl7XpnyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9960)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQZ9CgYtfycE",
        "outputId": "bcb68144-5e3e-4953-c09b-9252e77c95df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.3750\n",
            "Epoch 2, Validation Loss: 1.2335\n",
            "Epoch 3, Validation Loss: 1.1062\n",
            "Epoch 4, Validation Loss: 1.0164\n",
            "Epoch 5, Validation Loss: 0.9501\n",
            "Epoch 6, Validation Loss: 0.8935\n",
            "Epoch 7, Validation Loss: 0.8453\n",
            "Epoch 8, Validation Loss: 0.8035\n",
            "Epoch 9, Validation Loss: 0.7678\n",
            "Epoch 10, Validation Loss: 0.7366\n",
            "Validation Accuracy: 0.7033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID56: 9964"
      ],
      "metadata": {
        "id": "K8f8egKZnyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9964)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diltBFXkfzOc",
        "outputId": "920866a8-71c5-49fb-d975-2d454037625d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.2544\n",
            "Epoch 2, Validation Loss: 2.2103\n",
            "Epoch 3, Validation Loss: 2.1569\n",
            "Epoch 4, Validation Loss: 2.0902\n",
            "Epoch 5, Validation Loss: 2.0091\n",
            "Epoch 6, Validation Loss: 1.9161\n",
            "Epoch 7, Validation Loss: 1.8119\n",
            "Epoch 8, Validation Loss: 1.6974\n",
            "Epoch 9, Validation Loss: 1.5791\n",
            "Epoch 10, Validation Loss: 1.4525\n",
            "Validation Accuracy: 0.6991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID57: 22"
      ],
      "metadata": {
        "id": "A5ojkIxfnyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=22)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U_own0Vfz3u",
        "outputId": "15a8479d-0bd0-4c7f-9b0a-fffa98f9a867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.2549\n",
            "Epoch 2, Validation Loss: 2.2111\n",
            "Epoch 3, Validation Loss: 2.1641\n",
            "Epoch 4, Validation Loss: 2.1103\n",
            "Epoch 5, Validation Loss: 2.0456\n",
            "Epoch 6, Validation Loss: 1.9719\n",
            "Epoch 7, Validation Loss: 1.8883\n",
            "Epoch 8, Validation Loss: 1.7946\n",
            "Epoch 9, Validation Loss: 1.6943\n",
            "Epoch 10, Validation Loss: 1.5930\n",
            "Validation Accuracy: 0.6400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID58: 2079"
      ],
      "metadata": {
        "id": "aK5XgtsjnyGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=2079)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX_Lt088f0n7",
        "outputId": "bbad7d6f-0428-4706-c427-bef6951869f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.6230\n",
            "Epoch 2, Validation Loss: 1.6110\n",
            "Epoch 3, Validation Loss: 1.6001\n",
            "Epoch 4, Validation Loss: 1.5892\n",
            "Epoch 5, Validation Loss: 1.5793\n",
            "Epoch 6, Validation Loss: 1.5685\n",
            "Epoch 7, Validation Loss: 1.5575\n",
            "Epoch 8, Validation Loss: 1.5465\n",
            "Epoch 9, Validation Loss: 1.5355\n",
            "Epoch 10, Validation Loss: 1.5239\n",
            "Validation Accuracy: 0.4189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID59: 14969"
      ],
      "metadata": {
        "id": "hoghuFMCnyGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=14969)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_it__I7f1mC",
        "outputId": "fd7c4afb-5855-47d7-fe19-74f29a282030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.4949\n",
            "Epoch 2, Validation Loss: 1.4222\n",
            "Epoch 3, Validation Loss: 1.3568\n",
            "Epoch 4, Validation Loss: 1.3166\n",
            "Epoch 5, Validation Loss: 1.2917\n",
            "Epoch 6, Validation Loss: 1.2732\n",
            "Epoch 7, Validation Loss: 1.2587\n",
            "Epoch 8, Validation Loss: 1.2465\n",
            "Epoch 9, Validation Loss: 1.2376\n",
            "Epoch 10, Validation Loss: 1.2295\n",
            "Validation Accuracy: 0.4942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID60: 3560"
      ],
      "metadata": {
        "id": "17Z-vbrwnyGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3560)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0Ff7v3ff2Zd",
        "outputId": "1facc962-244d-4f24-db30-4f34d39b9a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.7911\n",
            "Epoch 2, Validation Loss: 1.7899\n",
            "Epoch 3, Validation Loss: 1.7889\n",
            "Epoch 4, Validation Loss: 1.7883\n",
            "Epoch 5, Validation Loss: 1.7875\n",
            "Epoch 6, Validation Loss: 1.7870\n",
            "Epoch 7, Validation Loss: 1.7861\n",
            "Epoch 8, Validation Loss: 1.7857\n",
            "Epoch 9, Validation Loss: 1.7852\n",
            "Epoch 10, Validation Loss: 1.7847\n",
            "Validation Accuracy: 0.2062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID61: 14952"
      ],
      "metadata": {
        "id": "RzI-aMg1nyGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=14952)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ0UyQ7ef3bD",
        "outputId": "10f078c1-6e0d-4516-c5d7-203d6929b238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.5276\n",
            "Epoch 2, Validation Loss: 0.2956\n",
            "Epoch 3, Validation Loss: 0.2029\n",
            "Epoch 4, Validation Loss: 0.1713\n",
            "Epoch 5, Validation Loss: 0.1561\n",
            "Epoch 6, Validation Loss: 0.1467\n",
            "Epoch 7, Validation Loss: 0.1411\n",
            "Epoch 8, Validation Loss: 0.1361\n",
            "Epoch 9, Validation Loss: 0.1325\n",
            "Epoch 10, Validation Loss: 0.1294\n",
            "Validation Accuracy: 0.9466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID62: 125920"
      ],
      "metadata": {
        "id": "DDhaMS3ynyGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=125920)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWsHNPphf8OH",
        "outputId": "f8e49aad-4e54-497f-8b3e-d7f86dd54c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6992\n",
            "Epoch 2, Validation Loss: 0.6978\n",
            "Epoch 3, Validation Loss: 0.6960\n",
            "Epoch 4, Validation Loss: 0.6944\n",
            "Epoch 5, Validation Loss: 0.6931\n",
            "Epoch 6, Validation Loss: 0.6918\n",
            "Epoch 7, Validation Loss: 0.6906\n",
            "Epoch 8, Validation Loss: 0.6894\n",
            "Epoch 9, Validation Loss: 0.6884\n",
            "Epoch 10, Validation Loss: 0.6875\n",
            "Validation Accuracy: 0.5500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID63: 23"
      ],
      "metadata": {
        "id": "OAEYaMiinyGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=23)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CvmYVBXf9HY",
        "outputId": "fb637647-8934-45c7-a862-d1a0eb0b0665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.0792\n",
            "Epoch 2, Validation Loss: 1.0700\n",
            "Epoch 3, Validation Loss: 1.0620\n",
            "Epoch 4, Validation Loss: 1.0542\n",
            "Epoch 5, Validation Loss: 1.0474\n",
            "Epoch 6, Validation Loss: 1.0411\n",
            "Epoch 7, Validation Loss: 1.0348\n",
            "Epoch 8, Validation Loss: 1.0299\n",
            "Epoch 9, Validation Loss: 1.0247\n",
            "Epoch 10, Validation Loss: 1.0197\n",
            "Validation Accuracy: 0.4441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID64: 3904"
      ],
      "metadata": {
        "id": "zofaRrcWnyGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3904)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KA2D69sf9yE",
        "outputId": "ac564f44-7725-420a-efb8-f56208fc7441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.5274\n",
            "Epoch 2, Validation Loss: 0.4472\n",
            "Epoch 3, Validation Loss: 0.4368\n",
            "Epoch 4, Validation Loss: 0.4322\n",
            "Epoch 5, Validation Loss: 0.4301\n",
            "Epoch 6, Validation Loss: 0.4286\n",
            "Epoch 7, Validation Loss: 0.4277\n",
            "Epoch 8, Validation Loss: 0.4278\n",
            "Epoch 9, Validation Loss: 0.4269\n",
            "Epoch 10, Validation Loss: 0.4269\n",
            "Validation Accuracy: 0.8103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID65: 3022"
      ],
      "metadata": {
        "id": "jECtrOx7nyGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3022)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdjIvjgFf-WD",
        "outputId": "879af115-67da-4c94-bacf-eb250780c48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.4103\n",
            "Epoch 2, Validation Loss: 2.4044\n",
            "Epoch 3, Validation Loss: 2.3989\n",
            "Epoch 4, Validation Loss: 2.3929\n",
            "Epoch 5, Validation Loss: 2.3870\n",
            "Epoch 6, Validation Loss: 2.3805\n",
            "Epoch 7, Validation Loss: 2.3737\n",
            "Epoch 8, Validation Loss: 2.3668\n",
            "Epoch 9, Validation Loss: 2.3592\n",
            "Epoch 10, Validation Loss: 2.3508\n",
            "Validation Accuracy: 0.2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID66: 9985"
      ],
      "metadata": {
        "id": "l7cII4VKnyGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9985)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHbo2JWIf-7U",
        "outputId": "1406210a-3b54-48c7-d801-b5d2fdb71961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.7212\n",
            "Epoch 2, Validation Loss: 1.6468\n",
            "Epoch 3, Validation Loss: 1.5865\n",
            "Epoch 4, Validation Loss: 1.5565\n",
            "Epoch 5, Validation Loss: 1.5380\n",
            "Epoch 6, Validation Loss: 1.5222\n",
            "Epoch 7, Validation Loss: 1.5085\n",
            "Epoch 8, Validation Loss: 1.4947\n",
            "Epoch 9, Validation Loss: 1.4838\n",
            "Epoch 10, Validation Loss: 1.4735\n",
            "Validation Accuracy: 0.4379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID67: 9910"
      ],
      "metadata": {
        "id": "hb5r5VVtnyGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9910)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSIpOa9kgAp-",
        "outputId": "f5524588-6b94-4262-bbb9-a09d1500f300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6170\n",
            "Epoch 2, Validation Loss: 0.5535\n",
            "Epoch 3, Validation Loss: 0.5076\n",
            "Epoch 4, Validation Loss: 0.4806\n",
            "Epoch 5, Validation Loss: 0.4689\n",
            "Epoch 6, Validation Loss: 0.4688\n",
            "Epoch 7, Validation Loss: 0.4845\n",
            "Epoch 8, Validation Loss: 0.4881\n",
            "Epoch 9, Validation Loss: 0.5072\n",
            "Epoch 10, Validation Loss: 0.5125\n",
            "Validation Accuracy: 0.7803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID68: 14970"
      ],
      "metadata": {
        "id": "B_0b-YAanyGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=14970)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7kEUM7UgBey",
        "outputId": "a72df24a-f12b-48b1-b124-22016a04cff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.7674\n",
            "Epoch 2, Validation Loss: 0.4253\n",
            "Epoch 3, Validation Loss: 0.2916\n",
            "Epoch 4, Validation Loss: 0.2276\n",
            "Epoch 5, Validation Loss: 0.1757\n",
            "Epoch 6, Validation Loss: 0.1487\n",
            "Epoch 7, Validation Loss: 0.1308\n",
            "Epoch 8, Validation Loss: 0.1237\n",
            "Epoch 9, Validation Loss: 0.1068\n",
            "Epoch 10, Validation Loss: 0.0977\n",
            "Validation Accuracy: 0.9665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID69: 3021"
      ],
      "metadata": {
        "id": "jlwsSMRonyGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3021)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNCzt3K6gCUS",
        "outputId": "eafab2a9-b9b4-449b-b57a-9d8f8ce2c787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.5455\n",
            "Epoch 2, Validation Loss: 0.4186\n",
            "Epoch 3, Validation Loss: 0.3130\n",
            "Epoch 4, Validation Loss: 0.2420\n",
            "Epoch 5, Validation Loss: 0.2021\n",
            "Epoch 6, Validation Loss: 0.1798\n",
            "Epoch 7, Validation Loss: 0.1655\n",
            "Epoch 8, Validation Loss: 0.1548\n",
            "Epoch 9, Validation Loss: 0.1467\n",
            "Epoch 10, Validation Loss: 0.1397\n",
            "Validation Accuracy: 0.9497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID70: 3481"
      ],
      "metadata": {
        "id": "DKMT0CJLnyGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=3481)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXI320RlgDEt",
        "outputId": "9e774080-73e1-484c-8402-b309cd2137d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.6960\n",
            "Epoch 2, Validation Loss: 1.9094\n",
            "Epoch 3, Validation Loss: 1.2953\n",
            "Epoch 4, Validation Loss: 0.8847\n",
            "Epoch 5, Validation Loss: 0.6628\n",
            "Epoch 6, Validation Loss: 0.5382\n",
            "Epoch 7, Validation Loss: 0.4591\n",
            "Epoch 8, Validation Loss: 0.4028\n",
            "Epoch 9, Validation Loss: 0.3583\n",
            "Epoch 10, Validation Loss: 0.3219\n",
            "Validation Accuracy: 0.9135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID71: 3573"
      ],
      "metadata": {
        "id": "4K4QzDR7nyGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=9946)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "VYgwf5pTgD6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID72: 146824"
      ],
      "metadata": {
        "id": "JvliOeEbnyGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146824)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkClEmZRgEmS",
        "outputId": "3bb3ea4b-3418-466d-b8f5-288942475b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.1643\n",
            "Epoch 2, Validation Loss: 2.0326\n",
            "Epoch 3, Validation Loss: 1.8724\n",
            "Epoch 4, Validation Loss: 1.6769\n",
            "Epoch 5, Validation Loss: 1.4650\n",
            "Epoch 6, Validation Loss: 1.2563\n",
            "Epoch 7, Validation Loss: 1.0627\n",
            "Epoch 8, Validation Loss: 0.8854\n",
            "Epoch 9, Validation Loss: 0.7300\n",
            "Epoch 10, Validation Loss: 0.6046\n",
            "Validation Accuracy: 0.9025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID73: 146820"
      ],
      "metadata": {
        "id": "qG4a3QvZnyGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146820)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3G_VygTgFcB",
        "outputId": "316a644f-94fb-4ca4-897f-9da794a47638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.4838\n",
            "Epoch 2, Validation Loss: 0.3455\n",
            "Epoch 3, Validation Loss: 0.2481\n",
            "Epoch 4, Validation Loss: 0.2040\n",
            "Epoch 5, Validation Loss: 0.1864\n",
            "Epoch 6, Validation Loss: 0.1785\n",
            "Epoch 7, Validation Loss: 0.1737\n",
            "Epoch 8, Validation Loss: 0.1702\n",
            "Epoch 9, Validation Loss: 0.1673\n",
            "Epoch 10, Validation Loss: 0.1648\n",
            "Validation Accuracy: 0.9473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID74: 146822"
      ],
      "metadata": {
        "id": "UjWdK3ZsnyGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146822)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhoQnCEkgGMs",
        "outputId": "3d42dac5-8b5c-47b8-e1bc-f8d54133add8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.9233\n",
            "Epoch 2, Validation Loss: 1.8733\n",
            "Epoch 3, Validation Loss: 1.8190\n",
            "Epoch 4, Validation Loss: 1.7579\n",
            "Epoch 5, Validation Loss: 1.6870\n",
            "Epoch 6, Validation Loss: 1.6068\n",
            "Epoch 7, Validation Loss: 1.5206\n",
            "Epoch 8, Validation Loss: 1.4356\n",
            "Epoch 9, Validation Loss: 1.3555\n",
            "Epoch 10, Validation Loss: 1.2816\n",
            "Validation Accuracy: 0.6429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID75: 146195"
      ],
      "metadata": {
        "id": "0IpaEPTSnyGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146195)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCScBJ2agHDD",
        "outputId": "013af9a2-2bff-48f4-ccaa-9bb7febe7eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6205\n",
            "Epoch 2, Validation Loss: 0.5788\n",
            "Epoch 3, Validation Loss: 0.5567\n",
            "Epoch 4, Validation Loss: 0.5365\n",
            "Epoch 5, Validation Loss: 0.5192\n",
            "Epoch 6, Validation Loss: 0.5063\n",
            "Epoch 7, Validation Loss: 0.4956\n",
            "Epoch 8, Validation Loss: 0.4878\n",
            "Epoch 9, Validation Loss: 0.4815\n",
            "Epoch 10, Validation Loss: 0.4753\n",
            "Validation Accuracy: 0.8122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID76: 146800"
      ],
      "metadata": {
        "id": "_ukM6eePnyGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146800)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltcaq7wHgH8l",
        "outputId": "ce22037c-df59-484d-c08c-5ab9ed46c7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 2.0621\n",
            "Epoch 2, Validation Loss: 2.0422\n",
            "Epoch 3, Validation Loss: 2.0219\n",
            "Epoch 4, Validation Loss: 2.0011\n",
            "Epoch 5, Validation Loss: 1.9783\n",
            "Epoch 6, Validation Loss: 1.9540\n",
            "Epoch 7, Validation Loss: 1.9259\n",
            "Epoch 8, Validation Loss: 1.8951\n",
            "Epoch 9, Validation Loss: 1.8606\n",
            "Epoch 10, Validation Loss: 1.8221\n",
            "Validation Accuracy: 0.5787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID77: 146817"
      ],
      "metadata": {
        "id": "E2qBcRownyGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146817)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmS70aACgIky",
        "outputId": "1b6f344a-d63a-42b4-8ae2-9a056bfe979a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.8724\n",
            "Epoch 2, Validation Loss: 1.8041\n",
            "Epoch 3, Validation Loss: 1.7286\n",
            "Epoch 4, Validation Loss: 1.6425\n",
            "Epoch 5, Validation Loss: 1.5520\n",
            "Epoch 6, Validation Loss: 1.4630\n",
            "Epoch 7, Validation Loss: 1.3808\n",
            "Epoch 8, Validation Loss: 1.3095\n",
            "Epoch 9, Validation Loss: 1.2487\n",
            "Epoch 10, Validation Loss: 1.1969\n",
            "Validation Accuracy: 0.5424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID78: 146819"
      ],
      "metadata": {
        "id": "-YXpJEChnyGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146819)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hOPeV4bgJO_",
        "outputId": "950ef55d-d31c-43ba-f41c-ab14334ed4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6470\n",
            "Epoch 2, Validation Loss: 0.6295\n",
            "Epoch 3, Validation Loss: 0.6127\n",
            "Epoch 4, Validation Loss: 0.5961\n",
            "Epoch 5, Validation Loss: 0.5802\n",
            "Epoch 6, Validation Loss: 0.5644\n",
            "Epoch 7, Validation Loss: 0.5490\n",
            "Epoch 8, Validation Loss: 0.5338\n",
            "Epoch 9, Validation Loss: 0.5189\n",
            "Epoch 10, Validation Loss: 0.5042\n",
            "Validation Accuracy: 0.9074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID79: 146821"
      ],
      "metadata": {
        "id": "mpXisXr9nyGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=146821)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAlp5jI2gJx3",
        "outputId": "3ab888dc-5bcf-4292-c281-b3ad9e5e3069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.3637\n",
            "Epoch 2, Validation Loss: 1.3112\n",
            "Epoch 3, Validation Loss: 1.2607\n",
            "Epoch 4, Validation Loss: 1.2085\n",
            "Epoch 5, Validation Loss: 1.1542\n",
            "Epoch 6, Validation Loss: 1.0967\n",
            "Epoch 7, Validation Loss: 1.0378\n",
            "Epoch 8, Validation Loss: 0.9819\n",
            "Epoch 9, Validation Loss: 0.9317\n",
            "Epoch 10, Validation Loss: 0.8885\n",
            "Validation Accuracy: 0.6792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID80: 14954"
      ],
      "metadata": {
        "id": "5OYkiwzPnyGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=14954)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfT0wXzagKac",
        "outputId": "4fd1fbab-d405-45cd-c5a5-fe7ac9798098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.6938\n",
            "Epoch 2, Validation Loss: 0.6906\n",
            "Epoch 3, Validation Loss: 0.6875\n",
            "Epoch 4, Validation Loss: 0.6842\n",
            "Epoch 5, Validation Loss: 0.6810\n",
            "Epoch 6, Validation Loss: 0.6775\n",
            "Epoch 7, Validation Loss: 0.6740\n",
            "Epoch 8, Validation Loss: 0.6703\n",
            "Epoch 9, Validation Loss: 0.6663\n",
            "Epoch 10, Validation Loss: 0.6620\n",
            "Validation Accuracy: 0.6204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID81: 167141"
      ],
      "metadata": {
        "id": "NZRqw3knnyGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=167141)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxzeCSopgLTx",
        "outputId": "a5294100-811c-4de6-efc7-a8939031f20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.5772\n",
            "Epoch 2, Validation Loss: 0.4845\n",
            "Epoch 3, Validation Loss: 0.4260\n",
            "Epoch 4, Validation Loss: 0.3952\n",
            "Epoch 5, Validation Loss: 0.3730\n",
            "Epoch 6, Validation Loss: 0.3538\n",
            "Epoch 7, Validation Loss: 0.3358\n",
            "Epoch 8, Validation Loss: 0.3198\n",
            "Epoch 9, Validation Loss: 0.3072\n",
            "Epoch 10, Validation Loss: 0.2973\n",
            "Validation Accuracy: 0.8610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID82: 167140"
      ],
      "metadata": {
        "id": "nchR5_2ynyGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=167140)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovLIKw_mgMPy",
        "outputId": "189a93ce-5223-4503-cb3d-ee1128544c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 1.0362\n",
            "Epoch 2, Validation Loss: 0.9816\n",
            "Epoch 3, Validation Loss: 0.9197\n",
            "Epoch 4, Validation Loss: 0.8465\n",
            "Epoch 5, Validation Loss: 0.7579\n",
            "Epoch 6, Validation Loss: 0.6573\n",
            "Epoch 7, Validation Loss: 0.5527\n",
            "Epoch 8, Validation Loss: 0.4574\n",
            "Epoch 9, Validation Loss: 0.3820\n",
            "Epoch 10, Validation Loss: 0.3258\n",
            "Validation Accuracy: 0.9060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID83: 167125"
      ],
      "metadata": {
        "id": "t0WH4gnfnyGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "X, y = load_openml_data(task_id=167125)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Update args based on data\n",
        "args[\"num_features\"] = X_train.shape[1]\n",
        "args[\"num_classes\"] = len(np.unique(y))\n",
        "\n",
        "# Initialize and train the model\n",
        "mlp_model = initialize_mlp(params[\"n_layers\"], args[\"num_features\"], params[\"hidden_dim\"], args[\"num_classes\"], args[\"objective\"])\n",
        "trained_model = train_mlp(mlp_model, X_train, y_train, X_val, y_val, args, params)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "predictions = predict_mlp(trained_model, X_val, args)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKt3HX1EgNI4",
        "outputId": "dfe0e1dd-f9a7-4131-d978-78cb4ad593f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.5308\n",
            "Epoch 2, Validation Loss: 0.3286\n",
            "Epoch 3, Validation Loss: 0.1991\n",
            "Epoch 4, Validation Loss: 0.1481\n",
            "Epoch 5, Validation Loss: 0.1243\n",
            "Epoch 6, Validation Loss: 0.1097\n",
            "Epoch 7, Validation Loss: 0.0990\n",
            "Epoch 8, Validation Loss: 0.0946\n",
            "Epoch 9, Validation Loss: 0.0911\n",
            "Epoch 10, Validation Loss: 0.0889\n",
            "Validation Accuracy: 0.9710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID84: 167124"
      ],
      "metadata": {
        "id": "X_RzOmV2nyGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID85: 167121"
      ],
      "metadata": {
        "id": "tOE_hQOznyGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fijmKftpUUM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load tabular data from OpenML with preprocessing\n",
        "def load_openml_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = openml.datasets.get_dataset(task.dataset_id)\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Handle missing values\n",
        "    imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "    X = imputer.fit_transform(X)\n",
        "\n",
        "    # Convert categorical columns in X to numerical codes\n",
        "    for col in range(X.shape[1]):\n",
        "        if isinstance(X[0, col], str):\n",
        "            X[:, col] = pd.factorize(X[:, col])[0]\n",
        "\n",
        "    # Convert target y to numeric if needed and ensure it's a numpy array\n",
        "    if isinstance(y[0], str):\n",
        "        y = pd.factorize(y)[0]\n",
        "    y = np.array(y)  # Ensure y is a numpy array\n",
        "\n",
        "    return X.astype(np.float32), y.astype(np.int64)\n",
        "\n",
        "# Define a custom dataset for tabular data\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Define a simple ResNet-like model for tabular data\n",
        "class ResNetTabular(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(ResNetTabular, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.fc1(x))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "# Get data loaders\n",
        "def get_data_loaders(X, y, batch_size):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "\n",
        "    train_dataset = TabularDataset(X_train, np.array(y_train))\n",
        "    val_dataset = TabularDataset(X_val, np.array(y_val))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "-ctJ1HAC_mLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID1: 14965"
      ],
      "metadata": {
        "id": "bGnNMZgOoGny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 14965  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytlHOSnbUwoH",
        "outputId": "ad9970c7-e61f-4cad-8a9d-bcefa52cf340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4260\n",
            "Epoch [2/10], Loss: 0.5946\n",
            "Epoch [3/10], Loss: 0.1920\n",
            "Epoch [4/10], Loss: 0.2464\n",
            "Epoch [5/10], Loss: 0.0870\n",
            "Epoch [6/10], Loss: 0.1267\n",
            "Epoch [7/10], Loss: 0.0284\n",
            "Epoch [8/10], Loss: 0.2418\n",
            "Epoch [9/10], Loss: 0.1787\n",
            "Epoch [10/10], Loss: 0.5847\n",
            "Accuracy: 0.8986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID2: 9977"
      ],
      "metadata": {
        "id": "UwZED4OKoGnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9977  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdrtsPQzVGEP",
        "outputId": "779ef7cd-fbe5-4b1c-a490-4e270765c9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0974\n",
            "Epoch [2/10], Loss: 0.2079\n",
            "Epoch [3/10], Loss: 0.2211\n",
            "Epoch [4/10], Loss: 0.1062\n",
            "Epoch [5/10], Loss: 0.2668\n",
            "Epoch [6/10], Loss: 0.1894\n",
            "Epoch [7/10], Loss: 0.0312\n",
            "Epoch [8/10], Loss: 0.0508\n",
            "Epoch [9/10], Loss: 0.2419\n",
            "Epoch [10/10], Loss: 0.0775\n",
            "Accuracy: 0.9569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNkUQeOXoGnz"
      },
      "source": [
        "# Task ID3: 34539"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 34539  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPGAxWnaVG-R",
        "outputId": "fba58a4a-059b-420f-bbba-1f9af171b112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0910\n",
            "Epoch [2/10], Loss: 0.4356\n",
            "Epoch [3/10], Loss: 0.0497\n",
            "Epoch [4/10], Loss: 0.4178\n",
            "Epoch [5/10], Loss: 0.0615\n",
            "Epoch [6/10], Loss: 0.0547\n",
            "Epoch [7/10], Loss: 0.4389\n",
            "Epoch [8/10], Loss: 0.0719\n",
            "Epoch [9/10], Loss: 0.0612\n",
            "Epoch [10/10], Loss: 0.0526\n",
            "Accuracy: 0.9437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aizv2PVToGnz"
      },
      "source": [
        "# Task ID4: 146606"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146606  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kqbDzHdVHuL",
        "outputId": "8ea9bb2f-87af-4b33-f2ba-2a7bba7a99aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7213\n",
            "Epoch [2/10], Loss: 0.4284\n",
            "Epoch [3/10], Loss: 0.6582\n",
            "Epoch [4/10], Loss: 0.5026\n",
            "Epoch [5/10], Loss: 0.6339\n",
            "Epoch [6/10], Loss: 0.4914\n",
            "Epoch [7/10], Loss: 0.3908\n",
            "Epoch [8/10], Loss: 0.5078\n",
            "Epoch [9/10], Loss: 0.4592\n",
            "Epoch [10/10], Loss: 0.5521\n",
            "Accuracy: 0.7110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8geuVnnoGnz"
      },
      "source": [
        "# Task ID5: 7592\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 7592  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6UmYlAHVIyM",
        "outputId": "045be0fa-13d9-4a58-ddf5-0f6092c58f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0014\n",
            "Epoch [2/10], Loss: 0.6058\n",
            "Epoch [3/10], Loss: 0.8110\n",
            "Epoch [4/10], Loss: 0.0000\n",
            "Epoch [5/10], Loss: 0.0696\n",
            "Epoch [6/10], Loss: 0.3649\n",
            "Epoch [7/10], Loss: 0.7185\n",
            "Epoch [8/10], Loss: 0.0277\n",
            "Epoch [9/10], Loss: 0.0139\n",
            "Epoch [10/10], Loss: 0.0697\n",
            "Accuracy: 0.8547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inqT814ZoGnz"
      },
      "source": [
        "# Task ID6: 146195\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146195  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct9rKIAAVJlJ",
        "outputId": "193ffa5d-6723-4e12-d789-14444de40b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.8686\n",
            "Epoch [2/10], Loss: 0.9432\n",
            "Epoch [3/10], Loss: 0.6589\n",
            "Epoch [4/10], Loss: 0.7128\n",
            "Epoch [5/10], Loss: 0.8151\n",
            "Epoch [6/10], Loss: 0.9076\n",
            "Epoch [7/10], Loss: 0.5395\n",
            "Epoch [8/10], Loss: 0.7531\n",
            "Epoch [9/10], Loss: 0.4656\n",
            "Epoch [10/10], Loss: 0.6060\n",
            "Accuracy: 0.7419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fLaraZioGn0"
      },
      "source": [
        "# Task ID7: 167119\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 167119  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2VTHfgFVKnR",
        "outputId": "08f8b198-98cf-48c2-95c3-90059de13b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7850\n",
            "Epoch [2/10], Loss: 0.5707\n",
            "Epoch [3/10], Loss: 0.5016\n",
            "Epoch [4/10], Loss: 0.4219\n",
            "Epoch [5/10], Loss: 0.5030\n",
            "Epoch [6/10], Loss: 0.2640\n",
            "Epoch [7/10], Loss: 0.3358\n",
            "Epoch [8/10], Loss: 0.5580\n",
            "Epoch [9/10], Loss: 0.3370\n",
            "Epoch [10/10], Loss: 0.6294\n",
            "Accuracy: 0.8168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4wcad19oGn0"
      },
      "source": [
        "# Task ID8: 167120\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 167120  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh5EOzGIVMMx",
        "outputId": "c67fd076-fc01-45e3-edd1-f7059ea5839c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6991\n",
            "Epoch [2/10], Loss: 0.6941\n",
            "Epoch [3/10], Loss: 0.6848\n",
            "Epoch [4/10], Loss: 0.6814\n",
            "Epoch [5/10], Loss: 0.6823\n",
            "Epoch [6/10], Loss: 0.7074\n",
            "Epoch [7/10], Loss: 0.6843\n",
            "Epoch [8/10], Loss: 0.6936\n",
            "Epoch [9/10], Loss: 0.6967\n",
            "Epoch [10/10], Loss: 0.6904\n",
            "Accuracy: 0.5147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytz8Q4cDoGn0"
      },
      "source": [
        "# Task ID11: \t168331\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 168331 # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2qsoS8HVNad",
        "outputId": "ab09177b-8aed-44e3-ec80-da4ab3478094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.2453\n",
            "Epoch [2/10], Loss: 1.2675\n",
            "Epoch [3/10], Loss: 1.3478\n",
            "Epoch [4/10], Loss: 1.6738\n",
            "Epoch [5/10], Loss: 1.0183\n",
            "Epoch [6/10], Loss: 0.6520\n",
            "Epoch [7/10], Loss: 1.0676\n",
            "Epoch [8/10], Loss: 1.2553\n",
            "Epoch [9/10], Loss: 1.3280\n",
            "Epoch [10/10], Loss: 1.3362\n",
            "Accuracy: 0.6153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dODnKeHwoGn1"
      },
      "source": [
        "# Task ID12: \t168330\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 168330  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWSRP0DWVOQS",
        "outputId": "0f42bf2a-2a16-4784-dbe7-3e8110c43914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.9018\n",
            "Epoch [2/10], Loss: 0.9688\n",
            "Epoch [3/10], Loss: 1.2281\n",
            "Epoch [4/10], Loss: 0.5237\n",
            "Epoch [5/10], Loss: 0.6596\n",
            "Epoch [6/10], Loss: 0.8224\n",
            "Epoch [7/10], Loss: 0.5093\n",
            "Epoch [8/10], Loss: 0.7616\n",
            "Epoch [9/10], Loss: 0.5017\n",
            "Epoch [10/10], Loss: 0.7088\n",
            "Accuracy: 0.6976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0NB-QyVoGn1"
      },
      "source": [
        "# Task ID13: \t168335\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 168335  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lySDofyjVPC5",
        "outputId": "d34b2f21-0550-4428-bb80-0218bba0bafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2640\n",
            "Epoch [2/10], Loss: 0.3778\n",
            "Epoch [3/10], Loss: 0.2037\n",
            "Epoch [4/10], Loss: 0.4725\n",
            "Epoch [5/10], Loss: 0.1526\n",
            "Epoch [6/10], Loss: 0.2035\n",
            "Epoch [7/10], Loss: 0.2507\n",
            "Epoch [8/10], Loss: 0.0468\n",
            "Epoch [9/10], Loss: 0.0393\n",
            "Epoch [10/10], Loss: 0.0797\n",
            "Accuracy: 0.9237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODIsI5MnoGn1"
      },
      "source": [
        "\n",
        "# Task ID16: \t146212\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146212  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5heVK7IFVQVd",
        "outputId": "47f292e2-9b8f-462e-e9ef-c812c338bd52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0848\n",
            "Epoch [2/10], Loss: 0.0027\n",
            "Epoch [3/10], Loss: 0.0044\n",
            "Epoch [4/10], Loss: 0.0607\n",
            "Epoch [5/10], Loss: 0.0098\n",
            "Epoch [6/10], Loss: 0.0027\n",
            "Epoch [7/10], Loss: 0.0029\n",
            "Epoch [8/10], Loss: 0.0078\n",
            "Epoch [9/10], Loss: 0.0013\n",
            "Epoch [10/10], Loss: 0.0029\n",
            "Accuracy: 0.9967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu2mTuaboGn2"
      },
      "source": [
        "# Task ID19: \t168868\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 168868  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8udT0ObfVRcp",
        "outputId": "48911c75-e331-4248-ffd7-615c9c3a6320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0231\n",
            "Epoch [2/10], Loss: 0.0178\n",
            "Epoch [3/10], Loss: 0.0001\n",
            "Epoch [4/10], Loss: 0.0014\n",
            "Epoch [5/10], Loss: 0.0202\n",
            "Epoch [6/10], Loss: 0.0688\n",
            "Epoch [7/10], Loss: 0.0147\n",
            "Epoch [8/10], Loss: 0.0085\n",
            "Epoch [9/10], Loss: 0.0034\n",
            "Epoch [10/10], Loss: 0.0060\n",
            "Accuracy: 0.9904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID20: 31"
      ],
      "metadata": {
        "id": "OwZVsl4JoGn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 31  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR9YiEEE-FfZ",
        "outputId": "d5cc0db9-6168-4ff5-8a8e-d0601e167af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6869\n",
            "Epoch [2/10], Loss: 0.5056\n",
            "Epoch [3/10], Loss: 0.5218\n",
            "Epoch [4/10], Loss: 0.4387\n",
            "Epoch [5/10], Loss: 0.5354\n",
            "Epoch [6/10], Loss: 0.5089\n",
            "Epoch [7/10], Loss: 0.6623\n",
            "Epoch [8/10], Loss: 0.5546\n",
            "Epoch [9/10], Loss: 0.5116\n",
            "Epoch [10/10], Loss: 0.5411\n",
            "Accuracy: 0.7700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID21: 10101"
      ],
      "metadata": {
        "id": "6D19ZnNVoGn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 10101  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tn5a68V-Ihb",
        "outputId": "26de17b5-3d18-4f54-94ea-f754dba36bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5918\n",
            "Epoch [2/10], Loss: 0.4164\n",
            "Epoch [3/10], Loss: 0.4128\n",
            "Epoch [4/10], Loss: 0.5102\n",
            "Epoch [5/10], Loss: 0.5324\n",
            "Epoch [6/10], Loss: 0.5481\n",
            "Epoch [7/10], Loss: 0.4402\n",
            "Epoch [8/10], Loss: 0.5139\n",
            "Epoch [9/10], Loss: 0.4102\n",
            "Epoch [10/10], Loss: 0.6876\n",
            "Accuracy: 0.7667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID22: \t3913"
      ],
      "metadata": {
        "id": "38Wc_fieoGn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3913  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKYE6nMH-Lsa",
        "outputId": "b4416930-6f6a-4407-ab5d-2231322aa97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5112\n",
            "Epoch [2/10], Loss: 0.2776\n",
            "Epoch [3/10], Loss: 1.0583\n",
            "Epoch [4/10], Loss: 0.2997\n",
            "Epoch [5/10], Loss: 0.0267\n",
            "Epoch [6/10], Loss: 0.0063\n",
            "Epoch [7/10], Loss: 0.0171\n",
            "Epoch [8/10], Loss: 0.0147\n",
            "Epoch [9/10], Loss: 0.0334\n",
            "Epoch [10/10], Loss: 0.0413\n",
            "Accuracy: 0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID23: 3"
      ],
      "metadata": {
        "id": "_XzylqDkoGn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3 # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK8OI4mO-PR3",
        "outputId": "1f0f4e06-4c78-4a5b-ada2-36f643f0163d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3384\n",
            "Epoch [2/10], Loss: 0.1961\n",
            "Epoch [3/10], Loss: 0.1481\n",
            "Epoch [4/10], Loss: 0.1710\n",
            "Epoch [5/10], Loss: 0.1064\n",
            "Epoch [6/10], Loss: 0.1497\n",
            "Epoch [7/10], Loss: 0.0458\n",
            "Epoch [8/10], Loss: 0.1258\n",
            "Epoch [9/10], Loss: 0.0724\n",
            "Epoch [10/10], Loss: 0.0192\n",
            "Accuracy: 0.9672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID24: 3917"
      ],
      "metadata": {
        "id": "6LnFiL76oGn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3917  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYni-Y0T-R7q",
        "outputId": "38aa6975-4830-4476-a673-dec6457c4c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4596\n",
            "Epoch [2/10], Loss: 0.5719\n",
            "Epoch [3/10], Loss: 0.4327\n",
            "Epoch [4/10], Loss: 0.3523\n",
            "Epoch [5/10], Loss: 0.2566\n",
            "Epoch [6/10], Loss: 0.4604\n",
            "Epoch [7/10], Loss: 0.4556\n",
            "Epoch [8/10], Loss: 0.1894\n",
            "Epoch [9/10], Loss: 0.3987\n",
            "Epoch [10/10], Loss: 0.3939\n",
            "Accuracy: 0.8555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID25: 9957"
      ],
      "metadata": {
        "id": "Lv_VPfTzoGn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9957 # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnCU6HZ1-VD_",
        "outputId": "8594f1a2-e3db-467e-f247-5b0f923df596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6627\n",
            "Epoch [2/10], Loss: 0.8670\n",
            "Epoch [3/10], Loss: 0.4477\n",
            "Epoch [4/10], Loss: 0.7525\n",
            "Epoch [5/10], Loss: 0.3378\n",
            "Epoch [6/10], Loss: 0.2250\n",
            "Epoch [7/10], Loss: 0.1704\n",
            "Epoch [8/10], Loss: 0.3046\n",
            "Epoch [9/10], Loss: 0.1955\n",
            "Epoch [10/10], Loss: 0.1029\n",
            "Accuracy: 0.8768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID26: 9946"
      ],
      "metadata": {
        "id": "KgwCJdbWoGn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9946  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csjO4u3v-Ycg",
        "outputId": "f9687932-afa7-47c2-e516-eb390ad4fd80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5340\n",
            "Epoch [2/10], Loss: 0.2510\n",
            "Epoch [3/10], Loss: 0.1707\n",
            "Epoch [4/10], Loss: 0.0688\n",
            "Epoch [5/10], Loss: 0.0360\n",
            "Epoch [6/10], Loss: 0.6044\n",
            "Epoch [7/10], Loss: 0.1031\n",
            "Epoch [8/10], Loss: 0.1066\n",
            "Epoch [9/10], Loss: 0.0260\n",
            "Epoch [10/10], Loss: 0.0106\n",
            "Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID27: 3918"
      ],
      "metadata": {
        "id": "QhkKNslIoGn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3918  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiCG0yJ-GR5y",
        "outputId": "b037760b-9c9a-4ed0-f2f9-6b44faf984f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3709\n",
            "Epoch [2/10], Loss: 0.1950\n",
            "Epoch [3/10], Loss: 0.2589\n",
            "Epoch [4/10], Loss: 0.1441\n",
            "Epoch [5/10], Loss: 0.1903\n",
            "Epoch [6/10], Loss: 0.2888\n",
            "Epoch [7/10], Loss: 0.2672\n",
            "Epoch [8/10], Loss: 0.1748\n",
            "Epoch [9/10], Loss: 0.1271\n",
            "Epoch [10/10], Loss: 0.0570\n",
            "Accuracy: 0.9234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID28: 3903"
      ],
      "metadata": {
        "id": "3LiQqHAPoGn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3903  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWDkzsiWGUlz",
        "outputId": "84b5f799-8301-48c2-fd88-96cd0636ab6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.1874\n",
            "Epoch [2/10], Loss: 0.0021\n",
            "Epoch [3/10], Loss: 0.0902\n",
            "Epoch [4/10], Loss: 0.0500\n",
            "Epoch [5/10], Loss: 0.2451\n",
            "Epoch [6/10], Loss: 0.0912\n",
            "Epoch [7/10], Loss: 0.2821\n",
            "Epoch [8/10], Loss: 0.0349\n",
            "Epoch [9/10], Loss: 0.0121\n",
            "Epoch [10/10], Loss: 0.2675\n",
            "Accuracy: 0.8946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID29: 37"
      ],
      "metadata": {
        "id": "JQ7TEXbyoGn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 37  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuG830SqGVbv",
        "outputId": "5ea568b6-b38e-41d3-d17d-2bf241cc91f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7980\n",
            "Epoch [2/10], Loss: 0.5303\n",
            "Epoch [3/10], Loss: 0.4557\n",
            "Epoch [4/10], Loss: 0.3613\n",
            "Epoch [5/10], Loss: 0.2631\n",
            "Epoch [6/10], Loss: 0.2889\n",
            "Epoch [7/10], Loss: 0.7615\n",
            "Epoch [8/10], Loss: 0.6689\n",
            "Epoch [9/10], Loss: 0.3163\n",
            "Epoch [10/10], Loss: 1.0312\n",
            "Accuracy: 0.7727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID30: 9971"
      ],
      "metadata": {
        "id": "uMi773nEoGn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9971  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VCXEp4MGWLh",
        "outputId": "d3b6dc8d-9afd-4213-dfe7-09c829dacd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7102\n",
            "Epoch [2/10], Loss: 0.6663\n",
            "Epoch [3/10], Loss: 0.6042\n",
            "Epoch [4/10], Loss: 0.5484\n",
            "Epoch [5/10], Loss: 0.5739\n",
            "Epoch [6/10], Loss: 0.5683\n",
            "Epoch [7/10], Loss: 0.4374\n",
            "Epoch [8/10], Loss: 0.5640\n",
            "Epoch [9/10], Loss: 0.4436\n",
            "Epoch [10/10], Loss: 0.5529\n",
            "Accuracy: 0.7521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID31: 9952"
      ],
      "metadata": {
        "id": "mKX-4Cg5oGn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9952  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdLWKWR3GYSa",
        "outputId": "11831023-997e-4dc8-fd96-24e77ec53782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4420\n",
            "Epoch [2/10], Loss: 0.2358\n",
            "Epoch [3/10], Loss: 0.6512\n",
            "Epoch [4/10], Loss: 0.2106\n",
            "Epoch [5/10], Loss: 0.0253\n",
            "Epoch [6/10], Loss: 0.0595\n",
            "Epoch [7/10], Loss: 0.2756\n",
            "Epoch [8/10], Loss: 0.7425\n",
            "Epoch [9/10], Loss: 0.1605\n",
            "Epoch [10/10], Loss: 0.1029\n",
            "Accuracy: 0.8409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID32: 3902"
      ],
      "metadata": {
        "id": "BiqcgeaNoGn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3902 # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbY26bfxGZOZ",
        "outputId": "a677dcd4-b417-4364-a28b-6d5ffde7ea97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5660\n",
            "Epoch [2/10], Loss: 0.2736\n",
            "Epoch [3/10], Loss: 0.5322\n",
            "Epoch [4/10], Loss: 0.1324\n",
            "Epoch [5/10], Loss: 0.2318\n",
            "Epoch [6/10], Loss: 0.0899\n",
            "Epoch [7/10], Loss: 0.0418\n",
            "Epoch [8/10], Loss: 0.1281\n",
            "Epoch [9/10], Loss: 0.2423\n",
            "Epoch [10/10], Loss: 0.1533\n",
            "Accuracy: 0.9007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID33: 49"
      ],
      "metadata": {
        "id": "4ishg2DjoGn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 49  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKe4kQ0qGgNe",
        "outputId": "22e1021f-9711-4f21-b8a3-ab47a623c034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6397\n",
            "Epoch [2/10], Loss: 0.6337\n",
            "Epoch [3/10], Loss: 0.6754\n",
            "Epoch [4/10], Loss: 0.5163\n",
            "Epoch [5/10], Loss: 0.5286\n",
            "Epoch [6/10], Loss: 0.5523\n",
            "Epoch [7/10], Loss: 0.5079\n",
            "Epoch [8/10], Loss: 0.6817\n",
            "Epoch [9/10], Loss: 0.4796\n",
            "Epoch [10/10], Loss: 0.5917\n",
            "Accuracy: 0.7448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID34: 43"
      ],
      "metadata": {
        "id": "m_TB8qxxoGn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 43  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxH-98QvGhAa",
        "outputId": "43111517-2b8c-4221-c308-f4a55b189cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3681\n",
            "Epoch [2/10], Loss: 0.4118\n",
            "Epoch [3/10], Loss: 0.1910\n",
            "Epoch [4/10], Loss: 0.1333\n",
            "Epoch [5/10], Loss: 0.3396\n",
            "Epoch [6/10], Loss: 0.3177\n",
            "Epoch [7/10], Loss: 0.2570\n",
            "Epoch [8/10], Loss: 0.0715\n",
            "Epoch [9/10], Loss: 0.1488\n",
            "Epoch [10/10], Loss: 0.2670\n",
            "Accuracy: 0.9414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID35: 9978"
      ],
      "metadata": {
        "id": "RAaJNcz0oGn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9978  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfcc7zTNGhoT",
        "outputId": "92b58122-f266-4f4b-e525-aaf8d6c6b199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2508\n",
            "Epoch [2/10], Loss: 0.0895\n",
            "Epoch [3/10], Loss: 0.2939\n",
            "Epoch [4/10], Loss: 0.2479\n",
            "Epoch [5/10], Loss: 0.1636\n",
            "Epoch [6/10], Loss: 0.0165\n",
            "Epoch [7/10], Loss: 0.0959\n",
            "Epoch [8/10], Loss: 0.0246\n",
            "Epoch [9/10], Loss: 0.0195\n",
            "Epoch [10/10], Loss: 0.0221\n",
            "Accuracy: 0.9428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID36: 10093"
      ],
      "metadata": {
        "id": "Tvs_fZFWoGn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 10093 # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILLstvLvGiO3",
        "outputId": "8c196b68-3dbd-43fe-e9e8-fab27134ab14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3740\n",
            "Epoch [2/10], Loss: 0.1824\n",
            "Epoch [3/10], Loss: 0.0304\n",
            "Epoch [4/10], Loss: 0.0635\n",
            "Epoch [5/10], Loss: 0.0438\n",
            "Epoch [6/10], Loss: 0.3713\n",
            "Epoch [7/10], Loss: 0.0670\n",
            "Epoch [8/10], Loss: 0.0077\n",
            "Epoch [9/10], Loss: 0.0743\n",
            "Epoch [10/10], Loss: 0.0029\n",
            "Accuracy: 0.9927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID37: 219"
      ],
      "metadata": {
        "id": "whY8b37moGn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 219  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i9uhhDJGi97",
        "outputId": "7a0fb322-5631-4b29-849f-2109e0423607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4165\n",
            "Epoch [2/10], Loss: 0.5305\n",
            "Epoch [3/10], Loss: 0.4529\n",
            "Epoch [4/10], Loss: 0.4580\n",
            "Epoch [5/10], Loss: 0.6772\n",
            "Epoch [6/10], Loss: 0.5069\n",
            "Epoch [7/10], Loss: 0.5180\n",
            "Epoch [8/10], Loss: 0.2981\n",
            "Epoch [9/10], Loss: 0.5443\n",
            "Epoch [10/10], Loss: 0.4480\n",
            "Accuracy: 0.7951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID38: 9976"
      ],
      "metadata": {
        "id": "hnhVIAZ1oGn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9976  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHZZtmHpGj1-",
        "outputId": "167fafa7-1d73-4cbe-e77b-9060782169b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6845\n",
            "Epoch [2/10], Loss: 0.5947\n",
            "Epoch [3/10], Loss: 0.4864\n",
            "Epoch [4/10], Loss: 0.6458\n",
            "Epoch [5/10], Loss: 0.4855\n",
            "Epoch [6/10], Loss: 0.2363\n",
            "Epoch [7/10], Loss: 0.2353\n",
            "Epoch [8/10], Loss: 0.0765\n",
            "Epoch [9/10], Loss: 0.3338\n",
            "Epoch [10/10], Loss: 0.2892\n",
            "Accuracy: 0.5731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID39: 6"
      ],
      "metadata": {
        "id": "g3kEB6RfoGn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 6  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyQs1IqdGk1N",
        "outputId": "d62adcd6-5041-4238-857c-dd5e477d2474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.9848\n",
            "Epoch [2/10], Loss: 1.1941\n",
            "Epoch [3/10], Loss: 1.0551\n",
            "Epoch [4/10], Loss: 1.0708\n",
            "Epoch [5/10], Loss: 1.2120\n",
            "Epoch [6/10], Loss: 0.8768\n",
            "Epoch [7/10], Loss: 1.1595\n",
            "Epoch [8/10], Loss: 0.5226\n",
            "Epoch [9/10], Loss: 1.2247\n",
            "Epoch [10/10], Loss: 0.6530\n",
            "Accuracy: 0.8377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID40: 53"
      ],
      "metadata": {
        "id": "nAN7X_ZEoGn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 53  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vybhbcENGl07",
        "outputId": "5032a44a-8f5a-44ff-b965-91a9647d4dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3027\n",
            "Epoch [2/10], Loss: 1.2649\n",
            "Epoch [3/10], Loss: 0.9712\n",
            "Epoch [4/10], Loss: 0.6766\n",
            "Epoch [5/10], Loss: 0.8990\n",
            "Epoch [6/10], Loss: 0.5625\n",
            "Epoch [7/10], Loss: 0.7454\n",
            "Epoch [8/10], Loss: 0.9923\n",
            "Epoch [9/10], Loss: 0.5062\n",
            "Epoch [10/10], Loss: 0.5359\n",
            "Accuracy: 0.7824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID41: 11"
      ],
      "metadata": {
        "id": "OcmPsMUNoGn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 11  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwpsPjLuGmaD",
        "outputId": "7cae04d4-888b-4377-8bd2-865fadc2e2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.9080\n",
            "Epoch [2/10], Loss: 0.8733\n",
            "Epoch [3/10], Loss: 0.8591\n",
            "Epoch [4/10], Loss: 0.4565\n",
            "Epoch [5/10], Loss: 0.5395\n",
            "Epoch [6/10], Loss: 0.4304\n",
            "Epoch [7/10], Loss: 0.2466\n",
            "Epoch [8/10], Loss: 0.2302\n",
            "Epoch [9/10], Loss: 0.3437\n",
            "Epoch [10/10], Loss: 0.4615\n",
            "Accuracy: 0.9040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID42: 15"
      ],
      "metadata": {
        "id": "qY7NHIfOoGn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 15  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQQnv5aSGnDE",
        "outputId": "e4e8a1c6-35ca-4e44-99b4-34f88560d58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4783\n",
            "Epoch [2/10], Loss: 0.1655\n",
            "Epoch [3/10], Loss: 0.3796\n",
            "Epoch [4/10], Loss: 0.0724\n",
            "Epoch [5/10], Loss: 0.0301\n",
            "Epoch [6/10], Loss: 0.2333\n",
            "Epoch [7/10], Loss: 0.6262\n",
            "Epoch [8/10], Loss: 0.0076\n",
            "Epoch [9/10], Loss: 0.0314\n",
            "Epoch [10/10], Loss: 0.0277\n",
            "Accuracy: 0.9714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID43: 16"
      ],
      "metadata": {
        "id": "0zjmhlvxoGn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 16  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wkqL0E1Gnz-",
        "outputId": "69294c39-7296-488b-9d0a-c1ae9204b50c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.9781\n",
            "Epoch [2/10], Loss: 0.8679\n",
            "Epoch [3/10], Loss: 0.6595\n",
            "Epoch [4/10], Loss: 0.3748\n",
            "Epoch [5/10], Loss: 0.2482\n",
            "Epoch [6/10], Loss: 0.5316\n",
            "Epoch [7/10], Loss: 0.1599\n",
            "Epoch [8/10], Loss: 0.2725\n",
            "Epoch [9/10], Loss: 0.1761\n",
            "Epoch [10/10], Loss: 0.1800\n",
            "Accuracy: 0.9575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID44: 14"
      ],
      "metadata": {
        "id": "jYV0SxLyoGn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 14  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne_sW0L_Gp0c",
        "outputId": "364326de-c2ce-4973-d257-f73bd0eb9fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.9336\n",
            "Epoch [2/10], Loss: 1.2328\n",
            "Epoch [3/10], Loss: 0.7231\n",
            "Epoch [4/10], Loss: 0.7118\n",
            "Epoch [5/10], Loss: 0.5210\n",
            "Epoch [6/10], Loss: 0.5873\n",
            "Epoch [7/10], Loss: 0.6012\n",
            "Epoch [8/10], Loss: 0.6615\n",
            "Epoch [9/10], Loss: 0.3988\n",
            "Epoch [10/10], Loss: 0.2812\n",
            "Accuracy: 0.8275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID45: 32"
      ],
      "metadata": {
        "id": "OEaOcEJ0oGn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 32  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xghdE8axGrBJ",
        "outputId": "a3d5adde-723b-46bd-ded3-4d75f57ee614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6213\n",
            "Epoch [2/10], Loss: 0.2162\n",
            "Epoch [3/10], Loss: 0.5217\n",
            "Epoch [4/10], Loss: 0.2088\n",
            "Epoch [5/10], Loss: 0.1713\n",
            "Epoch [6/10], Loss: 0.0627\n",
            "Epoch [7/10], Loss: 0.1875\n",
            "Epoch [8/10], Loss: 0.2259\n",
            "Epoch [9/10], Loss: 0.1253\n",
            "Epoch [10/10], Loss: 0.1890\n",
            "Accuracy: 0.9809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID46: 3549"
      ],
      "metadata": {
        "id": "qeb13cX8oGn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3549  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoxmCuN4GrrU",
        "outputId": "bc0ae973-955b-40d9-e905-2c3610ec2e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.0427\n",
            "Epoch [2/10], Loss: 0.6480\n",
            "Epoch [3/10], Loss: 0.2173\n",
            "Epoch [4/10], Loss: 0.3367\n",
            "Epoch [5/10], Loss: 0.1164\n",
            "Epoch [6/10], Loss: 0.0216\n",
            "Epoch [7/10], Loss: 0.0136\n",
            "Epoch [8/10], Loss: 0.0206\n",
            "Epoch [9/10], Loss: 0.0112\n",
            "Epoch [10/10], Loss: 0.0094\n",
            "Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID47: 12"
      ],
      "metadata": {
        "id": "9zEy2LqAoGn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 12 # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIbojVFhGsrc",
        "outputId": "bc05b9c0-3680-41d7-fc1d-89466db37f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.0848\n",
            "Epoch [2/10], Loss: 0.4446\n",
            "Epoch [3/10], Loss: 0.3421\n",
            "Epoch [4/10], Loss: 0.1097\n",
            "Epoch [5/10], Loss: 0.2730\n",
            "Epoch [6/10], Loss: 0.1099\n",
            "Epoch [7/10], Loss: 0.1292\n",
            "Epoch [8/10], Loss: 0.0717\n",
            "Epoch [9/10], Loss: 0.1066\n",
            "Epoch [10/10], Loss: 0.0456\n",
            "Accuracy: 0.9750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID48: 9981"
      ],
      "metadata": {
        "id": "8jCBpsvmoGn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9981  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulPEdAxnGtfj",
        "outputId": "217ba251-70f1-49dd-c4ed-5974d3e9cb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.0943\n",
            "Epoch [2/10], Loss: 1.6230\n",
            "Epoch [3/10], Loss: 0.9115\n",
            "Epoch [4/10], Loss: 0.4620\n",
            "Epoch [5/10], Loss: 0.3348\n",
            "Epoch [6/10], Loss: 0.3186\n",
            "Epoch [7/10], Loss: 0.0979\n",
            "Epoch [8/10], Loss: 0.1229\n",
            "Epoch [9/10], Loss: 0.1042\n",
            "Epoch [10/10], Loss: 0.1168\n",
            "Accuracy: 0.9491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID49: 18"
      ],
      "metadata": {
        "id": "E3mvP8AcoGn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 18  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVN3pwj1GuHt",
        "outputId": "ff496e56-f45a-4c9a-84f2-a1d728c076b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.7950\n",
            "Epoch [2/10], Loss: 1.5099\n",
            "Epoch [3/10], Loss: 1.0113\n",
            "Epoch [4/10], Loss: 0.8657\n",
            "Epoch [5/10], Loss: 0.7223\n",
            "Epoch [6/10], Loss: 0.6450\n",
            "Epoch [7/10], Loss: 0.7077\n",
            "Epoch [8/10], Loss: 0.9311\n",
            "Epoch [9/10], Loss: 0.9843\n",
            "Epoch [10/10], Loss: 0.7082\n",
            "Accuracy: 0.7300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID50: 28"
      ],
      "metadata": {
        "id": "tOpwRURaoGn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 28  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm1Jjh0NGuyc",
        "outputId": "661cd1fd-8289-4eed-dee6-ad50107b8561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2769\n",
            "Epoch [2/10], Loss: 0.3782\n",
            "Epoch [3/10], Loss: 0.4242\n",
            "Epoch [4/10], Loss: 0.0945\n",
            "Epoch [5/10], Loss: 0.2345\n",
            "Epoch [6/10], Loss: 0.0353\n",
            "Epoch [7/10], Loss: 0.0238\n",
            "Epoch [8/10], Loss: 0.1468\n",
            "Epoch [9/10], Loss: 0.0908\n",
            "Epoch [10/10], Loss: 0.0219\n",
            "Accuracy: 0.9751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID51: 2074"
      ],
      "metadata": {
        "id": "IkzeGn8toGn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 2074  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHR5J-jmGvee",
        "outputId": "f3283f6c-62c6-4159-fa32-23b13485f7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2922\n",
            "Epoch [2/10], Loss: 0.3533\n",
            "Epoch [3/10], Loss: 0.4110\n",
            "Epoch [4/10], Loss: 0.3735\n",
            "Epoch [5/10], Loss: 0.4006\n",
            "Epoch [6/10], Loss: 0.3744\n",
            "Epoch [7/10], Loss: 0.1820\n",
            "Epoch [8/10], Loss: 0.2762\n",
            "Epoch [9/10], Loss: 0.2456\n",
            "Epoch [10/10], Loss: 0.1159\n",
            "Accuracy: 0.8919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID52: 29"
      ],
      "metadata": {
        "id": "a_AU0-T9oGn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 29  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GAI46sTGwJk",
        "outputId": "43e11d9e-6160-46a5-a857-296f64c4b9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5683\n",
            "Epoch [2/10], Loss: 0.6107\n",
            "Epoch [3/10], Loss: 0.4976\n",
            "Epoch [4/10], Loss: 0.3759\n",
            "Epoch [5/10], Loss: 0.2898\n",
            "Epoch [6/10], Loss: 0.6791\n",
            "Epoch [7/10], Loss: 0.4550\n",
            "Epoch [8/10], Loss: 0.2221\n",
            "Epoch [9/10], Loss: 0.1403\n",
            "Epoch [10/10], Loss: 0.2874\n",
            "Accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID53: 45"
      ],
      "metadata": {
        "id": "xPNpuR5foGn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 45  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-ZRjnhhGwyI",
        "outputId": "f595202d-5572-4c48-c2dd-c5d59cbc2d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.8273\n",
            "Epoch [2/10], Loss: 0.3736\n",
            "Epoch [3/10], Loss: 0.4439\n",
            "Epoch [4/10], Loss: 0.5267\n",
            "Epoch [5/10], Loss: 0.2413\n",
            "Epoch [6/10], Loss: 0.1165\n",
            "Epoch [7/10], Loss: 0.2268\n",
            "Epoch [8/10], Loss: 0.1592\n",
            "Epoch [9/10], Loss: 0.2348\n",
            "Epoch [10/10], Loss: 0.0930\n",
            "Accuracy: 0.9028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID54: 125922"
      ],
      "metadata": {
        "id": "LFYxW1gNoGn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 125922  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sblb5PRWGxUR",
        "outputId": "1849804c-4d58-40ad-e783-8df34b45dae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7484\n",
            "Epoch [2/10], Loss: 1.1360\n",
            "Epoch [3/10], Loss: 0.4406\n",
            "Epoch [4/10], Loss: 0.2962\n",
            "Epoch [5/10], Loss: 0.2426\n",
            "Epoch [6/10], Loss: 0.0788\n",
            "Epoch [7/10], Loss: 0.0731\n",
            "Epoch [8/10], Loss: 0.2455\n",
            "Epoch [9/10], Loss: 0.0644\n",
            "Epoch [10/10], Loss: 0.2277\n",
            "Accuracy: 0.9809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID55: 9960"
      ],
      "metadata": {
        "id": "o_N_ovi3oGn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9960 # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aHQuFMQGx90",
        "outputId": "a2fe6255-e08d-43f6-a47a-be0758363b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.1461\n",
            "Epoch [2/10], Loss: 0.6053\n",
            "Epoch [3/10], Loss: 1.0406\n",
            "Epoch [4/10], Loss: 0.6747\n",
            "Epoch [5/10], Loss: 0.5201\n",
            "Epoch [6/10], Loss: 0.9662\n",
            "Epoch [7/10], Loss: 0.6516\n",
            "Epoch [8/10], Loss: 0.2981\n",
            "Epoch [9/10], Loss: 0.7435\n",
            "Epoch [10/10], Loss: 0.2616\n",
            "Accuracy: 0.8233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID56: 9964"
      ],
      "metadata": {
        "id": "U260wLW9oGn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9964  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVRI9YnFGyi8",
        "outputId": "952f3880-f25b-428e-a2ee-0f522a2adffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.7351\n",
            "Epoch [2/10], Loss: 0.8990\n",
            "Epoch [3/10], Loss: 0.5939\n",
            "Epoch [4/10], Loss: 0.3084\n",
            "Epoch [5/10], Loss: 0.3122\n",
            "Epoch [6/10], Loss: 0.3787\n",
            "Epoch [7/10], Loss: 0.1843\n",
            "Epoch [8/10], Loss: 0.3635\n",
            "Epoch [9/10], Loss: 0.2732\n",
            "Epoch [10/10], Loss: 0.1935\n",
            "Accuracy: 0.9122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID57: 22"
      ],
      "metadata": {
        "id": "CdKbuDzRoGn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 22  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR9FmsHRGzJZ",
        "outputId": "bf8a3ced-fdc0-4991-8513-00f5ef26c7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.9153\n",
            "Epoch [2/10], Loss: 1.1205\n",
            "Epoch [3/10], Loss: 0.8081\n",
            "Epoch [4/10], Loss: 0.8976\n",
            "Epoch [5/10], Loss: 0.7231\n",
            "Epoch [6/10], Loss: 0.9096\n",
            "Epoch [7/10], Loss: 0.4405\n",
            "Epoch [8/10], Loss: 0.6210\n",
            "Epoch [9/10], Loss: 0.4733\n",
            "Epoch [10/10], Loss: 0.6931\n",
            "Accuracy: 0.8100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID58: 2079"
      ],
      "metadata": {
        "id": "DaH70689oGn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 2079  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slOBNznBGz1e",
        "outputId": "5207a1aa-7364-4be2-84f9-0356e3121873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.5049\n",
            "Epoch [2/10], Loss: 1.3901\n",
            "Epoch [3/10], Loss: 1.4932\n",
            "Epoch [4/10], Loss: 1.3889\n",
            "Epoch [5/10], Loss: 1.1976\n",
            "Epoch [6/10], Loss: 0.9845\n",
            "Epoch [7/10], Loss: 1.0315\n",
            "Epoch [8/10], Loss: 0.9093\n",
            "Epoch [9/10], Loss: 1.1634\n",
            "Epoch [10/10], Loss: 1.2318\n",
            "Accuracy: 0.5743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID59: 14969"
      ],
      "metadata": {
        "id": "QlBhfD3hoGn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 14969  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fmdnYEPG0Yu",
        "outputId": "7949df09-aa9e-4f70-b71a-4c46c1ddf710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.2482\n",
            "Epoch [2/10], Loss: 1.2744\n",
            "Epoch [3/10], Loss: 1.2302\n",
            "Epoch [4/10], Loss: 1.1223\n",
            "Epoch [5/10], Loss: 1.0441\n",
            "Epoch [6/10], Loss: 1.0920\n",
            "Epoch [7/10], Loss: 1.2609\n",
            "Epoch [8/10], Loss: 1.2281\n",
            "Epoch [9/10], Loss: 1.5800\n",
            "Epoch [10/10], Loss: 1.1081\n",
            "Accuracy: 0.5337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID60: 3560"
      ],
      "metadata": {
        "id": "f-9WK_C-oGn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3560  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_ks55GYG07c",
        "outputId": "fb3871cf-fca3-4a23-e32c-dd778dc7db80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.7975\n",
            "Epoch [2/10], Loss: 1.7885\n",
            "Epoch [3/10], Loss: 1.7737\n",
            "Epoch [4/10], Loss: 1.7932\n",
            "Epoch [5/10], Loss: 1.7920\n",
            "Epoch [6/10], Loss: 1.7832\n",
            "Epoch [7/10], Loss: 1.7472\n",
            "Epoch [8/10], Loss: 1.7870\n",
            "Epoch [9/10], Loss: 1.7094\n",
            "Epoch [10/10], Loss: 1.7713\n",
            "Accuracy: 0.2625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID61: 14952"
      ],
      "metadata": {
        "id": "ZnxmNnYMoGn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 14952  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PidRUacvG1gi",
        "outputId": "dab295b4-16cd-49fc-fb86-f174c1a7d5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3069\n",
            "Epoch [2/10], Loss: 0.1478\n",
            "Epoch [3/10], Loss: 0.2428\n",
            "Epoch [4/10], Loss: 0.1103\n",
            "Epoch [5/10], Loss: 0.0717\n",
            "Epoch [6/10], Loss: 0.1554\n",
            "Epoch [7/10], Loss: 0.3517\n",
            "Epoch [8/10], Loss: 0.2146\n",
            "Epoch [9/10], Loss: 0.0141\n",
            "Epoch [10/10], Loss: 0.0763\n",
            "Accuracy: 0.9457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID62: 125920"
      ],
      "metadata": {
        "id": "KPHnOsfNoGn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 125920  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8WABmitG2E3",
        "outputId": "6c19093b-f962-405a-f58e-f775c6a6a7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6561\n",
            "Epoch [2/10], Loss: 0.6528\n",
            "Epoch [3/10], Loss: 0.6298\n",
            "Epoch [4/10], Loss: 0.6924\n",
            "Epoch [5/10], Loss: 0.6469\n",
            "Epoch [6/10], Loss: 0.6853\n",
            "Epoch [7/10], Loss: 0.7803\n",
            "Epoch [8/10], Loss: 0.7304\n",
            "Epoch [9/10], Loss: 0.7056\n",
            "Epoch [10/10], Loss: 0.6113\n",
            "Accuracy: 0.5800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID63: 23"
      ],
      "metadata": {
        "id": "YcYsPjdWoGn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 23  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYss7LlqG2_d",
        "outputId": "d715229e-e69c-4bdf-e8a1-eaddc2503313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.9581\n",
            "Epoch [2/10], Loss: 0.9892\n",
            "Epoch [3/10], Loss: 0.9708\n",
            "Epoch [4/10], Loss: 0.9396\n",
            "Epoch [5/10], Loss: 1.0600\n",
            "Epoch [6/10], Loss: 1.0222\n",
            "Epoch [7/10], Loss: 0.8828\n",
            "Epoch [8/10], Loss: 1.0116\n",
            "Epoch [9/10], Loss: 0.8553\n",
            "Epoch [10/10], Loss: 0.8614\n",
            "Accuracy: 0.5661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID64: 3904"
      ],
      "metadata": {
        "id": "8-9JWbXOoGn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3904  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMK945GWG3hN",
        "outputId": "b8101859-268f-42a1-a493-30b6b5bf05fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2033\n",
            "Epoch [2/10], Loss: 0.2305\n",
            "Epoch [3/10], Loss: 0.1396\n",
            "Epoch [4/10], Loss: 0.4330\n",
            "Epoch [5/10], Loss: 0.5676\n",
            "Epoch [6/10], Loss: 0.1745\n",
            "Epoch [7/10], Loss: 0.2576\n",
            "Epoch [8/10], Loss: 0.3292\n",
            "Epoch [9/10], Loss: 0.9026\n",
            "Epoch [10/10], Loss: 0.1090\n",
            "Accuracy: 0.8112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID65: 3022"
      ],
      "metadata": {
        "id": "UNucm0W1oGn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3022  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYLMB8ebG4Dj",
        "outputId": "d7256e93-6840-42fc-9b93-93353cb2d530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.3624\n",
            "Epoch [2/10], Loss: 2.3273\n",
            "Epoch [3/10], Loss: 2.0606\n",
            "Epoch [4/10], Loss: 1.8122\n",
            "Epoch [5/10], Loss: 1.6415\n",
            "Epoch [6/10], Loss: 1.5427\n",
            "Epoch [7/10], Loss: 1.5094\n",
            "Epoch [8/10], Loss: 1.1993\n",
            "Epoch [9/10], Loss: 1.2095\n",
            "Epoch [10/10], Loss: 1.0983\n",
            "Accuracy: 0.6263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID66: 9985"
      ],
      "metadata": {
        "id": "pEFz9yFYoGn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9985  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL5D5yNTG4k8",
        "outputId": "6b53163c-e0e3-4ce6-b76d-9e3a00ced88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3718\n",
            "Epoch [2/10], Loss: 1.3923\n",
            "Epoch [3/10], Loss: 1.4622\n",
            "Epoch [4/10], Loss: 1.2558\n",
            "Epoch [5/10], Loss: 1.3364\n",
            "Epoch [6/10], Loss: 1.1151\n",
            "Epoch [7/10], Loss: 1.4405\n",
            "Epoch [8/10], Loss: 1.4199\n",
            "Epoch [9/10], Loss: 1.1781\n",
            "Epoch [10/10], Loss: 1.5860\n",
            "Accuracy: 0.4943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID67: 9910"
      ],
      "metadata": {
        "id": "JZqtyZwdoGn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9910  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80yYihXiG5Qe",
        "outputId": "64716f96-98c0-45c5-b70d-823c1fe3db9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4531\n",
            "Epoch [2/10], Loss: 0.3834\n",
            "Epoch [3/10], Loss: 0.5013\n",
            "Epoch [4/10], Loss: 0.3747\n",
            "Epoch [5/10], Loss: 0.5048\n",
            "Epoch [6/10], Loss: 0.4651\n",
            "Epoch [7/10], Loss: 0.6111\n",
            "Epoch [8/10], Loss: 0.3295\n",
            "Epoch [9/10], Loss: 0.2774\n",
            "Epoch [10/10], Loss: 0.2270\n",
            "Accuracy: 0.7843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID68: 14970"
      ],
      "metadata": {
        "id": "nDPFMOr6oGn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 14970  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKeMP42OG6Nn",
        "outputId": "592d58b8-fa68-4efb-faed-dc37f73a22e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.1258\n",
            "Epoch [2/10], Loss: 0.0953\n",
            "Epoch [3/10], Loss: 0.1113\n",
            "Epoch [4/10], Loss: 0.0469\n",
            "Epoch [5/10], Loss: 0.0619\n",
            "Epoch [6/10], Loss: 0.0536\n",
            "Epoch [7/10], Loss: 0.0102\n",
            "Epoch [8/10], Loss: 0.0102\n",
            "Epoch [9/10], Loss: 0.0825\n",
            "Epoch [10/10], Loss: 0.2505\n",
            "Accuracy: 0.9699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID69: 3021"
      ],
      "metadata": {
        "id": "7z17r_BvoGoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3021  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IFS9-iVG6yG",
        "outputId": "526e57ee-bdfb-42ed-a69d-f82042ddede2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: nan\n",
            "Epoch [2/10], Loss: nan\n",
            "Epoch [3/10], Loss: nan\n",
            "Epoch [4/10], Loss: nan\n",
            "Epoch [5/10], Loss: nan\n",
            "Epoch [6/10], Loss: nan\n",
            "Epoch [7/10], Loss: nan\n",
            "Epoch [8/10], Loss: nan\n",
            "Epoch [9/10], Loss: nan\n",
            "Epoch [10/10], Loss: nan\n",
            "Accuracy: 0.9497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID70: 3481"
      ],
      "metadata": {
        "id": "I4G11HdloGoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 3481  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ3GQM-8G7UW",
        "outputId": "b7bc8805-7537-4381-db6e-209e9bd7ea9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6701\n",
            "Epoch [2/10], Loss: 0.5188\n",
            "Epoch [3/10], Loss: 0.4814\n",
            "Epoch [4/10], Loss: 0.3009\n",
            "Epoch [5/10], Loss: 0.4821\n",
            "Epoch [6/10], Loss: 0.1372\n",
            "Epoch [7/10], Loss: 0.1265\n",
            "Epoch [8/10], Loss: 0.1033\n",
            "Epoch [9/10], Loss: 0.1955\n",
            "Epoch [10/10], Loss: 0.4077\n",
            "Accuracy: 0.9519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID71: 3573"
      ],
      "metadata": {
        "id": "GA1hnUnLoGoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 9946  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "id": "hLJq01izG8BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID72: 146824"
      ],
      "metadata": {
        "id": "JKAdqqvroGoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146824  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qn3SoLvG8kt",
        "outputId": "759ddc48-eab3-453b-d2f8-e5d53b1aeb41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.0078\n",
            "Epoch [2/10], Loss: 0.2178\n",
            "Epoch [3/10], Loss: 0.2980\n",
            "Epoch [4/10], Loss: 0.1871\n",
            "Epoch [5/10], Loss: 0.1209\n",
            "Epoch [6/10], Loss: 0.2171\n",
            "Epoch [7/10], Loss: 0.1167\n",
            "Epoch [8/10], Loss: 0.1056\n",
            "Epoch [9/10], Loss: 0.0560\n",
            "Epoch [10/10], Loss: 0.1168\n",
            "Accuracy: 0.9625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID73: 146820"
      ],
      "metadata": {
        "id": "eEhXmfjCoGoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146820  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaHt0xXlG9Ur",
        "outputId": "2e4369e0-fb0d-41ec-fb35-9ab517b1a4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.1358\n",
            "Epoch [2/10], Loss: 0.1010\n",
            "Epoch [3/10], Loss: 0.0912\n",
            "Epoch [4/10], Loss: 0.3462\n",
            "Epoch [5/10], Loss: 0.1265\n",
            "Epoch [6/10], Loss: 0.3241\n",
            "Epoch [7/10], Loss: 0.0289\n",
            "Epoch [8/10], Loss: 0.0818\n",
            "Epoch [9/10], Loss: 0.0911\n",
            "Epoch [10/10], Loss: 0.0320\n",
            "Accuracy: 0.9783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID74: 146822"
      ],
      "metadata": {
        "id": "AjDyUmVjoGoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146822  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5CF3BiWG92o",
        "outputId": "166deb4c-9af3-4ab0-bc16-1100eab97d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3144\n",
            "Epoch [2/10], Loss: 0.6705\n",
            "Epoch [3/10], Loss: 0.4327\n",
            "Epoch [4/10], Loss: 0.6459\n",
            "Epoch [5/10], Loss: 0.4002\n",
            "Epoch [6/10], Loss: 0.4064\n",
            "Epoch [7/10], Loss: 0.5871\n",
            "Epoch [8/10], Loss: 0.5033\n",
            "Epoch [9/10], Loss: 0.2537\n",
            "Epoch [10/10], Loss: 0.4168\n",
            "Accuracy: 0.8636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID75: 146195"
      ],
      "metadata": {
        "id": "2P19Cn-toGoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146195  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JeV60LfG-X4",
        "outputId": "837072b1-c6a8-4fa5-942b-2f89500cb5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7639\n",
            "Epoch [2/10], Loss: 0.7727\n",
            "Epoch [3/10], Loss: 0.8116\n",
            "Epoch [4/10], Loss: 1.0752\n",
            "Epoch [5/10], Loss: 0.9051\n",
            "Epoch [6/10], Loss: 0.5998\n",
            "Epoch [7/10], Loss: 0.7801\n",
            "Epoch [8/10], Loss: 0.7414\n",
            "Epoch [9/10], Loss: 0.8102\n",
            "Epoch [10/10], Loss: 0.8910\n",
            "Accuracy: 0.7438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID76: 146800"
      ],
      "metadata": {
        "id": "n1Fp7duGoGoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146800  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DbGjO9_G-58",
        "outputId": "9827eb9d-3757-4446-8aec-825a26ae4a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.8622\n",
            "Epoch [2/10], Loss: 1.4900\n",
            "Epoch [3/10], Loss: 1.1354\n",
            "Epoch [4/10], Loss: 0.9604\n",
            "Epoch [5/10], Loss: 0.6071\n",
            "Epoch [6/10], Loss: 0.6917\n",
            "Epoch [7/10], Loss: 0.4655\n",
            "Epoch [8/10], Loss: 0.5691\n",
            "Epoch [9/10], Loss: 0.3564\n",
            "Epoch [10/10], Loss: 0.3355\n",
            "Accuracy: 0.9537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID77: 146817"
      ],
      "metadata": {
        "id": "IM6dtNPNoGoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146817  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE6RjJ76G_aj",
        "outputId": "94528aea-70b5-48a2-c38b-024f3ecbd618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.4363\n",
            "Epoch [2/10], Loss: 1.5182\n",
            "Epoch [3/10], Loss: 0.9396\n",
            "Epoch [4/10], Loss: 0.9890\n",
            "Epoch [5/10], Loss: 1.1000\n",
            "Epoch [6/10], Loss: 0.7605\n",
            "Epoch [7/10], Loss: 0.4802\n",
            "Epoch [8/10], Loss: 0.8595\n",
            "Epoch [9/10], Loss: 1.2034\n",
            "Epoch [10/10], Loss: 0.4989\n",
            "Accuracy: 0.7224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID78: 146819"
      ],
      "metadata": {
        "id": "bjMEV3nRoGoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146819  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM8wd_uKG_9I",
        "outputId": "eb03b08d-f729-4d79-d80a-4eaafa05370f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5807\n",
            "Epoch [2/10], Loss: 0.1377\n",
            "Epoch [3/10], Loss: 0.5074\n",
            "Epoch [4/10], Loss: 0.2223\n",
            "Epoch [5/10], Loss: 0.0579\n",
            "Epoch [6/10], Loss: 0.0759\n",
            "Epoch [7/10], Loss: 0.1981\n",
            "Epoch [8/10], Loss: 0.0467\n",
            "Epoch [9/10], Loss: 0.1606\n",
            "Epoch [10/10], Loss: 0.0927\n",
            "Accuracy: 0.9074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID79: 146821"
      ],
      "metadata": {
        "id": "12bFV6wnoGoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 146821  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng12D2pJHAaU",
        "outputId": "9f141212-d6f9-4c6a-d119-def3e96d2454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.9359\n",
            "Epoch [2/10], Loss: 0.5941\n",
            "Epoch [3/10], Loss: 0.6852\n",
            "Epoch [4/10], Loss: 0.3264\n",
            "Epoch [5/10], Loss: 0.1279\n",
            "Epoch [6/10], Loss: 0.3459\n",
            "Epoch [7/10], Loss: 0.5113\n",
            "Epoch [8/10], Loss: 0.6479\n",
            "Epoch [9/10], Loss: 0.0682\n",
            "Epoch [10/10], Loss: 0.4343\n",
            "Accuracy: 0.8931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID80: 14954"
      ],
      "metadata": {
        "id": "eZ5B8Qe4oGoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 14954  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWZQIM4mHBBt",
        "outputId": "4f192a14-7baa-40d7-f5d0-4a8bd26387b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6958\n",
            "Epoch [2/10], Loss: 0.6255\n",
            "Epoch [3/10], Loss: 0.6655\n",
            "Epoch [4/10], Loss: 0.5182\n",
            "Epoch [5/10], Loss: 0.4262\n",
            "Epoch [6/10], Loss: 0.4989\n",
            "Epoch [7/10], Loss: 0.5278\n",
            "Epoch [8/10], Loss: 0.5725\n",
            "Epoch [9/10], Loss: 0.3996\n",
            "Epoch [10/10], Loss: 0.5399\n",
            "Accuracy: 0.7315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID81: 167141"
      ],
      "metadata": {
        "id": "jmJ4uaXaoGoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 167141  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nyCWQB4HBfN",
        "outputId": "78c8ce74-157c-4c93-96cb-f8c8a25fdb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3436\n",
            "Epoch [2/10], Loss: 0.3594\n",
            "Epoch [3/10], Loss: 0.1749\n",
            "Epoch [4/10], Loss: 0.4268\n",
            "Epoch [5/10], Loss: 0.5808\n",
            "Epoch [6/10], Loss: 0.3436\n",
            "Epoch [7/10], Loss: 0.3714\n",
            "Epoch [8/10], Loss: 0.1146\n",
            "Epoch [9/10], Loss: 0.2928\n",
            "Epoch [10/10], Loss: 0.2401\n",
            "Accuracy: 0.9260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID82: 167140"
      ],
      "metadata": {
        "id": "khe3EzqyoGoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 167140  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDWrpR-jHB60",
        "outputId": "33353124-4371-4580-8d03-8d69c3729470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5412\n",
            "Epoch [2/10], Loss: 0.1624\n",
            "Epoch [3/10], Loss: 0.1953\n",
            "Epoch [4/10], Loss: 0.1324\n",
            "Epoch [5/10], Loss: 0.0358\n",
            "Epoch [6/10], Loss: 0.2913\n",
            "Epoch [7/10], Loss: 0.2629\n",
            "Epoch [8/10], Loss: 0.0061\n",
            "Epoch [9/10], Loss: 0.0200\n",
            "Epoch [10/10], Loss: 0.0457\n",
            "Accuracy: 0.9498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID83: 167125"
      ],
      "metadata": {
        "id": "aP57C1GFoGoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "task_id = 167125  # Example task ID from OpenML\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "X, y = load_openml_data(task_id)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader, val_loader = get_data_loaders(X, y, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(np.unique(y))\n",
        "input_size = X.shape[1]  # Number of features\n",
        "model = ResNetTabular(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for features, labels in val_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Klv-0LYHCeN",
        "outputId": "604be379-0ae1-4723-cda7-76e4a12ab9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2874\n",
            "Epoch [2/10], Loss: 0.0253\n",
            "Epoch [3/10], Loss: 0.0332\n",
            "Epoch [4/10], Loss: 0.0362\n",
            "Epoch [5/10], Loss: 0.0667\n",
            "Epoch [6/10], Loss: 0.0069\n",
            "Epoch [7/10], Loss: 0.0061\n",
            "Epoch [8/10], Loss: 0.0323\n",
            "Epoch [9/10], Loss: 0.0654\n",
            "Epoch [10/10], Loss: 0.0032\n",
            "Accuracy: 0.9680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID84: 167124"
      ],
      "metadata": {
        "id": "fmrTR5y5oGoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID85: 167121"
      ],
      "metadata": {
        "id": "CxG5tfYeoGoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NODE"
      ],
      "metadata": {
        "id": "2cnRui4C3zCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import shutil\n",
        "import numpy as np\n",
        "import openml\n",
        "import pandas as pd  # Make sure to import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim  # Importing standard PyTorch optimizers\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Set a custom OpenML cache directory\n",
        "openml.config.cache_directory = os.path.expanduser(\"~/.openml_cache\")\n",
        "\n",
        "class NODE(nn.Module):\n",
        "    def __init__(self, params, num_features, num_classes):\n",
        "        super(NODE, self).__init__()\n",
        "\n",
        "        layer_dim = int(params[\"total_tree_count\"] / params[\"num_layers\"])\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(num_features, layer_dim),  # Example layer, modify as needed\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(layer_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            return self.forward(X)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "def get_random_parameters(seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"num_layers\": rs.choice([2, 4, 8]),\n",
        "        \"total_tree_count\": rs.choice([1024, 2048]),\n",
        "        \"tree_depth\": rs.choice([6, 8]),\n",
        "        \"tree_output_dim\": rs.choice([2, 3]),\n",
        "        \"learning_rate\": 1e-4,  # Smaller learning rate\n",
        "        \"epochs\": 100,  # Example value, set as needed\n",
        "        \"objective\": 'classification',  # Assuming classification\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def load_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = openml.datasets.get_dataset(task.dataset_id)\n",
        "\n",
        "    # Retrieve the target feature directly from the task\n",
        "    target_feature = task.target_name  # Changed from task.get_target() to task.target_name\n",
        "\n",
        "    # Retrieve the dataset's data\n",
        "    X, y, _, _ = dataset.get_data(target=target_feature)\n",
        "    return X, y\n",
        "\n",
        "def preprocess_data(X):\n",
        "    # Convert categorical columns to numeric using one-hot encoding\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        X = pd.get_dummies(X, drop_first=True)  # One-hot encode categorical variables\n",
        "\n",
        "    # Handle NaN values by filling them with the column mean or dropping rows\n",
        "    X = X.fillna(X.mean())  # Fill NaNs with column mean for numeric stability\n",
        "    return X\n",
        "\n",
        "def encode_target(y):\n",
        "    # Use LabelEncoder to convert categorical target variable to numeric\n",
        "    le = LabelEncoder()\n",
        "    return le.fit_transform(y)\n",
        "\n",
        "def train_model(X_train, y_train, X_val, y_val, params):\n",
        "    num_features = X_train.shape[1]\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    model = NODE(params, num_features, num_classes).to(device)\n",
        "\n",
        "    if params['objective'] == 'classification':\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "    elif params['objective'] == 'regression':\n",
        "        loss_function = nn.MSELoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])  # Use standard PyTorch Adam\n",
        "\n",
        "    # Check for NaN values in input data using Pandas method\n",
        "    if X_train.isna().any().any() or X_val.isna().any().any():\n",
        "        raise ValueError(\"Input data contains NaN values.\")\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "\n",
        "    # Initialize model weights\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(params['epochs']):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Convert DataFrame to FloatTensor\n",
        "        outputs = model(torch.FloatTensor(X_train).to(device))  # Convert to NumPy array\n",
        "        loss = loss_function(outputs, torch.LongTensor(y_train).to(device))  # Ensure y_train is LongTensor for classification\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if np.isnan(loss.item()):  # Check for NaN loss\n",
        "            print(\"NaN loss encountered, stopping training.\")\n",
        "            break\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(torch.FloatTensor(X_val).to(device))  # Convert to NumPy array\n",
        "        val_predictions = torch.argmax(val_outputs, dim=1)\n",
        "        accuracy = accuracy_score(y_val, val_predictions.cpu())\n",
        "\n",
        "    return model, accuracy\n",
        "\n",
        "# Define your parameters\n",
        "params = get_random_parameters(seed=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "dgl-RBCmHvhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID1: 14965"
      ],
      "metadata": {
        "id": "kjTY9DTMoPQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 14965  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY5czDRz4OGd",
        "outputId": "ea98a8ab-1826-47ba-fabe-e55b1c491362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID2: 9977"
      ],
      "metadata": {
        "id": "XfSa0mE5oPQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9977  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV-b8aZW4ozs",
        "outputId": "ab612daa-d5cd-4d84-998f-e1d2b6c264c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6xLqyXDoPQo"
      },
      "source": [
        "# Task ID3: 34539"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 34539  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izhw7l9B4p8k",
        "outputId": "d6f571e6-86a7-4e49-d0bb-4f68f16f4cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toqBb7tyoPQo"
      },
      "source": [
        "# Task ID4: 146606"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146606  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfXBlekk4qzA",
        "outputId": "b4506ede-997a-4407-a147-c3ce3998d198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPzdXgLqoPQo"
      },
      "source": [
        "# Task ID5: 7592\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 7592  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV-uIgI8B7Uj",
        "outputId": "07c53060-302d-4e47-a446-134945727912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e9JG8f7oPQo"
      },
      "source": [
        "# Task ID6: 146195\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146195  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SI6p_XkB8cM",
        "outputId": "5e976538-809a-4307-e496-f44fbe84827d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFMYjFtwoPQp"
      },
      "source": [
        "# Task ID7: 167119\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 167119  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lga-2oQB9xr",
        "outputId": "c841bdeb-8081-4b2a-cfee-de5ba0d769a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA1Jaso0oPQp"
      },
      "source": [
        "# Task ID8: 167120\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 167120  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCWKeMDVB-hI",
        "outputId": "f4750edb-84cc-4191-8292-5c516f899f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jD1reYnoPQp"
      },
      "source": [
        "# Task ID11: \t168331\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 168331  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlLy9tdtB_eV",
        "outputId": "31a20557-f03d-4c5d-9c81-1b4ca2fd667c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1bbbUmmoPQp"
      },
      "source": [
        "# Task ID12: \t168330\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 168330  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDJ39KNPCAkT",
        "outputId": "2c9ffe99-a9c9-4ed1-e474-20f1c99465dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPZ77y_VoPQp"
      },
      "source": [
        "# Task ID13: \t168335\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 168335  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv1bHx_fCBXB",
        "outputId": "f5046f02-29f6-48de-c99d-fa49540e65d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxWwy4pCoPQq"
      },
      "source": [
        "\n",
        "# Task ID16: \t146212\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146212  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywcof5KZCCEB",
        "outputId": "b0ffc7ed-a673-4e01-c9c9-f05577b93da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNm1ynGWoPQq"
      },
      "source": [
        "# Task ID19: \t168868\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 168868  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R18SKcqSCCvF",
        "outputId": "b7dc8777-3dc6-4c8c-c7ad-f7054d9e03a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID20: 31"
      ],
      "metadata": {
        "id": "n-FkLfTEoPQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 31  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF7NyBUrI_vZ",
        "outputId": "3cb2230e-41dd-42dc-a026-533a54b96368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID21: 10101"
      ],
      "metadata": {
        "id": "I3tlQXDqoPQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 10101  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd_tJqphJB4r",
        "outputId": "143e63cf-c068-44cc-fd45-b420324fed6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID22: \t3913"
      ],
      "metadata": {
        "id": "WEvlPo8SoPQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3913  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO3qXRh_JFuA",
        "outputId": "3a0d3c4e-3e58-4f61-c75d-02c95fe3bcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID23: 3"
      ],
      "metadata": {
        "id": "LgUvh-lKoPQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW_oFQKLJIus",
        "outputId": "973faeeb-a3f5-4263-b537-6759fc16ce0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID24: 3917"
      ],
      "metadata": {
        "id": "Rt_HmxUtoPQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3917  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB4bBKSpJLWr",
        "outputId": "517e87a5-15ed-42b4-e670-59815330ec62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID25: 9957"
      ],
      "metadata": {
        "id": "EhMsxT12oPQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9957 # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-h3ce9uJOXk",
        "outputId": "cb8a2dee-2ec6-4655-ea5f-3e864f98ec46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID26: 9946"
      ],
      "metadata": {
        "id": "c8rOSJg5oPQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9946  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYUgK9YmJSQS",
        "outputId": "1d43cbff-ed94-4d46-847a-a6ec0e7db4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID27: 3918"
      ],
      "metadata": {
        "id": "jP7p2_TWoPQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3918  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2a0e79-2575-4dcb-861b-5c9b21fd8863",
        "id": "c-1ZVdfvHq0i"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID28: 3903"
      ],
      "metadata": {
        "id": "onLJG5yFoPQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3903  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2961bfe7-d88f-4b7e-fb4b-9b01b4bc754f",
        "id": "lYqbyxzlHrRP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID29: 37"
      ],
      "metadata": {
        "id": "TYWNsAkCoPQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 37  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6b1982-ea70-4329-ec4e-04745d71ca5e",
        "id": "6Syx_pZgHrpq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID30: 9971"
      ],
      "metadata": {
        "id": "Ddu0xKRXoPQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9971  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2130878-0e79-44ae-fd77-224b25ae4396",
        "id": "QrMJINSmHsER"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID31: 9952"
      ],
      "metadata": {
        "id": "eVmmOgVxoPQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9952  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c73b195-f30a-40d1-b4f3-c16b63c0327f",
        "id": "5tL9dPMFHsnm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID32: 3902"
      ],
      "metadata": {
        "id": "F0KTU_tqoPQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3902  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0773cdf3-35a2-4416-c1d2-dff81873f004",
        "id": "qVSW9MiWHtEv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID33: 49"
      ],
      "metadata": {
        "id": "aHKSKDUnoPQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 49  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4d322c-8230-48ee-fdae-9934396a79a0",
        "id": "Mg2MKiZlHtjW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID34: 43"
      ],
      "metadata": {
        "id": "A7xo2Bx5oPQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 43  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3f8046-b5ce-410b-bd51-79559df2043a",
        "id": "ya1gLfopHuBx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID35: 9978"
      ],
      "metadata": {
        "id": "32wbJoDgoPQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9978  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05127f5e-450b-4c79-d797-43f0289c5fc0",
        "id": "NNzGVPkAHugI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID36: 10093"
      ],
      "metadata": {
        "id": "zqW1PkbloPQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 10093  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279fbe47-8e62-45c9-c857-2e2799dcc1a6",
        "id": "GhnLly_2Hvw6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID37: 219"
      ],
      "metadata": {
        "id": "O3IIFagNoPQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 219  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b92562-ace2-4758-aeee-61b0ff1b9762",
        "id": "_BqZauO0HwM2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID38: 9976"
      ],
      "metadata": {
        "id": "M3UXZtRfoPQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9976  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64846a1b-3e3b-4165-e010-0eb045a331ca",
        "id": "xWQZ9zFmHwtB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID39: 6"
      ],
      "metadata": {
        "id": "u3JHDNABoPQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 6  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a6af198-b707-4c78-ed97-742b95fcc8f1",
        "id": "moX0wTW2HxPu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID40: 53"
      ],
      "metadata": {
        "id": "6IRo6zvIoPQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 53  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0337a0-23f7-449c-ec66-d1d60ca92376",
        "id": "BJUm5UdQHxwY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID41: 11"
      ],
      "metadata": {
        "id": "yE3M7n7HoPQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 11  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ff316b-13a6-4fe5-ae7a-aff3ab360cfa",
        "id": "MkfDpuVhHyNO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID42: 15"
      ],
      "metadata": {
        "id": "HcfzyK28oPQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 15  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d10529-ca92-47f5-f6be-62da945e48ee",
        "id": "C5c4r1LIHytP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID43: 16"
      ],
      "metadata": {
        "id": "RkaQkENFoPQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 16  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbca2ee-6626-4278-c5ea-09d3a06d8df1",
        "id": "hNgxKXQyHzPv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID44: 14"
      ],
      "metadata": {
        "id": "2FKS9G7ToPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 14  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe41dc6-d29a-48e1-be31-d64adc0e7786",
        "id": "baN9sPZ_Hzvw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID45: 32"
      ],
      "metadata": {
        "id": "viwFLhULoPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 32  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c177075-babc-49db-c462-7c5a065d46af",
        "id": "V8oePA_GH0On"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID46: 3549"
      ],
      "metadata": {
        "id": "EQjB0Pc7oPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3549  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b979ca9-51a7-4d6f-ced5-ec0b1e476391",
        "id": "clcGS47sH1OW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID47: 12"
      ],
      "metadata": {
        "id": "DQudTqgdoPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 12  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c9ee228-756e-45b7-d03c-9e6417b91e56",
        "id": "1mHgZqnsH1wV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID48: 9981"
      ],
      "metadata": {
        "id": "zNlj-2T7oPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9981  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cd0351-2c74-4e1b-fd51-b7d50763d335",
        "id": "RrQHBaIaH2M-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID49: 18"
      ],
      "metadata": {
        "id": "I4e-25D0oPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 18  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c3f4e3-1742-4599-8c13-d1d71509c1c0",
        "id": "ExB04BleH2rH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID50: 28"
      ],
      "metadata": {
        "id": "HM4zjHphoPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 28  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b75ae79-b9b3-4625-8d86-91ce3d5965d5",
        "id": "5OPmVhIkH3JA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID51: 2074"
      ],
      "metadata": {
        "id": "TPwjEtlgoPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 2074  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915519a9-1a15-44c6-b3ca-e70f8a0ef3b0",
        "id": "bzDCLLOCH3oT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID52: 29"
      ],
      "metadata": {
        "id": "3rYq66sooPQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 29  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd24676-2ab6-45fa-98a8-a6bd510b7bfa",
        "id": "p88WHIEsH4Iz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID53: 45"
      ],
      "metadata": {
        "id": "NHPHrmlEoPQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 45  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8571312-d874-4ca0-d78e-6da7bb3d9fee",
        "id": "NT_Orm3vH4ok"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID54: 125922"
      ],
      "metadata": {
        "id": "9IjsB8azoPQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 125922  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8636e33d-41d1-472f-e9ac-6c16d4c43197",
        "id": "2wtk1NLPH5Ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID55: 9960"
      ],
      "metadata": {
        "id": "bLe5PW-HoPQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9960  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5da506b-327e-4c22-8c7b-9f1685b207a4",
        "id": "lSysXnx3H5kr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID56: 9964"
      ],
      "metadata": {
        "id": "53cS-QPfoPQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9964  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a92be1-6ff4-46d4-a08e-6b8dbba7156c",
        "id": "eZzUqc_9H6FS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID57: 22"
      ],
      "metadata": {
        "id": "_9tqO5FxoPQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 22  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c15c67-bf25-4ac0-9787-6b05a8b09ce2",
        "id": "_y8XDjtqH7Iu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID58: 2079"
      ],
      "metadata": {
        "id": "5yXiiJLZoPQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 2079  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0794bb9-8389-4a15-fa08-5936289b2a0f",
        "id": "QgsB6k0mH7nQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID59: 14969"
      ],
      "metadata": {
        "id": "fI4zbfO6oPQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 14969  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5bf006b-7181-4cae-ceb5-52c726a6d4ad",
        "id": "tWybzgj4H8Jw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID60: 3560"
      ],
      "metadata": {
        "id": "SlWMpVPEoPQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3560  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "990b9d2f-8dd1-414b-fa1c-b9c8119efdb0",
        "id": "j_rT5ICUH8mq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID61: 14952"
      ],
      "metadata": {
        "id": "Oo2RMr9QoPQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 14952  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8468a2ba-a0fb-44c5-81fd-dc7a0624cc7b",
        "id": "I7hHeP0rH9Gr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID62: 125920"
      ],
      "metadata": {
        "id": "IZQmdeJAoPQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 125920  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad66a0e-abb6-40ef-e339-e9627666d725",
        "id": "o8qHDM56H9lf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID63: 23"
      ],
      "metadata": {
        "id": "cNkPSfMfoPQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 23  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c06e68-2720-41c2-e3d7-6633e0610319",
        "id": "RF_Kp4-5H9_1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID64: 3904"
      ],
      "metadata": {
        "id": "qDfLzPzeoPQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3904  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ddf022-12f0-4307-a715-0f261f9a82cd",
        "id": "TuauG3ERH-cC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID65: 3022"
      ],
      "metadata": {
        "id": "dU7vm4BqoPQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3022  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c73f89-072d-451a-c690-708414a43e05",
        "id": "2gTfon4JH-4N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID66: 9985"
      ],
      "metadata": {
        "id": "SPJdWG4yoPQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9985  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fff8bfd-a36c-46eb-9544-43d67a5c5cbb",
        "id": "8-M6ZN-TH_Qe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID67: 9910"
      ],
      "metadata": {
        "id": "P8kMlFw1oPQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 9910  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0474d79b-033e-41d3-f713-456d06daffd8",
        "id": "3_kcCUfvH_qm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID68: 14970"
      ],
      "metadata": {
        "id": "IpgLuWigoPQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 14970  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20835496-7c5a-4776-8ba6-d3a094c160a3",
        "id": "oLGtRvzdIAd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID69: 3021"
      ],
      "metadata": {
        "id": "gUOEvSJJoPQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3021  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbecc5c7-30a7-42c6-e6eb-312e3e29e3d9",
        "id": "tFnSGryKIA4C"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID70: 3481"
      ],
      "metadata": {
        "id": "kGRT8x38oPQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3481  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1edbd1-3b75-412f-b5af-e40e625c73bf",
        "id": "47OspdLIIBW5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID71: 3573"
      ],
      "metadata": {
        "id": "EuML6eigoPQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 3573  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "m3C1EjLNIB7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID72: 146824"
      ],
      "metadata": {
        "id": "sUCT6aTpoPQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146824  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82c91bfa-0d1b-4f62-9041-e8ae17399895",
        "id": "TVlHkAi5ICVc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID73: 146820"
      ],
      "metadata": {
        "id": "Wzj9YodgoPQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146820 # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e86746-6e26-4cab-9be2-d3bab75f6095",
        "id": "PL3FRvEyIC0Z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID74: 146822"
      ],
      "metadata": {
        "id": "tqauucFFoPQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146822  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42a190c-7972-4913-d152-2655f0924b50",
        "id": "6Lfq8yuOIDPL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID75: 146195"
      ],
      "metadata": {
        "id": "-vhZIB7ioPQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146195  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8657b092-1b12-4011-822d-ec6783044bf6",
        "id": "6xltrPqbIDrO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID76: 146800"
      ],
      "metadata": {
        "id": "75HjKRMooPQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146800  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453ba230-f2b6-4c3a-fee6-042a8eb93a47",
        "id": "fWEklxIbIEmd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID77: 146817"
      ],
      "metadata": {
        "id": "S6AE6WGjoPQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146817  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce2b042-8ced-4d6a-fd64-eaa2c97f47fb",
        "id": "myy0QY3_IFCm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID78: 146819"
      ],
      "metadata": {
        "id": "SkK9YKyeoPQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146819  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029c24dd-b02c-40de-b93d-48094e8ade40",
        "id": "YJM3ekx9IFfZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID79: 146821"
      ],
      "metadata": {
        "id": "f02fONaIoPQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 146821  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e37e739-9f08-4b22-8ee7-a80a0861e3d1",
        "id": "72KBoLqkIF7R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID80: 14954"
      ],
      "metadata": {
        "id": "MiNxc6HQoPQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 14954  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31cba0b2-a077-4f75-bb3f-9cdffa24bfa7",
        "id": "vxZtgLxdIGX3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID81: 167141"
      ],
      "metadata": {
        "id": "rbnMu0aeoPQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 167141  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bdbbbbc-2001-4c30-f72f-a529f07ded39",
        "id": "PaepKK6QIGxf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID82: 167140"
      ],
      "metadata": {
        "id": "TX3XmuXIoPQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 167140  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5d850b-abd2-4520-fb02-da07aeee6467",
        "id": "bQABncWsIHO_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID83: 167125"
      ],
      "metadata": {
        "id": "YJlj1nGpoPQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from OpenML\n",
        "task_id = 167125  # Replace with your OpenML task ID\n",
        "X, y = load_data(task_id)\n",
        "\n",
        "# Preprocess the data\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Encode the target variable\n",
        "y = encode_target(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef1c99a-77f7-43f2-8007-522309316d26",
        "id": "_2UUW8eLIHrv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID84: 167124"
      ],
      "metadata": {
        "id": "6EUhemtFoPQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID85: 167121"
      ],
      "metadata": {
        "id": "R6IyVenhoPQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TabNet\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fckn0NGUwCc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import openml\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class TabNetModel:\n",
        "    def __init__(self, params, args):\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.params[\"n_a\"] = self.params[\"n_d\"]\n",
        "        self.params[\"cat_idxs\"] = args.cat_idx\n",
        "        self.params[\"cat_dims\"] = args.cat_dims\n",
        "        self.params[\"device_name\"] = device  # Set device for TabNet\n",
        "\n",
        "        if args.objective == \"regression\":\n",
        "            self.model = TabNetRegressor(**self.params)\n",
        "            self.metric = \"rmse\"\n",
        "        elif args.objective == \"classification\":\n",
        "            self.model = TabNetClassifier(**self.params)\n",
        "            self.metric = \"logloss\"\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if self.args.objective == \"regression\":\n",
        "            y, y_val = y.reshape(-1, 1), y_val.reshape(-1, 1)\n",
        "\n",
        "        drop_last = X.shape[0] % self.args.batch_size == 1\n",
        "        self.model.fit(\n",
        "            X,\n",
        "            y,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_name=[\"eval\"],\n",
        "            eval_metric=[self.metric],\n",
        "            max_epochs=self.args.epochs,\n",
        "            patience=self.args.early_stopping_rounds,\n",
        "            batch_size=self.args.batch_size,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        history = self.model.history\n",
        "        return history[\"loss\"], history[\"eval_\" + self.metric]\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        if self.args.objective == \"regression\":\n",
        "            return self.model.predict(X)\n",
        "        else:\n",
        "            return self.model.predict_proba(X)\n",
        "\n",
        "def define_trial_parameters(cls, trial, args):\n",
        "    params = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
        "        \"cat_emb_dim\": trial.suggest_int(\"cat_emb_dim\", 1, 3),\n",
        "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
        "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
        "        \"momentum\": trial.suggest_float(\"momentum\", 0.001, 0.4, log=True),\n",
        "        \"mask_type\": trial.suggest_categorical(\"mask_type\", [\"sparsemax\", \"entmax\"]),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def get_random_parameters(cls, seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"n_d\": rs.randint(8, 65),\n",
        "        \"n_steps\": rs.randint(3, 11),\n",
        "        \"gamma\": 1.0 + rs.rand(),\n",
        "        \"cat_emb_dim\": rs.randint(1, 4),\n",
        "        \"n_independent\": rs.randint(1, 6),\n",
        "        \"n_shared\": rs.randint(1, 6),\n",
        "        \"momentum\": 0.4 * np.power(10, rs.uniform(-3, -1)),\n",
        "        \"mask_type\": rs.choice([\"sparsemax\", \"entmax\"]),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "# Load data from OpenML, remove classes, and encode categorical features\n",
        "def load_openml_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Remove class labels for training\n",
        "    X = X.drop(columns=[dataset.default_target_attribute], errors='ignore')\n",
        "\n",
        "    # Identify categorical columns and encode them\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    cat_dims = []  # Track max values to set embedding dimensions correctly\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        X[col], uniques = pd.factorize(X[col])  # Ensure all categorical columns are integer-encoded\n",
        "        cat_dims.append(len(uniques))  # Track unique counts as embedding dimensions\n",
        "\n",
        "    # Replace any remaining non-numeric entries with NaN and drop them or fill appropriately\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    X.fillna(-1, inplace=True)  # Or replace with another valid category ID if needed\n",
        "\n",
        "    # Encode target variable\n",
        "    y, _ = pd.factorize(y)  # Converts labels to numeric format\n",
        "\n",
        "    return X, y, cat_dims\n",
        "\n",
        "# Example configuration and arguments\n",
        "class Args:\n",
        "    objective = \"classification\"  # Or \"regression\" based on task\n",
        "    batch_size = 32\n",
        "    epochs = 10\n",
        "    early_stopping_rounds = 3\n",
        "\n",
        "    def __init__(self, X, cat_dims):\n",
        "        # Get categorical column indices based on factorized object columns\n",
        "        self.cat_idx = [X.columns.get_loc(col) for col in X.select_dtypes(include=['object']).columns]\n",
        "        self.cat_dims = cat_dims  # Use dimensions from factorization for embedding sizes"
      ],
      "metadata": {
        "id": "o9OPbm5l4YWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID1: 14965"
      ],
      "metadata": {
        "id": "ZMmCdceVodhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=14965)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm914lNcwNqI",
        "outputId": "37adb38a-167f-465d-c91e-61640c38fbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.3465  | eval_logloss: 0.89848 |  0:01:38s\n",
            "epoch 1  | loss: 0.28701 | eval_logloss: 0.42651 |  0:03:17s\n",
            "epoch 2  | loss: 0.28254 | eval_logloss: 0.29386 |  0:04:55s\n",
            "epoch 3  | loss: 0.28017 | eval_logloss: 0.30636 |  0:06:34s\n",
            "epoch 4  | loss: 0.28278 | eval_logloss: 0.31493 |  0:08:13s\n",
            "epoch 5  | loss: 0.28208 | eval_logloss: 0.39377 |  0:09:54s\n",
            "\n",
            "Early stopping occurred at epoch 5 with best_epoch = 2 and best_eval_logloss = 0.29386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID2: 9977"
      ],
      "metadata": {
        "id": "0HgcUOUcodhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9977)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvXEcROf2y9b",
        "outputId": "fbd2f326-1576-469d-f66b-d3dffeba6074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.3963  | eval_logloss: 1.45557 |  0:01:18s\n",
            "epoch 1  | loss: 0.17881 | eval_logloss: 1.02047 |  0:02:49s\n",
            "epoch 2  | loss: 0.1493  | eval_logloss: 1.04512 |  0:04:06s\n",
            "epoch 3  | loss: 0.1381  | eval_logloss: 0.92759 |  0:05:23s\n",
            "epoch 4  | loss: 0.12986 | eval_logloss: 0.53905 |  0:06:40s\n",
            "epoch 5  | loss: 0.12656 | eval_logloss: 0.87485 |  0:07:58s\n",
            "epoch 6  | loss: 0.12038 | eval_logloss: 0.46256 |  0:09:20s\n",
            "epoch 7  | loss: 0.11966 | eval_logloss: 0.25007 |  0:10:38s\n",
            "epoch 8  | loss: 0.11339 | eval_logloss: 1.29685 |  0:11:56s\n",
            "epoch 9  | loss: 0.11283 | eval_logloss: 0.92155 |  0:13:13s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_eval_logloss = 0.25007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hst0eOlHodhY"
      },
      "source": [
        "# Task ID3: 34539"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=34539)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6WmEuUkAuX2",
        "outputId": "b9068209-058f-4b4c-f0eb-4a97e666bb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.26029 | eval_logloss: 0.68683 |  0:01:12s\n",
            "epoch 1  | loss: 0.22643 | eval_logloss: 0.25801 |  0:02:26s\n",
            "epoch 2  | loss: 0.22433 | eval_logloss: 0.24328 |  0:03:38s\n",
            "epoch 3  | loss: 0.22227 | eval_logloss: 0.21793 |  0:04:51s\n",
            "epoch 4  | loss: 0.22344 | eval_logloss: 0.21722 |  0:06:04s\n",
            "epoch 5  | loss: 0.2225  | eval_logloss: 0.23177 |  0:07:18s\n",
            "epoch 6  | loss: 0.2221  | eval_logloss: 0.2354  |  0:08:30s\n",
            "epoch 7  | loss: 0.22352 | eval_logloss: 0.28985 |  0:09:42s\n",
            "\n",
            "Early stopping occurred at epoch 7 with best_epoch = 4 and best_eval_logloss = 0.21722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyLaOqfModhY"
      },
      "source": [
        "# Task ID4: 146606"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=146606)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgk2_ty0AvWB",
        "outputId": "2497f68f-d2d5-40c5-e1b7-7262f25f9026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.71186 | eval_logloss: 0.7747  |  0:03:32s\n",
            "epoch 1  | loss: 0.67884 | eval_logloss: 0.708   |  0:07:51s\n",
            "epoch 2  | loss: 0.67799 | eval_logloss: 0.69193 |  0:11:32s\n",
            "epoch 3  | loss: 0.67771 | eval_logloss: 0.71725 |  0:15:06s\n",
            "epoch 4  | loss: 0.67498 | eval_logloss: 0.67667 |  0:18:39s\n",
            "epoch 5  | loss: 0.67353 | eval_logloss: 0.7062  |  0:22:16s\n",
            "epoch 6  | loss: 0.66881 | eval_logloss: 0.72076 |  0:25:55s\n",
            "epoch 7  | loss: 0.66763 | eval_logloss: 0.67615 |  0:29:32s\n",
            "epoch 8  | loss: 0.66067 | eval_logloss: 0.65701 |  0:33:05s\n",
            "epoch 9  | loss: 0.66065 | eval_logloss: 0.65639 |  0:36:40s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 0.65639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2E6F1htodhY"
      },
      "source": [
        "# Task ID5: 7592\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=7592)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrScHN5TEeg9",
        "outputId": "634c7c96-9836-4b14-e4f0-28973a0f339e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.51344 | eval_logloss: 0.8332  |  0:01:46s\n",
            "epoch 1  | loss: 0.41641 | eval_logloss: 0.57792 |  0:03:35s\n",
            "epoch 2  | loss: 0.40977 | eval_logloss: 0.51473 |  0:05:23s\n",
            "epoch 3  | loss: 0.41555 | eval_logloss: 0.91587 |  0:07:14s\n",
            "epoch 4  | loss: 0.41224 | eval_logloss: 1.37989 |  0:09:06s\n",
            "epoch 5  | loss: 0.41125 | eval_logloss: 0.81665 |  0:10:55s\n",
            "\n",
            "Early stopping occurred at epoch 5 with best_epoch = 2 and best_eval_logloss = 0.51473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlMzwRyHodhZ"
      },
      "source": [
        "# Task ID6: 146195\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=146195)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95cdlxC1Efer",
        "outputId": "2fcadbf9-24dd-4482-a449-e40875785a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.896   | eval_logloss: 2.01412 |  0:02:33s\n",
            "epoch 1  | loss: 0.8415  | eval_logloss: 0.84725 |  0:05:04s\n",
            "epoch 2  | loss: 0.83912 | eval_logloss: 6.05471 |  0:07:37s\n",
            "epoch 3  | loss: 0.83894 | eval_logloss: 0.84646 |  0:10:08s\n",
            "epoch 4  | loss: 0.83915 | eval_logloss: 1.60132 |  0:12:42s\n",
            "epoch 5  | loss: 0.83728 | eval_logloss: 1.00331 |  0:15:13s\n",
            "epoch 6  | loss: 0.83939 | eval_logloss: 0.88437 |  0:17:49s\n",
            "\n",
            "Early stopping occurred at epoch 6 with best_epoch = 3 and best_eval_logloss = 0.84646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUbsANIhodhZ"
      },
      "source": [
        "# Task ID7: 167119\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=167119)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GePJIvPhEgnU",
        "outputId": "ed152448-130a-411d-ec9c-dbb4a410ab2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.77246 | eval_logloss: 1.57999 |  0:01:38s\n",
            "epoch 1  | loss: 0.64025 | eval_logloss: 1.18152 |  0:03:27s\n",
            "epoch 2  | loss: 0.60071 | eval_logloss: 0.81802 |  0:05:05s\n",
            "epoch 3  | loss: 0.55812 | eval_logloss: 0.7299  |  0:06:43s\n",
            "epoch 4  | loss: 0.54603 | eval_logloss: 1.11812 |  0:08:21s\n",
            "epoch 5  | loss: 0.53139 | eval_logloss: 0.81699 |  0:09:59s\n",
            "epoch 6  | loss: 0.52363 | eval_logloss: 0.4834  |  0:11:38s\n",
            "epoch 7  | loss: 0.49867 | eval_logloss: 0.4671  |  0:13:18s\n",
            "epoch 8  | loss: 0.48945 | eval_logloss: 0.47089 |  0:14:57s\n",
            "epoch 9  | loss: 0.48366 | eval_logloss: 0.46573 |  0:16:37s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 0.46573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGKabQBeodhZ"
      },
      "source": [
        "# Task ID8: 167120\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=167120)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFdBWtT_EhVQ",
        "outputId": "93a30d8c-caa5-473c-fcce-699ace32403d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.71209 | eval_logloss: 0.69413 |  0:03:37s\n",
            "epoch 1  | loss: 0.69438 | eval_logloss: 0.69306 |  0:07:14s\n",
            "epoch 2  | loss: 0.69331 | eval_logloss: 0.69404 |  0:10:53s\n",
            "epoch 3  | loss: 0.69327 | eval_logloss: 0.69329 |  0:14:28s\n",
            "epoch 4  | loss: 0.69302 | eval_logloss: 0.69295 |  0:18:02s\n",
            "epoch 5  | loss: 0.69318 | eval_logloss: 0.71189 |  0:21:38s\n",
            "epoch 6  | loss: 0.69329 | eval_logloss: 0.69334 |  0:25:12s\n",
            "epoch 7  | loss: 0.69327 | eval_logloss: 0.70784 |  0:28:50s\n",
            "\n",
            "Early stopping occurred at epoch 7 with best_epoch = 4 and best_eval_logloss = 0.69295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbiDczB5odha"
      },
      "source": [
        "# Task ID11: \t168331\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=168331)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZx48yvXEiDg",
        "outputId": "9161c460-a9c7-4585-8875-2a7cf334ee0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.6942  | eval_logloss: 3.93489 |  0:02:32s\n",
            "epoch 1  | loss: 1.38955 | eval_logloss: 2.20657 |  0:04:52s\n",
            "epoch 2  | loss: 1.31262 | eval_logloss: 3.18054 |  0:07:12s\n",
            "epoch 3  | loss: 1.2644  | eval_logloss: 1.50697 |  0:09:35s\n",
            "epoch 4  | loss: 1.27018 | eval_logloss: 1.88719 |  0:11:54s\n",
            "epoch 5  | loss: 1.21719 | eval_logloss: 1.46682 |  0:14:13s\n",
            "epoch 6  | loss: 1.1868  | eval_logloss: 1.45254 |  0:16:30s\n",
            "epoch 7  | loss: 1.17425 | eval_logloss: 1.47863 |  0:18:50s\n",
            "epoch 8  | loss: 1.16154 | eval_logloss: 1.43581 |  0:21:39s\n",
            "epoch 9  | loss: 1.14858 | eval_logloss: 1.45584 |  0:24:00s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_eval_logloss = 1.43581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBiOeDVIodhb"
      },
      "source": [
        "# Task ID12: \t168330\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=168330)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lLxy2eFEi_f",
        "outputId": "9c203e8f-d06d-4ec2-9792-28c4cb0dea84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.99482 | eval_logloss: 1.24022 |  0:03:05s\n",
            "epoch 1  | loss: 0.84874 | eval_logloss: 1.46023 |  0:06:13s\n",
            "epoch 2  | loss: 0.80891 | eval_logloss: 0.87615 |  0:09:22s\n",
            "epoch 3  | loss: 0.77896 | eval_logloss: 0.85331 |  0:12:31s\n",
            "epoch 4  | loss: 0.76773 | eval_logloss: 1.10859 |  0:15:41s\n",
            "epoch 5  | loss: 0.75482 | eval_logloss: 0.79825 |  0:18:58s\n",
            "epoch 6  | loss: 0.75109 | eval_logloss: 0.96151 |  0:22:10s\n",
            "epoch 7  | loss: 0.74612 | eval_logloss: 0.72795 |  0:25:19s\n",
            "epoch 8  | loss: 0.73993 | eval_logloss: 0.77799 |  0:28:33s\n",
            "epoch 9  | loss: 0.73496 | eval_logloss: 0.82595 |  0:31:47s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_eval_logloss = 0.72795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKGrMb4Sodhc"
      },
      "source": [
        "# Task ID13: \t168335\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=168335)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwqB0ZExEj4T",
        "outputId": "0ccb1904-5544-4586-d008-c93bd6bbf6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.31082 | eval_logloss: 0.3181  |  0:04:08s\n",
            "epoch 1  | loss: 0.2099  | eval_logloss: 0.41314 |  0:08:21s\n",
            "epoch 2  | loss: 0.19644 | eval_logloss: 1.19324 |  0:12:31s\n",
            "epoch 3  | loss: 0.20571 | eval_logloss: 0.36119 |  0:16:54s\n",
            "\n",
            "Early stopping occurred at epoch 3 with best_epoch = 0 and best_eval_logloss = 0.3181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqUgoFAoodhc"
      },
      "source": [
        "\n",
        "# Task ID16: \t146212\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=146212)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi_1uOCCEkhk",
        "outputId": "d2ee3271-afe6-4832-e670-d53b6d810007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.16955 | eval_logloss: 2.33135 |  0:02:24s\n",
            "epoch 1  | loss: 0.05515 | eval_logloss: 0.78685 |  0:04:48s\n",
            "epoch 2  | loss: 0.0538  | eval_logloss: 0.26912 |  0:07:12s\n",
            "epoch 3  | loss: 0.04524 | eval_logloss: 0.13735 |  0:09:38s\n",
            "epoch 4  | loss: 0.02799 | eval_logloss: 0.10543 |  0:12:03s\n",
            "epoch 5  | loss: 0.02399 | eval_logloss: 0.20159 |  0:14:27s\n",
            "epoch 6  | loss: 0.03056 | eval_logloss: 0.25784 |  0:16:49s\n",
            "epoch 7  | loss: 0.0208  | eval_logloss: 0.11338 |  0:19:08s\n",
            "\n",
            "Early stopping occurred at epoch 7 with best_epoch = 4 and best_eval_logloss = 0.10543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rJULYDXodhc"
      },
      "source": [
        "# Task ID19: \t168868\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=168868)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxZC2ccbElNq",
        "outputId": "28e4ef42-2540-470b-ed9a-72aa2644e6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.10852 | eval_logloss: 0.09223 |  0:02:37s\n",
            "epoch 1  | loss: 0.0514  | eval_logloss: 0.13615 |  0:05:18s\n",
            "epoch 2  | loss: 0.04712 | eval_logloss: 0.06024 |  0:08:08s\n",
            "epoch 3  | loss: 0.0495  | eval_logloss: 0.08095 |  0:11:00s\n",
            "epoch 4  | loss: 0.05274 | eval_logloss: 0.05522 |  0:13:52s\n",
            "epoch 5  | loss: 0.05017 | eval_logloss: 0.06555 |  0:16:45s\n",
            "epoch 6  | loss: 0.04313 | eval_logloss: 0.06066 |  0:19:26s\n",
            "epoch 7  | loss: 0.04361 | eval_logloss: 0.05076 |  0:22:05s\n",
            "epoch 8  | loss: 0.04403 | eval_logloss: 0.59572 |  0:24:43s\n",
            "epoch 9  | loss: 0.04406 | eval_logloss: 0.05768 |  0:27:22s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_eval_logloss = 0.05076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID20: 31"
      ],
      "metadata": {
        "id": "Zy-8fqBTodhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=31)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkiXMAp9_y4g",
        "outputId": "4ef8e488-bf4a-4243-8a33-52459325215e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.34689 | eval_logloss: 9.24658 |  0:00:05s\n",
            "epoch 1  | loss: 1.18631 | eval_logloss: 4.94214 |  0:00:08s\n",
            "epoch 2  | loss: 0.83039 | eval_logloss: 6.37695 |  0:00:10s\n",
            "epoch 3  | loss: 0.66874 | eval_logloss: 11.23938|  0:00:12s\n",
            "epoch 4  | loss: 0.63382 | eval_logloss: 9.88428 |  0:00:14s\n",
            "\n",
            "Early stopping occurred at epoch 4 with best_epoch = 1 and best_eval_logloss = 4.94214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID21: 10101"
      ],
      "metadata": {
        "id": "oA_eZQoUodhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=10101)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10zJDtr0_1cT",
        "outputId": "b5a87b66-2a74-4201-b83e-352215601e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.3636  | eval_logloss: 4.46387 |  0:00:01s\n",
            "epoch 1  | loss: 0.9386  | eval_logloss: 6.05811 |  0:00:03s\n",
            "epoch 2  | loss: 0.60565 | eval_logloss: 4.67643 |  0:00:04s\n",
            "epoch 3  | loss: 0.55462 | eval_logloss: 5.46162 |  0:00:06s\n",
            "\n",
            "Early stopping occurred at epoch 3 with best_epoch = 0 and best_eval_logloss = 4.46387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID22: \t3913"
      ],
      "metadata": {
        "id": "zlZ8fDblodhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3913)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qK_6j2V_4fk",
        "outputId": "d18ecf2d-e055-495c-aa7e-1a4bc75a608d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.92756 | eval_logloss: 8.50322 |  0:00:01s\n",
            "epoch 1  | loss: 0.93712 | eval_logloss: 8.19894 |  0:00:02s\n",
            "epoch 2  | loss: 1.14424 | eval_logloss: 10.02093|  0:00:03s\n",
            "epoch 3  | loss: 0.73191 | eval_logloss: 11.84291|  0:00:04s\n",
            "epoch 4  | loss: 1.05149 | eval_logloss: 6.07329 |  0:00:05s\n",
            "epoch 5  | loss: 0.67271 | eval_logloss: 9.71726 |  0:00:06s\n",
            "epoch 6  | loss: 0.6837  | eval_logloss: 5.92657 |  0:00:07s\n",
            "epoch 7  | loss: 0.53478 | eval_logloss: 12.29841|  0:00:09s\n",
            "epoch 8  | loss: 0.44473 | eval_logloss: 10.40043|  0:00:10s\n",
            "epoch 9  | loss: 0.38226 | eval_logloss: 3.59684 |  0:00:11s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 3.59684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID23: 3"
      ],
      "metadata": {
        "id": "zKC8GsYuodhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5eK7pPh_8Vq",
        "outputId": "6b3d5f57-b3ac-481e-bd5a-fcbea03327a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.69673 | eval_logloss: 7.47299 |  0:00:28s\n",
            "epoch 1  | loss: 0.69673 | eval_logloss: 7.47299 |  0:00:48s\n",
            "epoch 2  | loss: 0.69673 | eval_logloss: 7.47299 |  0:01:05s\n",
            "epoch 3  | loss: 0.69673 | eval_logloss: 7.47299 |  0:01:22s\n",
            "\n",
            "Early stopping occurred at epoch 3 with best_epoch = 0 and best_eval_logloss = 7.47299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID24: 3917"
      ],
      "metadata": {
        "id": "ZTTiNgawodhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3917)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEPml9Bi_-YI",
        "outputId": "c622e4c5-a3f4-42b9-c3b5-33390552329e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.07655 | eval_logloss: 9.02898 |  0:00:05s\n",
            "epoch 1  | loss: 0.77614 | eval_logloss: 2.7201  |  0:00:10s\n",
            "epoch 2  | loss: 0.49718 | eval_logloss: 3.00605 |  0:00:14s\n",
            "epoch 3  | loss: 0.42752 | eval_logloss: 3.83157 |  0:00:20s\n",
            "epoch 4  | loss: 0.37198 | eval_logloss: 2.68079 |  0:00:24s\n",
            "epoch 5  | loss: 0.37401 | eval_logloss: 1.75649 |  0:00:29s\n",
            "epoch 6  | loss: 0.3726  | eval_logloss: 1.96631 |  0:00:35s\n",
            "epoch 7  | loss: 0.37644 | eval_logloss: 2.03858 |  0:00:39s\n",
            "epoch 8  | loss: 0.3669  | eval_logloss: 1.95649 |  0:00:43s\n",
            "\n",
            "Early stopping occurred at epoch 8 with best_epoch = 5 and best_eval_logloss = 1.75649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID25: 9957"
      ],
      "metadata": {
        "id": "h0BrjH6eodhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9957)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQlOXLk3ABJZ",
        "outputId": "15a158c7-8062-41bf-f216-f36aa3084e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.42965 | eval_logloss: 5.81784 |  0:00:02s\n",
            "epoch 1  | loss: 1.3272  | eval_logloss: 5.41684 |  0:00:05s\n",
            "epoch 2  | loss: 1.25499 | eval_logloss: 6.04451 |  0:00:08s\n",
            "epoch 3  | loss: 0.69689 | eval_logloss: 5.74228 |  0:00:10s\n",
            "epoch 4  | loss: 0.61616 | eval_logloss: 5.31628 |  0:00:12s\n",
            "epoch 5  | loss: 0.56039 | eval_logloss: 6.27827 |  0:00:15s\n",
            "epoch 6  | loss: 0.55947 | eval_logloss: 5.6358  |  0:00:17s\n",
            "epoch 7  | loss: 0.53881 | eval_logloss: 5.55946 |  0:00:20s\n",
            "\n",
            "Early stopping occurred at epoch 7 with best_epoch = 4 and best_eval_logloss = 5.31628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID26: 9946"
      ],
      "metadata": {
        "id": "JzJkM5wTodhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9946)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNk3_Rl5ACLm",
        "outputId": "a0003b84-0f4f-4725-de73-51e84bfcec22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.9977  | eval_logloss: 6.53087 |  0:00:02s\n",
            "epoch 1  | loss: 0.92732 | eval_logloss: 8.67054 |  0:00:05s\n",
            "epoch 2  | loss: 0.86623 | eval_logloss: 9.2298  |  0:00:06s\n",
            "epoch 3  | loss: 0.46641 | eval_logloss: 9.50949 |  0:00:07s\n",
            "\n",
            "Early stopping occurred at epoch 3 with best_epoch = 0 and best_eval_logloss = 6.53087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID27: 3918"
      ],
      "metadata": {
        "id": "LHEnt8rYodhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3918)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17873962-f62d-4490-ae3e-d46735eb3cff",
        "id": "xukgMKW-HHO3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.04891 | eval_logloss: 4.18315 |  0:00:07s\n",
            "epoch 1  | loss: 1.19057 | eval_logloss: 8.25844 |  0:00:12s\n",
            "epoch 2  | loss: 0.46498 | eval_logloss: 4.0222  |  0:00:18s\n",
            "epoch 3  | loss: 0.29515 | eval_logloss: 12.44818|  0:00:22s\n",
            "epoch 4  | loss: 0.25252 | eval_logloss: 2.88959 |  0:00:24s\n",
            "epoch 5  | loss: 0.31126 | eval_logloss: 9.04523 |  0:00:27s\n",
            "epoch 6  | loss: 0.26592 | eval_logloss: 1.36444 |  0:00:29s\n",
            "epoch 7  | loss: 0.27141 | eval_logloss: 1.44562 |  0:00:33s\n",
            "epoch 8  | loss: 0.25278 | eval_logloss: 3.28764 |  0:00:36s\n",
            "epoch 9  | loss: 0.24221 | eval_logloss: 3.89003 |  0:00:39s\n",
            "\n",
            "Early stopping occurred at epoch 9 with best_epoch = 6 and best_eval_logloss = 1.36444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID28: 3903"
      ],
      "metadata": {
        "id": "2Cq_pcgIodhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3903)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d714eeea-5c8a-466c-c63d-f5ddf634ba76",
        "id": "wyQaw_vkHHwI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.71368 | eval_logloss: 2.03737 |  0:00:04s\n",
            "epoch 1  | loss: 0.52013 | eval_logloss: 1.83363 |  0:00:08s\n",
            "epoch 2  | loss: 0.41295 | eval_logloss: 1.73176 |  0:00:11s\n",
            "epoch 3  | loss: 0.37941 | eval_logloss: 3.11145 |  0:00:15s\n",
            "epoch 4  | loss: 0.34096 | eval_logloss: 1.68083 |  0:00:20s\n",
            "epoch 5  | loss: 0.33321 | eval_logloss: 1.62989 |  0:00:24s\n",
            "epoch 6  | loss: 0.29487 | eval_logloss: 1.76104 |  0:00:28s\n",
            "epoch 7  | loss: 0.30232 | eval_logloss: 1.59179 |  0:00:32s\n",
            "epoch 8  | loss: 0.29718 | eval_logloss: 8.36491 |  0:00:36s\n",
            "epoch 9  | loss: 0.2992  | eval_logloss: 1.24993 |  0:00:40s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 1.24993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID29: 37"
      ],
      "metadata": {
        "id": "hW57h3EFodhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=37)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac78c3f5-d19a-400a-f0bf-43ef37756676",
        "id": "BDt38f3MHIXM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.17244 | eval_logloss: 6.83245 |  0:00:01s\n",
            "epoch 1  | loss: 0.95167 | eval_logloss: 7.76415 |  0:00:03s\n",
            "epoch 2  | loss: 0.61299 | eval_logloss: 7.86767 |  0:00:05s\n",
            "epoch 3  | loss: 0.61923 | eval_logloss: 10.14676|  0:00:07s\n",
            "\n",
            "Early stopping occurred at epoch 3 with best_epoch = 0 and best_eval_logloss = 6.83245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID30: 9971"
      ],
      "metadata": {
        "id": "OgaNGRqkodhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9971)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99d1d12-628a-4462-a7fc-19282932a47d",
        "id": "lmx8NjNQHLHE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.31847 | eval_logloss: 6.13169 |  0:00:01s\n",
            "epoch 1  | loss: 1.14787 | eval_logloss: 5.58665 |  0:00:02s\n",
            "epoch 2  | loss: 0.93563 | eval_logloss: 7.78689 |  0:00:04s\n",
            "epoch 3  | loss: 0.76167 | eval_logloss: 6.13169 |  0:00:05s\n",
            "epoch 4  | loss: 0.63493 | eval_logloss: 6.67673 |  0:00:06s\n",
            "\n",
            "Early stopping occurred at epoch 4 with best_epoch = 1 and best_eval_logloss = 5.58665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID31: 9952"
      ],
      "metadata": {
        "id": "s2Hgzu-nodhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9952)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef314e2b-df7f-421c-8143-2cf432405031",
        "id": "HRIXBMnyHLqD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.69858 | eval_logloss: 4.06578 |  0:00:13s\n",
            "epoch 1  | loss: 0.50012 | eval_logloss: 5.10602 |  0:00:27s\n",
            "epoch 2  | loss: 0.46375 | eval_logloss: 3.98746 |  0:00:40s\n",
            "epoch 3  | loss: 0.45129 | eval_logloss: 2.53219 |  0:00:54s\n",
            "epoch 4  | loss: 0.43466 | eval_logloss: 1.06911 |  0:01:07s\n",
            "epoch 5  | loss: 0.43551 | eval_logloss: 0.88662 |  0:01:21s\n",
            "epoch 6  | loss: 0.42693 | eval_logloss: 0.6498  |  0:01:35s\n",
            "epoch 7  | loss: 0.42591 | eval_logloss: 0.49176 |  0:01:48s\n",
            "epoch 8  | loss: 0.43433 | eval_logloss: 0.62525 |  0:02:02s\n",
            "epoch 9  | loss: 0.424   | eval_logloss: 0.56668 |  0:02:15s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_eval_logloss = 0.49176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID32: 3902"
      ],
      "metadata": {
        "id": "jwG63f85odhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3902)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a11553-4f12-475a-fb80-4ad0c251645f",
        "id": "XwA8vX6-HMKD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.84881 | eval_logloss: 5.9511  |  0:00:04s\n",
            "epoch 1  | loss: 0.73736 | eval_logloss: 4.53803 |  0:00:08s\n",
            "epoch 2  | loss: 0.46176 | eval_logloss: 2.29308 |  0:00:11s\n",
            "epoch 3  | loss: 0.35295 | eval_logloss: 2.62067 |  0:00:16s\n",
            "epoch 4  | loss: 0.32215 | eval_logloss: 2.29308 |  0:00:20s\n",
            "epoch 5  | loss: 0.29483 | eval_logloss: 2.29308 |  0:00:24s\n",
            "\n",
            "Early stopping occurred at epoch 5 with best_epoch = 2 and best_eval_logloss = 2.29308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID33: 49"
      ],
      "metadata": {
        "id": "Sl22lXi2odhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import openml\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class TabNetModel:\n",
        "    def __init__(self, params, args):\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.params[\"n_a\"] = self.params[\"n_d\"]\n",
        "        self.params[\"cat_idxs\"] = args.cat_idx\n",
        "        self.params[\"cat_dims\"] = args.cat_dims\n",
        "        self.params[\"device_name\"] = device  # Set device for TabNet\n",
        "\n",
        "        if args.objective == \"regression\":\n",
        "            self.model = TabNetRegressor(**self.params)\n",
        "            self.metric = \"rmse\"\n",
        "        elif args.objective == \"classification\":\n",
        "            self.model = TabNetClassifier(**self.params)\n",
        "            self.metric = \"logloss\"\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if self.args.objective == \"regression\":\n",
        "            y, y_val = y.reshape(-1, 1), y_val.reshape(-1, 1)\n",
        "\n",
        "        drop_last = X.shape[0] % self.args.batch_size == 1\n",
        "        self.model.fit(\n",
        "            X,\n",
        "            y,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_name=[\"eval\"],\n",
        "            eval_metric=[self.metric],\n",
        "            max_epochs=self.args.epochs,\n",
        "            patience=self.args.early_stopping_rounds,\n",
        "            batch_size=self.args.batch_size,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        history = self.model.history\n",
        "        return history[\"loss\"], history[\"eval_\" + self.metric]\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        if self.args.objective == \"regression\":\n",
        "            return self.model.predict(X)\n",
        "        else:\n",
        "            return self.model.predict_proba(X)\n",
        "\n",
        "def define_trial_parameters(cls, trial, args):\n",
        "    params = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
        "        \"cat_emb_dim\": min(trial.suggest_int(\"cat_emb_dim\", 1, 3), 2),\n",
        "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
        "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
        "        \"momentum\": trial.suggest_float(\"momentum\", 0.001, 0.4, log=True),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax to avoid sparsemax issues\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def get_random_parameters(cls, seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"n_d\": rs.randint(8, 65),\n",
        "        \"n_steps\": rs.randint(3, 11),\n",
        "        \"gamma\": 1.0 + rs.rand(),\n",
        "        \"cat_emb_dim\": min(rs.randint(1, 4), 2),\n",
        "        \"n_independent\": rs.randint(1, 6),\n",
        "        \"n_shared\": rs.randint(1, 6),\n",
        "        \"momentum\": 0.4 * np.power(10, rs.uniform(-3, -1)),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax as alternative to sparsemax\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def load_openml_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Drop class labels for training\n",
        "    X = X.drop(columns=[dataset.default_target_attribute], errors='ignore')\n",
        "\n",
        "    # Identify and encode categorical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    cat_idx = []\n",
        "    cat_dims = []\n",
        "    for col in categorical_cols:\n",
        "        X[col], uniques = pd.factorize(X[col])\n",
        "        X[col] = X[col].replace(-1, len(uniques))  # Replace NaN factorized values with a valid new index\n",
        "        cat_idx.append(X.columns.get_loc(col))\n",
        "        cat_dims.append(len(uniques) + 1)  # Account for NaN as an additional category\n",
        "\n",
        "    # Scale numeric features and handle missing values\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    X.fillna(-1, inplace=True)  # Replace NaN with -1 for non-categorical columns\n",
        "    scaler = StandardScaler()\n",
        "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    y, _ = pd.factorize(y)\n",
        "\n",
        "    return X, y, cat_dims, cat_idx\n",
        "\n",
        "\n",
        "# Example configuration and arguments\n",
        "class Args:\n",
        "    objective = \"classification\"  # Change to \"regression\" if needed\n",
        "    batch_size = 4  # Larger batch size for numerical stability\n",
        "    epochs = 10\n",
        "    early_stopping_rounds = 3\n",
        "\n",
        "    def __init__(self, cat_idx, cat_dims):\n",
        "        self.cat_idx = cat_idx\n",
        "        self.cat_dims = cat_dims\n",
        "\n",
        "# Load data and set up arguments\n",
        "X, y, cat_dims, cat_idx = load_openml_data(task_id=49)  # Example OpenML task ID\n",
        "args = Args(cat_idx, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameters and initialize the model\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "params[\"device_name\"] = device\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbd6e25-0a87-43b7-e617-635c881d5466",
        "id": "wTRVXjpJHMvO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.69534 | eval_logloss: 0.69315 |  0:00:29s\n",
            "epoch 1  | loss: 0.69534 | eval_logloss: 0.69315 |  0:01:05s\n",
            "epoch 2  | loss: 0.69534 | eval_logloss: 0.69315 |  0:01:24s\n",
            "epoch 3  | loss: 0.69534 | eval_logloss: 0.69315 |  0:01:43s\n",
            "\n",
            "Early stopping occurred at epoch 3 with best_epoch = 0 and best_eval_logloss = 0.69315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:761: RuntimeWarning: invalid value encountered in divide\n",
            "  feature_importances_ = sum_explain / np.sum(sum_explain)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID34: 43"
      ],
      "metadata": {
        "id": "UlATkzp1odhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=43)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff03bd1-2294-4c60-a30c-d27c97884450",
        "id": "-7eHlo6gHNOc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.89573 | eval_logloss: 3.83524 |  0:00:51s\n",
            "epoch 1  | loss: 0.48869 | eval_logloss: 1.58229 |  0:01:35s\n",
            "epoch 2  | loss: 0.3461  | eval_logloss: 1.10953 |  0:02:19s\n",
            "epoch 3  | loss: 0.30336 | eval_logloss: 1.20901 |  0:03:03s\n",
            "epoch 4  | loss: 0.31972 | eval_logloss: 0.9515  |  0:03:46s\n",
            "epoch 5  | loss: 0.29299 | eval_logloss: 0.55348 |  0:04:31s\n",
            "epoch 6  | loss: 0.2637  | eval_logloss: 0.53266 |  0:05:14s\n",
            "epoch 7  | loss: 0.26325 | eval_logloss: 0.61412 |  0:05:57s\n",
            "epoch 8  | loss: 0.25624 | eval_logloss: 1.01671 |  0:06:39s\n",
            "epoch 9  | loss: 0.23249 | eval_logloss: 0.65936 |  0:07:22s\n",
            "\n",
            "Early stopping occurred at epoch 9 with best_epoch = 6 and best_eval_logloss = 0.53266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID35: 9978"
      ],
      "metadata": {
        "id": "dLg81fv8odhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9978)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a035f39e-3877-4cf4-ad70-d75c03f94403",
        "id": "n-LE2Z0XHNw3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.67168 | eval_logloss: 1.03767 |  0:00:23s\n",
            "epoch 1  | loss: 0.25097 | eval_logloss: 1.03496 |  0:00:47s\n",
            "epoch 2  | loss: 0.25197 | eval_logloss: 0.74863 |  0:01:13s\n",
            "epoch 3  | loss: 0.22699 | eval_logloss: 1.88177 |  0:01:37s\n",
            "epoch 4  | loss: 0.23745 | eval_logloss: 0.97193 |  0:02:01s\n",
            "epoch 5  | loss: 0.23823 | eval_logloss: 0.96236 |  0:02:25s\n",
            "\n",
            "Early stopping occurred at epoch 5 with best_epoch = 2 and best_eval_logloss = 0.74863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID36: 10093"
      ],
      "metadata": {
        "id": "iS1bfJk_odhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=10093)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0491a08-be1b-429f-e1a9-d6af7733b119",
        "id": "ggHLSMwqHOPG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.97136 | eval_logloss: 4.34792 |  0:00:13s\n",
            "epoch 1  | loss: 0.43185 | eval_logloss: 3.86977 |  0:00:25s\n",
            "epoch 2  | loss: 0.36574 | eval_logloss: 4.06272 |  0:00:38s\n",
            "epoch 3  | loss: 0.32282 | eval_logloss: 3.34202 |  0:00:52s\n",
            "epoch 4  | loss: 0.29568 | eval_logloss: 2.94016 |  0:01:05s\n",
            "epoch 5  | loss: 0.26915 | eval_logloss: 5.25189 |  0:01:18s\n",
            "epoch 6  | loss: 0.29462 | eval_logloss: 2.68352 |  0:01:31s\n",
            "epoch 7  | loss: 0.23989 | eval_logloss: 1.66723 |  0:01:44s\n",
            "epoch 8  | loss: 0.23303 | eval_logloss: 1.09113 |  0:01:57s\n",
            "epoch 9  | loss: 0.17572 | eval_logloss: 0.81969 |  0:02:10s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 0.81969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID37: 219"
      ],
      "metadata": {
        "id": "85jHP5QHodhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=219)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a8c2a2-4f98-4a4f-ae9d-84b6878cc57c",
        "id": "ULsuR_OkHOsI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.55812 | eval_logloss: 1.24437 |  0:02:12s\n",
            "epoch 1  | loss: 0.50988 | eval_logloss: 1.71687 |  0:04:11s\n",
            "epoch 2  | loss: 0.51261 | eval_logloss: 0.64444 |  0:06:08s\n",
            "epoch 3  | loss: 0.50404 | eval_logloss: 0.51315 |  0:08:07s\n",
            "epoch 4  | loss: 0.50175 | eval_logloss: 1.15444 |  0:10:07s\n",
            "epoch 5  | loss: 0.49845 | eval_logloss: 0.54822 |  0:12:06s\n",
            "epoch 6  | loss: 0.50422 | eval_logloss: 1.59942 |  0:14:03s\n",
            "\n",
            "Early stopping occurred at epoch 6 with best_epoch = 3 and best_eval_logloss = 0.51315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID38: 9976"
      ],
      "metadata": {
        "id": "Rjsqsr2modhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9976)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af558a5-968f-4676-9f3e-867e6a3cb38d",
        "id": "5VZ8IoHpHPVy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.35332 | eval_logloss: 7.78724 |  0:00:09s\n",
            "epoch 1  | loss: 0.99687 | eval_logloss: 8.04319 |  0:00:19s\n",
            "epoch 2  | loss: 0.797   | eval_logloss: 8.06317 |  0:00:27s\n",
            "epoch 3  | loss: 0.72739 | eval_logloss: 7.84856 |  0:00:36s\n",
            "\n",
            "Early stopping occurred at epoch 3 with best_epoch = 0 and best_eval_logloss = 7.78724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID39: 6"
      ],
      "metadata": {
        "id": "tJSrrgOcodhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=6)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476346a0-3a8d-424a-b02e-745dce62c1ec",
        "id": "PCrg_rdoHP5P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.52516 | eval_logloss: 13.27656|  0:00:51s\n",
            "epoch 1  | loss: 1.43021 | eval_logloss: 10.25438|  0:01:43s\n",
            "epoch 2  | loss: 1.20465 | eval_logloss: 5.88078 |  0:02:35s\n",
            "epoch 3  | loss: 1.10063 | eval_logloss: 4.37772 |  0:03:26s\n",
            "epoch 4  | loss: 1.02321 | eval_logloss: 5.01252 |  0:04:19s\n",
            "epoch 5  | loss: 0.97172 | eval_logloss: 2.61749 |  0:05:11s\n",
            "epoch 6  | loss: 0.94548 | eval_logloss: 2.53675 |  0:06:04s\n",
            "epoch 7  | loss: 0.89046 | eval_logloss: 2.87962 |  0:06:56s\n",
            "epoch 8  | loss: 0.89461 | eval_logloss: 3.54118 |  0:07:48s\n",
            "epoch 9  | loss: 0.85949 | eval_logloss: 2.82158 |  0:08:41s\n",
            "\n",
            "Early stopping occurred at epoch 9 with best_epoch = 6 and best_eval_logloss = 2.53675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID40: 53"
      ],
      "metadata": {
        "id": "GQMFF05jodhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=53)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36d3b5b-62b7-4e62-981e-2106b14bb586",
        "id": "pGEIyMKXHQWz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.35915 | eval_logloss: 11.53478|  0:00:02s\n",
            "epoch 1  | loss: 2.02485 | eval_logloss: 11.53478|  0:00:04s\n",
            "epoch 2  | loss: 2.17864 | eval_logloss: 12.19124|  0:00:07s\n",
            "epoch 3  | loss: 1.42329 | eval_logloss: 12.00368|  0:00:10s\n",
            "\n",
            "Early stopping occurred at epoch 3 with best_epoch = 0 and best_eval_logloss = 11.53478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID41: 11"
      ],
      "metadata": {
        "id": "oEviFGX8odhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=11)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09f917c-2048-4b18-e418-859952ba89e5",
        "id": "UNJBy55EHQ7N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.14849 | eval_logloss: 9.31035 |  0:00:01s\n",
            "epoch 1  | loss: 1.11661 | eval_logloss: 7.77988 |  0:00:04s\n",
            "epoch 2  | loss: 0.83861 | eval_logloss: 6.76513 |  0:00:06s\n",
            "epoch 3  | loss: 0.6374  | eval_logloss: 4.97402 |  0:00:07s\n",
            "epoch 4  | loss: 0.54761 | eval_logloss: 4.71895 |  0:00:09s\n",
            "epoch 5  | loss: 0.46219 | eval_logloss: 9.18281 |  0:00:11s\n",
            "epoch 6  | loss: 0.47785 | eval_logloss: 8.29004 |  0:00:12s\n",
            "epoch 7  | loss: 0.47396 | eval_logloss: 7.26973 |  0:00:14s\n",
            "\n",
            "Early stopping occurred at epoch 7 with best_epoch = 4 and best_eval_logloss = 4.71895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID42: 15"
      ],
      "metadata": {
        "id": "FhyXNAaVodhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=15)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93b040d-dbe0-46cd-df96-af9c36e58d49",
        "id": "HoyBIjVyHRcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.51653 | eval_logloss: 10.93192|  0:00:02s\n",
            "epoch 1  | loss: 0.41641 | eval_logloss: 8.65444 |  0:00:04s\n",
            "epoch 2  | loss: 0.32008 | eval_logloss: 10.81805|  0:00:06s\n",
            "epoch 3  | loss: 0.27131 | eval_logloss: 7.74344 |  0:00:08s\n",
            "epoch 4  | loss: 0.21542 | eval_logloss: 9.22381 |  0:00:10s\n",
            "epoch 5  | loss: 0.13686 | eval_logloss: 10.5903 |  0:00:12s\n",
            "epoch 6  | loss: 0.23555 | eval_logloss: 8.14845 |  0:00:13s\n",
            "\n",
            "Early stopping occurred at epoch 6 with best_epoch = 3 and best_eval_logloss = 7.74344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID43: 16"
      ],
      "metadata": {
        "id": "Dmo5WtaVodhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=16)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5f0eac-6f6c-47cc-d7af-5d9a19be43ef",
        "id": "O_c0pqXXHR6Y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.68524 | eval_logloss: 13.91963|  0:00:06s\n",
            "epoch 1  | loss: 2.71258 | eval_logloss: 13.45409|  0:00:11s\n",
            "epoch 2  | loss: 2.07086 | eval_logloss: 13.51117|  0:00:16s\n",
            "epoch 3  | loss: 1.58317 | eval_logloss: 13.21697|  0:00:23s\n",
            "epoch 4  | loss: 1.37061 | eval_logloss: 13.72293|  0:00:28s\n",
            "epoch 5  | loss: 1.15076 | eval_logloss: 12.34968|  0:00:34s\n",
            "epoch 6  | loss: 0.99943 | eval_logloss: 10.6877 |  0:00:39s\n",
            "epoch 7  | loss: 0.92738 | eval_logloss: 9.86958 |  0:00:45s\n",
            "epoch 8  | loss: 0.86861 | eval_logloss: 10.69403|  0:00:51s\n",
            "epoch 9  | loss: 0.8236  | eval_logloss: 9.75459 |  0:00:56s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 9.75459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID44: 14"
      ],
      "metadata": {
        "id": "XzXa17h8odhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=14)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a949e8-6079-4594-fd81-ce876e55c8cb",
        "id": "HmWdn6vrHSmv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.63054 | eval_logloss: 14.54598|  0:00:05s\n",
            "epoch 1  | loss: 2.79112 | eval_logloss: 13.8594 |  0:00:10s\n",
            "epoch 2  | loss: 2.19693 | eval_logloss: 14.39966|  0:00:15s\n",
            "epoch 3  | loss: 1.57906 | eval_logloss: 14.03264|  0:00:21s\n",
            "epoch 4  | loss: 1.32264 | eval_logloss: 13.58695|  0:00:26s\n",
            "epoch 5  | loss: 1.18568 | eval_logloss: 13.43258|  0:00:32s\n",
            "epoch 6  | loss: 0.96883 | eval_logloss: 9.66555 |  0:00:37s\n",
            "epoch 7  | loss: 0.92166 | eval_logloss: 9.94002 |  0:00:43s\n",
            "epoch 8  | loss: 0.92694 | eval_logloss: 10.09771|  0:00:48s\n",
            "epoch 9  | loss: 0.8296  | eval_logloss: 8.79039 |  0:00:54s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 8.79039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID45: 32"
      ],
      "metadata": {
        "id": "T40eq3bVodhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=32)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cb404e-d20f-4fbf-af6f-1bcc26b7c045",
        "id": "LvFtGfECHTEb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.62185 | eval_logloss: 12.88854|  0:00:28s\n",
            "epoch 1  | loss: 0.65008 | eval_logloss: 11.81195|  0:00:57s\n",
            "epoch 2  | loss: 0.46543 | eval_logloss: 7.9902  |  0:01:25s\n",
            "epoch 3  | loss: 0.36477 | eval_logloss: 4.86628 |  0:01:55s\n",
            "epoch 4  | loss: 0.28835 | eval_logloss: 4.84269 |  0:02:24s\n",
            "epoch 5  | loss: 0.28201 | eval_logloss: 3.60895 |  0:02:53s\n",
            "epoch 6  | loss: 0.23222 | eval_logloss: 3.13824 |  0:03:22s\n",
            "epoch 7  | loss: 0.21231 | eval_logloss: 2.6245  |  0:03:51s\n",
            "epoch 8  | loss: 0.18237 | eval_logloss: 1.74174 |  0:04:19s\n",
            "epoch 9  | loss: 0.16706 | eval_logloss: 2.71475 |  0:04:47s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_eval_logloss = 1.74174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID46: 3549"
      ],
      "metadata": {
        "id": "AADdV7arodhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3549)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed849c2a-f23d-4e2b-eef9-6b26c9edeb24",
        "id": "Wsxg4bbdHTgD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.10331 | eval_logloss: 11.98037|  0:00:02s\n",
            "epoch 1  | loss: 2.22585 | eval_logloss: 9.8107  |  0:00:04s\n",
            "epoch 2  | loss: 1.68331 | eval_logloss: 8.96228 |  0:00:06s\n",
            "epoch 3  | loss: 1.15805 | eval_logloss: 10.3767 |  0:00:08s\n",
            "epoch 4  | loss: 1.02533 | eval_logloss: 9.91327 |  0:00:11s\n",
            "epoch 5  | loss: 0.8891  | eval_logloss: 9.2447  |  0:00:13s\n",
            "\n",
            "Early stopping occurred at epoch 5 with best_epoch = 2 and best_eval_logloss = 8.96228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID47: 12"
      ],
      "metadata": {
        "id": "kM26BzuJodhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=12)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d85d16b-7864-4b77-f6d9-9516b9a1273b",
        "id": "g0HdCmIdHUR7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.78448 | eval_logloss: 13.0329 |  0:00:16s\n",
            "epoch 1  | loss: 2.02954 | eval_logloss: 13.48574|  0:00:28s\n",
            "epoch 2  | loss: 1.35196 | eval_logloss: 11.50661|  0:00:37s\n",
            "epoch 3  | loss: 1.13319 | eval_logloss: 11.61802|  0:00:48s\n",
            "epoch 4  | loss: 1.09804 | eval_logloss: 11.83884|  0:00:58s\n",
            "epoch 5  | loss: 1.00785 | eval_logloss: 8.07718 |  0:01:09s\n",
            "epoch 6  | loss: 0.90997 | eval_logloss: 7.63674 |  0:01:19s\n",
            "epoch 7  | loss: 0.83216 | eval_logloss: 8.50177 |  0:01:29s\n",
            "epoch 8  | loss: 0.78182 | eval_logloss: 5.64768 |  0:01:40s\n",
            "epoch 9  | loss: 0.68053 | eval_logloss: 4.96465 |  0:01:50s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 4.96465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID48: 9981"
      ],
      "metadata": {
        "id": "EHBUGeLaodhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9981)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4fe21ef-15e9-49d8-c279-4928b63f35f9",
        "id": "24oNOTIEHU7_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 4.18252 | eval_logloss: 14.61385|  0:00:13s\n",
            "epoch 1  | loss: 3.98198 | eval_logloss: 13.72817|  0:00:24s\n",
            "epoch 2  | loss: 2.66812 | eval_logloss: 14.61564|  0:00:30s\n",
            "epoch 3  | loss: 2.2368  | eval_logloss: 13.7897 |  0:00:38s\n",
            "epoch 4  | loss: 1.97772 | eval_logloss: 10.91763|  0:00:45s\n",
            "epoch 5  | loss: 1.70386 | eval_logloss: 8.8964  |  0:00:53s\n",
            "epoch 6  | loss: 1.5747  | eval_logloss: 9.02813 |  0:01:00s\n",
            "epoch 7  | loss: 1.36443 | eval_logloss: 6.73812 |  0:01:08s\n",
            "epoch 8  | loss: 1.17264 | eval_logloss: 8.93185 |  0:01:15s\n",
            "epoch 9  | loss: 1.16976 | eval_logloss: 4.89783 |  0:01:22s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 4.89783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID49: 18"
      ],
      "metadata": {
        "id": "kobLJq9hodhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=18)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d52dda7-93c3-4373-8b1d-4664a0a765c4",
        "id": "7U2QdPWpHVdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.21757 | eval_logloss: 12.87348|  0:00:08s\n",
            "epoch 1  | loss: 1.34322 | eval_logloss: 14.82642|  0:00:19s\n",
            "epoch 2  | loss: 1.0787  | eval_logloss: 12.65845|  0:00:29s\n",
            "epoch 3  | loss: 1.02473 | eval_logloss: 9.31076 |  0:00:38s\n",
            "epoch 4  | loss: 0.94744 | eval_logloss: 13.22309|  0:00:47s\n",
            "epoch 5  | loss: 0.91763 | eval_logloss: 11.69855|  0:00:57s\n",
            "epoch 6  | loss: 0.91225 | eval_logloss: 12.40859|  0:01:07s\n",
            "\n",
            "Early stopping occurred at epoch 6 with best_epoch = 3 and best_eval_logloss = 9.31076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID50: 28"
      ],
      "metadata": {
        "id": "rzTZGxOaodhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=28)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281f05e7-0e35-498e-ebfd-5c90b75deace",
        "id": "zF78ljvSHV8H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.51465 | eval_logloss: 10.40119|  0:00:28s\n",
            "epoch 1  | loss: 1.215   | eval_logloss: 9.918   |  0:00:56s\n",
            "epoch 2  | loss: 0.86968 | eval_logloss: 8.05036 |  0:01:23s\n",
            "epoch 3  | loss: 0.74891 | eval_logloss: 5.88487 |  0:01:51s\n",
            "epoch 4  | loss: 0.62198 | eval_logloss: 2.33301 |  0:02:19s\n",
            "epoch 5  | loss: 0.57125 | eval_logloss: 2.11888 |  0:02:47s\n",
            "epoch 6  | loss: 0.49438 | eval_logloss: 3.0191  |  0:03:15s\n",
            "epoch 7  | loss: 0.44801 | eval_logloss: 2.85334 |  0:03:43s\n",
            "epoch 8  | loss: 0.4076  | eval_logloss: 0.75577 |  0:04:10s\n",
            "epoch 9  | loss: 0.36435 | eval_logloss: 0.71362 |  0:04:38s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 0.71362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID51: 2074"
      ],
      "metadata": {
        "id": "hrZpp5Mpodhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=2074)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a202108f-ca80-434b-cb22-bd3374c5f443",
        "id": "FyUCay7HHWmK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.51129 | eval_logloss: 12.15818|  0:00:32s\n",
            "epoch 1  | loss: 0.75999 | eval_logloss: 6.77683 |  0:01:03s\n",
            "epoch 2  | loss: 0.68029 | eval_logloss: 4.79691 |  0:01:33s\n",
            "epoch 3  | loss: 0.66508 | eval_logloss: 5.62337 |  0:02:05s\n",
            "epoch 4  | loss: 0.6356  | eval_logloss: 1.38191 |  0:02:37s\n",
            "epoch 5  | loss: 0.58669 | eval_logloss: 1.71945 |  0:03:08s\n",
            "epoch 6  | loss: 0.53544 | eval_logloss: 1.31969 |  0:03:39s\n",
            "epoch 7  | loss: 0.54407 | eval_logloss: 0.66251 |  0:04:11s\n",
            "epoch 8  | loss: 0.53615 | eval_logloss: 0.71303 |  0:04:41s\n",
            "epoch 9  | loss: 0.54719 | eval_logloss: 1.65011 |  0:05:12s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_eval_logloss = 0.66251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID52: 29"
      ],
      "metadata": {
        "id": "NPYri0bzodhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=29)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a46c30d-78c5-4f3c-b32c-3cd683ca90c4",
        "id": "1s6yYiKrHXCf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.48005 | eval_logloss: 8.08672 |  0:00:04s\n",
            "epoch 1  | loss: 1.37652 | eval_logloss: 6.00728 |  0:00:07s\n",
            "epoch 2  | loss: 0.99528 | eval_logloss: 7.62462 |  0:00:10s\n",
            "epoch 3  | loss: 0.86614 | eval_logloss: 7.77241 |  0:00:13s\n",
            "epoch 4  | loss: 0.6973  | eval_logloss: 8.59213 |  0:00:17s\n",
            "\n",
            "Early stopping occurred at epoch 4 with best_epoch = 1 and best_eval_logloss = 6.00728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID53: 45"
      ],
      "metadata": {
        "id": "U1DH2-Plodhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=45)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39634a75-9eef-4028-cb3f-e0e744cc3885",
        "id": "YOdmZDxUHXd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.10271 | eval_logloss: 11.46952|  0:00:15s\n",
            "epoch 1  | loss: 1.10271 | eval_logloss: 7.97119 |  0:00:32s\n",
            "epoch 2  | loss: 1.10271 | eval_logloss: 11.46952|  0:00:49s\n",
            "epoch 3  | loss: 1.10271 | eval_logloss: 7.97119 |  0:01:06s\n",
            "epoch 4  | loss: 1.10271 | eval_logloss: 7.97119 |  0:01:23s\n",
            "\n",
            "Early stopping occurred at epoch 4 with best_epoch = 1 and best_eval_logloss = 7.97119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID54: 125922"
      ],
      "metadata": {
        "id": "PpaDSrLwodhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=125922)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddbe2a3-78f9-495a-dd2d-0f00ceb0146d",
        "id": "rFhR_Y26HYNQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.25871 | eval_logloss: 13.14887|  0:00:25s\n",
            "epoch 1  | loss: 1.11513 | eval_logloss: 9.44277 |  0:00:52s\n",
            "epoch 2  | loss: 0.94684 | eval_logloss: 5.33629 |  0:01:17s\n",
            "epoch 3  | loss: 0.71211 | eval_logloss: 5.09027 |  0:01:44s\n",
            "epoch 4  | loss: 0.64502 | eval_logloss: 6.74302 |  0:02:10s\n",
            "epoch 5  | loss: 0.63858 | eval_logloss: 6.14965 |  0:02:37s\n",
            "epoch 6  | loss: 0.58242 | eval_logloss: 3.33172 |  0:03:05s\n",
            "epoch 7  | loss: 0.52517 | eval_logloss: 2.09625 |  0:03:32s\n",
            "epoch 8  | loss: 0.48328 | eval_logloss: 1.80142 |  0:04:00s\n",
            "epoch 9  | loss: 0.46713 | eval_logloss: 1.96301 |  0:04:26s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_eval_logloss = 1.80142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID55: 9960"
      ],
      "metadata": {
        "id": "VC-iHg1kodhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9960)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30033772-3149-4f21-9cc0-b0b123bf99ad",
        "id": "IwM7onG3HYnB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.54984 | eval_logloss: 9.47903 |  0:00:26s\n",
            "epoch 1  | loss: 0.82261 | eval_logloss: 9.10091 |  0:00:52s\n",
            "epoch 2  | loss: 0.65935 | eval_logloss: 8.04586 |  0:01:20s\n",
            "epoch 3  | loss: 0.56649 | eval_logloss: 7.88895 |  0:01:47s\n",
            "epoch 4  | loss: 0.5372  | eval_logloss: 6.3676  |  0:02:14s\n",
            "epoch 5  | loss: 0.52447 | eval_logloss: 4.94183 |  0:02:41s\n",
            "epoch 6  | loss: 0.53178 | eval_logloss: 3.8235  |  0:03:08s\n",
            "epoch 7  | loss: 0.48892 | eval_logloss: 2.3727  |  0:03:36s\n",
            "epoch 8  | loss: 0.49948 | eval_logloss: 3.48711 |  0:04:02s\n",
            "epoch 9  | loss: 0.48963 | eval_logloss: 3.17181 |  0:04:30s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_eval_logloss = 2.3727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID56: 9964"
      ],
      "metadata": {
        "id": "Y-EWQvshodhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9964)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4792641-99c7-4808-dc48-0c276fd871ae",
        "id": "tKf1Gj6THZGS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.68528 | eval_logloss: 14.05106|  0:00:10s\n",
            "epoch 1  | loss: 2.98703 | eval_logloss: 14.24641|  0:00:19s\n",
            "epoch 2  | loss: 2.43497 | eval_logloss: 14.04329|  0:00:26s\n",
            "epoch 3  | loss: 2.15597 | eval_logloss: 14.29339|  0:00:34s\n",
            "epoch 4  | loss: 1.93001 | eval_logloss: 13.65193|  0:00:38s\n",
            "epoch 5  | loss: 1.58914 | eval_logloss: 13.45126|  0:00:43s\n",
            "epoch 6  | loss: 1.50048 | eval_logloss: 12.56901|  0:00:48s\n",
            "epoch 7  | loss: 1.27909 | eval_logloss: 11.61492|  0:00:53s\n",
            "epoch 8  | loss: 1.21407 | eval_logloss: 10.66038|  0:00:58s\n",
            "epoch 9  | loss: 1.15783 | eval_logloss: 8.9243  |  0:01:03s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 8.9243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID57: 22"
      ],
      "metadata": {
        "id": "Je4bKpz9odhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=22)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5e0935-b314-4a38-8823-9c393497823c",
        "id": "aq_0c12IHZgI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.62943 | eval_logloss: 14.41203|  0:00:05s\n",
            "epoch 1  | loss: 2.63318 | eval_logloss: 14.388  |  0:00:10s\n",
            "epoch 2  | loss: 2.01606 | eval_logloss: 12.73724|  0:00:15s\n",
            "epoch 3  | loss: 1.66916 | eval_logloss: 12.03187|  0:00:21s\n",
            "epoch 4  | loss: 1.40582 | eval_logloss: 12.62031|  0:00:26s\n",
            "epoch 5  | loss: 1.29474 | eval_logloss: 9.84051 |  0:00:32s\n",
            "epoch 6  | loss: 1.17072 | eval_logloss: 10.6358 |  0:00:37s\n",
            "epoch 7  | loss: 1.08807 | eval_logloss: 7.63036 |  0:00:42s\n",
            "epoch 8  | loss: 1.01745 | eval_logloss: 9.25853 |  0:00:48s\n",
            "epoch 9  | loss: 1.00206 | eval_logloss: 8.77457 |  0:00:53s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_eval_logloss = 7.63036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID58: 2079"
      ],
      "metadata": {
        "id": "MafyzJa5odhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=2079)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c363084e-2fc9-47bb-ba4f-6e934978fdc7",
        "id": "idpu_ccqHZ8U"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.33494 | eval_logloss: 12.81854|  0:00:05s\n",
            "epoch 1  | loss: 2.76566 | eval_logloss: 13.57257|  0:00:12s\n",
            "epoch 2  | loss: 1.84611 | eval_logloss: 13.62319|  0:00:16s\n",
            "epoch 3  | loss: 1.27614 | eval_logloss: 12.17223|  0:00:21s\n",
            "epoch 4  | loss: 1.24854 | eval_logloss: 10.01785|  0:00:28s\n",
            "epoch 5  | loss: 1.16541 | eval_logloss: 12.5888 |  0:00:32s\n",
            "epoch 6  | loss: 1.20899 | eval_logloss: 12.61539|  0:00:35s\n",
            "epoch 7  | loss: 1.04628 | eval_logloss: 12.37821|  0:00:39s\n",
            "\n",
            "Early stopping occurred at epoch 7 with best_epoch = 4 and best_eval_logloss = 10.01785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID59: 14969"
      ],
      "metadata": {
        "id": "niuvw8Lxodhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=14969)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0292be-0fec-4e84-acac-a79cd517c7b1",
        "id": "_-ARZhuhHadC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.78831 | eval_logloss: 9.95755 |  0:00:49s\n",
            "epoch 1  | loss: 1.33728 | eval_logloss: 1.69743 |  0:01:38s\n",
            "epoch 2  | loss: 1.31625 | eval_logloss: 2.08021 |  0:02:27s\n",
            "epoch 3  | loss: 1.32187 | eval_logloss: 3.15533 |  0:03:17s\n",
            "epoch 4  | loss: 1.31574 | eval_logloss: 1.50279 |  0:04:08s\n",
            "epoch 5  | loss: 1.29438 | eval_logloss: 1.73998 |  0:04:57s\n",
            "epoch 6  | loss: 1.31121 | eval_logloss: 3.35892 |  0:05:47s\n",
            "epoch 7  | loss: 1.29905 | eval_logloss: 3.95843 |  0:06:35s\n",
            "\n",
            "Early stopping occurred at epoch 7 with best_epoch = 4 and best_eval_logloss = 1.50279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID60: 3560"
      ],
      "metadata": {
        "id": "VSKHOKxsodhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3560)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f7143c-9949-4adf-856c-ae8975342fe4",
        "id": "nVHf7sj9HbDH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.45934 | eval_logloss: 13.67008|  0:00:04s\n",
            "epoch 1  | loss: 2.43579 | eval_logloss: 11.94223|  0:00:08s\n",
            "epoch 2  | loss: 1.89646 | eval_logloss: 13.75031|  0:00:12s\n",
            "epoch 3  | loss: 1.84084 | eval_logloss: 13.39133|  0:00:16s\n",
            "epoch 4  | loss: 1.79361 | eval_logloss: 12.15877|  0:00:20s\n",
            "\n",
            "Early stopping occurred at epoch 4 with best_epoch = 1 and best_eval_logloss = 11.94223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID61: 14952"
      ],
      "metadata": {
        "id": "seu130Weodhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=14952)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db1b9f2-6dcd-4ff3-f02c-cdc6a4f1d943",
        "id": "H2z0ETp4HbdP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.60541 | eval_logloss: 2.28642 |  0:00:54s\n",
            "epoch 1  | loss: 0.27768 | eval_logloss: 1.92621 |  0:01:50s\n",
            "epoch 2  | loss: 0.28006 | eval_logloss: 0.48772 |  0:02:46s\n",
            "epoch 3  | loss: 0.25894 | eval_logloss: 0.27911 |  0:03:41s\n",
            "epoch 4  | loss: 0.22448 | eval_logloss: 0.42473 |  0:04:36s\n",
            "epoch 5  | loss: 0.20491 | eval_logloss: 0.89658 |  0:05:30s\n",
            "epoch 6  | loss: 0.18057 | eval_logloss: 1.24414 |  0:06:25s\n",
            "\n",
            "Early stopping occurred at epoch 6 with best_epoch = 3 and best_eval_logloss = 0.27911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID62: 125920"
      ],
      "metadata": {
        "id": "rRFhBYXoodhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import openml\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class TabNetModel:\n",
        "    def __init__(self, params, args):\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.params[\"n_a\"] = self.params[\"n_d\"]\n",
        "        self.params[\"cat_idxs\"] = args.cat_idx\n",
        "        self.params[\"cat_dims\"] = args.cat_dims\n",
        "        self.params[\"device_name\"] = device  # Set device for TabNet\n",
        "\n",
        "        if args.objective == \"regression\":\n",
        "            self.model = TabNetRegressor(**self.params)\n",
        "            self.metric = \"rmse\"\n",
        "        elif args.objective == \"classification\":\n",
        "            self.model = TabNetClassifier(**self.params)\n",
        "            self.metric = \"logloss\"\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if self.args.objective == \"regression\":\n",
        "            y, y_val = y.reshape(-1, 1), y_val.reshape(-1, 1)\n",
        "\n",
        "        drop_last = X.shape[0] % self.args.batch_size == 1\n",
        "        self.model.fit(\n",
        "            X,\n",
        "            y,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_name=[\"eval\"],\n",
        "            eval_metric=[self.metric],\n",
        "            max_epochs=self.args.epochs,\n",
        "            patience=self.args.early_stopping_rounds,\n",
        "            batch_size=self.args.batch_size,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        history = self.model.history\n",
        "        return history[\"loss\"], history[\"eval_\" + self.metric]\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        if self.args.objective == \"regression\":\n",
        "            return self.model.predict(X)\n",
        "        else:\n",
        "            return self.model.predict_proba(X)\n",
        "\n",
        "def define_trial_parameters(cls, trial, args):\n",
        "    params = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
        "        \"cat_emb_dim\": min(trial.suggest_int(\"cat_emb_dim\", 1, 3), 2),\n",
        "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
        "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
        "        \"momentum\": trial.suggest_float(\"momentum\", 0.001, 0.4, log=True),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax to avoid sparsemax issues\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def get_random_parameters(cls, seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"n_d\": rs.randint(8, 65),\n",
        "        \"n_steps\": rs.randint(3, 11),\n",
        "        \"gamma\": 1.0 + rs.rand(),\n",
        "        \"cat_emb_dim\": min(rs.randint(1, 4), 2),\n",
        "        \"n_independent\": rs.randint(1, 6),\n",
        "        \"n_shared\": rs.randint(1, 6),\n",
        "        \"momentum\": 0.4 * np.power(10, rs.uniform(-3, -1)),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax as alternative to sparsemax\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def load_openml_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Drop class labels for training\n",
        "    X = X.drop(columns=[dataset.default_target_attribute], errors='ignore')\n",
        "\n",
        "    # Identify and encode categorical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    cat_idx = []\n",
        "    cat_dims = []\n",
        "    for col in categorical_cols:\n",
        "        X[col], uniques = pd.factorize(X[col])\n",
        "        X[col] = X[col].replace(-1, len(uniques))  # Replace NaN factorized values with a valid new index\n",
        "        cat_idx.append(X.columns.get_loc(col))\n",
        "        cat_dims.append(len(uniques) + 1)  # Account for NaN as an additional category\n",
        "\n",
        "    # Scale numeric features and handle missing values\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    X.fillna(-1, inplace=True)  # Replace NaN with -1 for non-categorical columns\n",
        "    scaler = StandardScaler()\n",
        "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    y, _ = pd.factorize(y)\n",
        "\n",
        "    return X, y, cat_dims, cat_idx\n",
        "\n",
        "\n",
        "# Example configuration and arguments\n",
        "class Args:\n",
        "    objective = \"classification\"  # Change to \"regression\" if needed\n",
        "    batch_size = 6  # Larger batch size for numerical stability\n",
        "    epochs = 10\n",
        "    early_stopping_rounds = 3\n",
        "\n",
        "    def __init__(self, cat_idx, cat_dims):\n",
        "        self.cat_idx = cat_idx\n",
        "        self.cat_dims = cat_dims\n",
        "\n",
        "# Load data and set up arguments\n",
        "X, y, cat_dims, cat_idx = load_openml_data(task_id=125920)  # Example OpenML task ID\n",
        "args = Args(cat_idx, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameters and initialize the model\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "params[\"device_name\"] = device\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3156ca-90f9-4dd4-de99-77efc5af50d5",
        "id": "ZkOZ3RXUHb5n"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.31841 | eval_logloss: 8.44946 |  0:00:10s\n",
            "epoch 1  | loss: 1.1383  | eval_logloss: 7.77588 |  0:00:19s\n",
            "epoch 2  | loss: 0.74923 | eval_logloss: 9.24658 |  0:00:29s\n",
            "epoch 3  | loss: 0.70811 | eval_logloss: 4.90499 |  0:00:41s\n",
            "epoch 4  | loss: 0.69134 | eval_logloss: 7.42831 |  0:00:49s\n",
            "epoch 5  | loss: 0.71446 | eval_logloss: 2.69128 |  0:00:59s\n",
            "epoch 6  | loss: 0.71875 | eval_logloss: 3.57783 |  0:01:05s\n",
            "epoch 7  | loss: 0.69537 | eval_logloss: 4.63117 |  0:01:12s\n",
            "epoch 8  | loss: 0.70034 | eval_logloss: 0.9218  |  0:01:18s\n",
            "epoch 9  | loss: 0.69606 | eval_logloss: 0.73823 |  0:01:25s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 0.73823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID63: 23"
      ],
      "metadata": {
        "id": "0yTf232godho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=23)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d769f1-ee77-49d5-a62e-22b02568b9bf",
        "id": "JoikBqPoHcbD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.3021  | eval_logloss: 9.88968 |  0:00:04s\n",
            "epoch 1  | loss: 1.4079  | eval_logloss: 9.56543 |  0:00:08s\n",
            "epoch 2  | loss: 1.11022 | eval_logloss: 9.85276 |  0:00:11s\n",
            "epoch 3  | loss: 1.06278 | eval_logloss: 9.04951 |  0:00:15s\n",
            "epoch 4  | loss: 1.01892 | eval_logloss: 9.56996 |  0:00:22s\n",
            "epoch 5  | loss: 1.04256 | eval_logloss: 9.00102 |  0:00:27s\n",
            "epoch 6  | loss: 0.99035 | eval_logloss: 7.67257 |  0:00:35s\n",
            "epoch 7  | loss: 0.98769 | eval_logloss: 8.34191 |  0:00:40s\n",
            "epoch 8  | loss: 0.99289 | eval_logloss: 7.85675 |  0:00:49s\n",
            "epoch 9  | loss: 0.98539 | eval_logloss: 8.16619 |  0:00:52s\n",
            "\n",
            "Early stopping occurred at epoch 9 with best_epoch = 6 and best_eval_logloss = 7.67257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID64: 3904"
      ],
      "metadata": {
        "id": "j55mck9yodho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3904)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0373fb-ab75-4f6a-ce03-1b16d5e83db2",
        "id": "OQEYq1-DHdli"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.69085 | eval_logloss: 3.02211 |  0:00:29s\n",
            "epoch 1  | loss: 0.47165 | eval_logloss: 0.64944 |  0:00:57s\n",
            "epoch 2  | loss: 0.47162 | eval_logloss: 1.28842 |  0:01:26s\n",
            "epoch 3  | loss: 0.47037 | eval_logloss: 0.85295 |  0:01:55s\n",
            "epoch 4  | loss: 0.46402 | eval_logloss: 0.47787 |  0:02:25s\n",
            "epoch 5  | loss: 0.46248 | eval_logloss: 0.47846 |  0:02:52s\n",
            "epoch 6  | loss: 0.46463 | eval_logloss: 0.49844 |  0:03:21s\n",
            "epoch 7  | loss: 0.46658 | eval_logloss: 0.55034 |  0:03:50s\n",
            "\n",
            "Early stopping occurred at epoch 7 with best_epoch = 4 and best_eval_logloss = 0.47787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID65: 3022"
      ],
      "metadata": {
        "id": "LOoUPfmSodho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3022)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df3f938-9889-4967-d0c1-e946c04c795e",
        "id": "qhNhaA2XHeHM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.83868 | eval_logloss: 14.33204|  0:00:02s\n",
            "epoch 1  | loss: 3.06355 | eval_logloss: 13.44636|  0:00:05s\n",
            "epoch 2  | loss: 2.74071 | eval_logloss: 14.25153|  0:00:07s\n",
            "epoch 3  | loss: 2.4456  | eval_logloss: 13.26899|  0:00:10s\n",
            "epoch 4  | loss: 2.03694 | eval_logloss: 13.60739|  0:00:13s\n",
            "epoch 5  | loss: 1.82103 | eval_logloss: 12.95499|  0:00:16s\n",
            "epoch 6  | loss: 1.62725 | eval_logloss: 12.83425|  0:00:18s\n",
            "epoch 7  | loss: 1.4455  | eval_logloss: 13.4795 |  0:00:20s\n",
            "epoch 8  | loss: 1.34725 | eval_logloss: 13.1243 |  0:00:23s\n",
            "epoch 9  | loss: 1.39547 | eval_logloss: 12.96336|  0:00:25s\n",
            "\n",
            "Early stopping occurred at epoch 9 with best_epoch = 6 and best_eval_logloss = 12.83425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.1919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID66: 9985"
      ],
      "metadata": {
        "id": "jGBIY2deodho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9985)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f0063a-7106-45d6-e1f5-36e7cfbdea69",
        "id": "kTvPWPk3HekO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.42217 | eval_logloss: 10.8249 |  0:00:16s\n",
            "epoch 1  | loss: 1.60615 | eval_logloss: 10.70007|  0:00:32s\n",
            "epoch 2  | loss: 1.53296 | eval_logloss: 10.67791|  0:00:49s\n",
            "epoch 3  | loss: 1.50847 | eval_logloss: 7.18164 |  0:01:06s\n",
            "epoch 4  | loss: 1.49401 | eval_logloss: 3.21664 |  0:01:22s\n",
            "epoch 5  | loss: 1.48047 | eval_logloss: 3.2488  |  0:01:38s\n",
            "epoch 6  | loss: 1.49348 | eval_logloss: 4.74702 |  0:01:53s\n",
            "epoch 7  | loss: 1.49008 | eval_logloss: 5.15324 |  0:02:09s\n",
            "\n",
            "Early stopping occurred at epoch 7 with best_epoch = 4 and best_eval_logloss = 3.21664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID67: 9910"
      ],
      "metadata": {
        "id": "i5gFNGDeodhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=9910)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d59106-666c-496a-c3e4-47fe496f5ede",
        "id": "FWjxggM_HfWQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.29178 | eval_logloss: 9.00076 |  0:00:25s\n",
            "epoch 1  | loss: 0.89262 | eval_logloss: 9.06445 |  0:00:51s\n",
            "epoch 2  | loss: 0.71167 | eval_logloss: 5.5455  |  0:01:18s\n",
            "epoch 3  | loss: 0.70148 | eval_logloss: 5.43918 |  0:01:43s\n",
            "epoch 4  | loss: 0.68886 | eval_logloss: 2.96545 |  0:02:08s\n",
            "epoch 5  | loss: 0.68351 | eval_logloss: 1.76203 |  0:02:35s\n",
            "epoch 6  | loss: 0.66777 | eval_logloss: 1.22934 |  0:03:00s\n",
            "epoch 7  | loss: 0.65735 | eval_logloss: 1.32232 |  0:03:26s\n",
            "epoch 8  | loss: 0.62609 | eval_logloss: 3.20282 |  0:03:52s\n",
            "epoch 9  | loss: 0.58995 | eval_logloss: 3.42794 |  0:04:19s\n",
            "\n",
            "Early stopping occurred at epoch 9 with best_epoch = 6 and best_eval_logloss = 1.22934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID68: 14970"
      ],
      "metadata": {
        "id": "0CdFbMZoodhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=14970)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d941e1a2-f429-46f6-815b-5dc6d0e87a95",
        "id": "DMz9n8LAHfz8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.64427 | eval_logloss: 11.77286|  0:00:37s\n",
            "epoch 1  | loss: 0.89124 | eval_logloss: 9.11257 |  0:01:13s\n",
            "epoch 2  | loss: 0.45323 | eval_logloss: 3.97269 |  0:01:49s\n",
            "epoch 3  | loss: 0.2724  | eval_logloss: 2.01379 |  0:02:27s\n",
            "epoch 4  | loss: 0.1953  | eval_logloss: 3.0766  |  0:03:04s\n",
            "epoch 5  | loss: 0.19732 | eval_logloss: 4.46687 |  0:03:40s\n",
            "epoch 6  | loss: 0.13282 | eval_logloss: 6.04901 |  0:04:17s\n",
            "\n",
            "Early stopping occurred at epoch 6 with best_epoch = 3 and best_eval_logloss = 2.01379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID69: 3021"
      ],
      "metadata": {
        "id": "9vOb8d79odhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import openml\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class TabNetModel:\n",
        "    def __init__(self, params, args):\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.params[\"n_a\"] = self.params[\"n_d\"]\n",
        "        self.params[\"cat_idxs\"] = args.cat_idx\n",
        "        self.params[\"cat_dims\"] = args.cat_dims\n",
        "        self.params[\"device_name\"] = device  # Set device for TabNet\n",
        "\n",
        "        if args.objective == \"regression\":\n",
        "            self.model = TabNetRegressor(**self.params)\n",
        "            self.metric = \"rmse\"\n",
        "        elif args.objective == \"classification\":\n",
        "            self.model = TabNetClassifier(**self.params)\n",
        "            self.metric = \"logloss\"\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if self.args.objective == \"regression\":\n",
        "            y, y_val = y.reshape(-1, 1), y_val.reshape(-1, 1)\n",
        "\n",
        "        drop_last = X.shape[0] % self.args.batch_size == 1\n",
        "        self.model.fit(\n",
        "            X,\n",
        "            y,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_name=[\"eval\"],\n",
        "            eval_metric=[self.metric],\n",
        "            max_epochs=self.args.epochs,\n",
        "            patience=self.args.early_stopping_rounds,\n",
        "            batch_size=self.args.batch_size,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        history = self.model.history\n",
        "        return history[\"loss\"], history[\"eval_\" + self.metric]\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        if self.args.objective == \"regression\":\n",
        "            return self.model.predict(X)\n",
        "        else:\n",
        "            return self.model.predict_proba(X)\n",
        "\n",
        "def define_trial_parameters(cls, trial, args):\n",
        "    params = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
        "        \"cat_emb_dim\": min(trial.suggest_int(\"cat_emb_dim\", 1, 3), 2),\n",
        "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
        "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
        "        \"momentum\": trial.suggest_float(\"momentum\", 0.001, 0.4, log=True),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax to avoid sparsemax issues\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def get_random_parameters(cls, seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"n_d\": rs.randint(8, 65),\n",
        "        \"n_steps\": rs.randint(3, 11),\n",
        "        \"gamma\": 1.0 + rs.rand(),\n",
        "        \"cat_emb_dim\": min(rs.randint(1, 4), 2),\n",
        "        \"n_independent\": rs.randint(1, 6),\n",
        "        \"n_shared\": rs.randint(1, 6),\n",
        "        \"momentum\": 0.4 * np.power(10, rs.uniform(-3, -1)),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax as alternative to sparsemax\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def load_openml_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Drop class labels for training\n",
        "    X = X.drop(columns=[dataset.default_target_attribute], errors='ignore')\n",
        "\n",
        "    # Identify and encode categorical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    cat_idx = []\n",
        "    cat_dims = []\n",
        "    for col in categorical_cols:\n",
        "        X[col], uniques = pd.factorize(X[col])\n",
        "        X[col] = X[col].replace(-1, len(uniques))  # Replace NaN factorized values with a valid new index\n",
        "        cat_idx.append(X.columns.get_loc(col))\n",
        "        cat_dims.append(len(uniques) + 1)  # Account for NaN as an additional category\n",
        "\n",
        "    # Scale numeric features and handle missing values\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    X.fillna(-1, inplace=True)  # Replace NaN with -1 for non-categorical columns\n",
        "    scaler = StandardScaler()\n",
        "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    y, _ = pd.factorize(y)\n",
        "\n",
        "    return X, y, cat_dims, cat_idx\n",
        "\n",
        "\n",
        "# Example configuration and arguments\n",
        "class Args:\n",
        "    objective = \"classification\"  # Change to \"regression\" if needed\n",
        "    batch_size = 64  # Larger batch size for numerical stability\n",
        "    epochs = 10\n",
        "    early_stopping_rounds = 3\n",
        "\n",
        "    def __init__(self, cat_idx, cat_dims):\n",
        "        self.cat_idx = cat_idx\n",
        "        self.cat_dims = cat_dims\n",
        "\n",
        "# Load data and set up arguments\n",
        "X, y, cat_dims, cat_idx = load_openml_data(task_id=3021)  # Example OpenML task ID\n",
        "args = Args(cat_idx, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameters and initialize the model\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "params[\"device_name\"] = device\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f716cab0-76d2-4ebd-943d-7fed6663bd45",
        "id": "qhmNDCfZHgQe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.39779 | eval_logloss: 0.9362  |  0:00:08s\n",
            "epoch 1  | loss: 0.30459 | eval_logloss: 2.02867 |  0:00:18s\n",
            "epoch 2  | loss: 0.17925 | eval_logloss: 0.95467 |  0:00:32s\n",
            "epoch 3  | loss: 0.12473 | eval_logloss: 1.06857 |  0:00:39s\n",
            "\n",
            "Early stopping occurred at epoch 3 with best_epoch = 0 and best_eval_logloss = 0.9362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID70: 3481"
      ],
      "metadata": {
        "id": "Otgfq1qXodhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3481)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb06784d-d02e-4b62-b74b-1a25248a4061",
        "id": "sWrJOIFTHgrp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.64021 | eval_logloss: 15.17622|  0:00:33s\n",
            "epoch 1  | loss: 1.94732 | eval_logloss: 14.67334|  0:01:25s\n",
            "epoch 2  | loss: 1.36603 | eval_logloss: 12.75947|  0:01:56s\n",
            "epoch 3  | loss: 1.0548  | eval_logloss: 10.73966|  0:02:24s\n",
            "epoch 4  | loss: 0.82436 | eval_logloss: 10.94935|  0:02:53s\n",
            "epoch 5  | loss: 0.77931 | eval_logloss: 9.17639 |  0:03:21s\n",
            "epoch 6  | loss: 0.6501  | eval_logloss: 6.36518 |  0:03:49s\n",
            "epoch 7  | loss: 0.58232 | eval_logloss: 6.5604  |  0:04:18s\n",
            "epoch 8  | loss: 0.52974 | eval_logloss: 8.6821  |  0:04:47s\n",
            "epoch 9  | loss: 0.47891 | eval_logloss: 7.94506 |  0:05:16s\n",
            "\n",
            "Early stopping occurred at epoch 9 with best_epoch = 6 and best_eval_logloss = 6.36518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID71: 3573"
      ],
      "metadata": {
        "id": "2FsAl4bkodhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=3573)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "Z-mK5XSuHhH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID72: 146824"
      ],
      "metadata": {
        "id": "W4vWS1BZodhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=146824)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba9e7e8-e922-4f41-c47a-988849c62d47",
        "id": "wfq5P35bHhhT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.47736 | eval_logloss: 13.63074|  0:00:06s\n",
            "epoch 1  | loss: 2.97089 | eval_logloss: 13.98944|  0:00:12s\n",
            "epoch 2  | loss: 2.20004 | eval_logloss: 13.34332|  0:00:18s\n",
            "epoch 3  | loss: 1.59475 | eval_logloss: 11.96605|  0:00:24s\n",
            "epoch 4  | loss: 1.31504 | eval_logloss: 10.83907|  0:00:29s\n",
            "epoch 5  | loss: 0.97346 | eval_logloss: 10.56946|  0:00:35s\n",
            "epoch 6  | loss: 0.83513 | eval_logloss: 8.05052 |  0:00:41s\n",
            "epoch 7  | loss: 0.72657 | eval_logloss: 9.93368 |  0:00:46s\n",
            "epoch 8  | loss: 0.64297 | eval_logloss: 9.11678 |  0:00:53s\n",
            "epoch 9  | loss: 0.58012 | eval_logloss: 5.32921 |  0:00:58s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 5.32921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID73: 146820"
      ],
      "metadata": {
        "id": "jyz2Eg0sodhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=146820)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eafd32cb-386c-4c0e-c9a2-2c118a28d1d9",
        "id": "jm5mv92wHiDK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.25688 | eval_logloss: 0.85641 |  0:00:12s\n",
            "epoch 1  | loss: 0.14395 | eval_logloss: 1.88568 |  0:00:24s\n",
            "epoch 2  | loss: 0.13104 | eval_logloss: 0.80956 |  0:00:36s\n",
            "epoch 3  | loss: 0.13393 | eval_logloss: 1.69857 |  0:00:48s\n",
            "epoch 4  | loss: 0.11677 | eval_logloss: 1.0874  |  0:01:00s\n",
            "epoch 5  | loss: 0.07932 | eval_logloss: 2.48067 |  0:01:13s\n",
            "\n",
            "Early stopping occurred at epoch 5 with best_epoch = 2 and best_eval_logloss = 0.80956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID74: 146822"
      ],
      "metadata": {
        "id": "8xFV2R8aodhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=146822)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d801bcfa-de35-4fa7-8cc7-52604b5de067",
        "id": "lm0DncrCHilU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.11134 | eval_logloss: 11.90503|  0:00:05s\n",
            "epoch 1  | loss: 1.22501 | eval_logloss: 13.21631|  0:00:12s\n",
            "epoch 2  | loss: 1.03007 | eval_logloss: 11.3184 |  0:00:17s\n",
            "epoch 3  | loss: 0.82236 | eval_logloss: 11.73652|  0:00:24s\n",
            "epoch 4  | loss: 0.83585 | eval_logloss: 11.02817|  0:00:30s\n",
            "epoch 5  | loss: 0.69213 | eval_logloss: 11.18421|  0:00:36s\n",
            "epoch 6  | loss: 0.6192  | eval_logloss: 10.03292|  0:00:42s\n",
            "epoch 7  | loss: 0.66025 | eval_logloss: 9.87935 |  0:00:47s\n",
            "epoch 8  | loss: 0.60797 | eval_logloss: 10.61304|  0:00:54s\n",
            "epoch 9  | loss: 0.52615 | eval_logloss: 9.25311 |  0:00:59s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 9.25311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID75: 146195"
      ],
      "metadata": {
        "id": "xcX_XqgDodhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import openml\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class TabNetModel:\n",
        "    def __init__(self, params, args):\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.params[\"n_a\"] = self.params[\"n_d\"]\n",
        "        self.params[\"cat_idxs\"] = args.cat_idx\n",
        "        self.params[\"cat_dims\"] = args.cat_dims\n",
        "        self.params[\"device_name\"] = device  # Set device for TabNet\n",
        "\n",
        "        if args.objective == \"regression\":\n",
        "            self.model = TabNetRegressor(**self.params)\n",
        "            self.metric = \"rmse\"\n",
        "        elif args.objective == \"classification\":\n",
        "            self.model = TabNetClassifier(**self.params)\n",
        "            self.metric = \"logloss\"\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if self.args.objective == \"regression\":\n",
        "            y, y_val = y.reshape(-1, 1), y_val.reshape(-1, 1)\n",
        "\n",
        "        drop_last = X.shape[0] % self.args.batch_size == 1\n",
        "        self.model.fit(\n",
        "            X,\n",
        "            y,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_name=[\"eval\"],\n",
        "            eval_metric=[self.metric],\n",
        "            max_epochs=self.args.epochs,\n",
        "            patience=self.args.early_stopping_rounds,\n",
        "            batch_size=self.args.batch_size,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        history = self.model.history\n",
        "        return history[\"loss\"], history[\"eval_\" + self.metric]\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        if self.args.objective == \"regression\":\n",
        "            return self.model.predict(X)\n",
        "        else:\n",
        "            return self.model.predict_proba(X)\n",
        "\n",
        "def define_trial_parameters(cls, trial, args):\n",
        "    params = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
        "        \"cat_emb_dim\": min(trial.suggest_int(\"cat_emb_dim\", 1, 3), 2),\n",
        "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
        "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
        "        \"momentum\": trial.suggest_float(\"momentum\", 0.001, 0.4, log=True),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax to avoid sparsemax issues\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def get_random_parameters(cls, seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"n_d\": rs.randint(8, 65),\n",
        "        \"n_steps\": rs.randint(3, 11),\n",
        "        \"gamma\": 1.0 + rs.rand(),\n",
        "        \"cat_emb_dim\": min(rs.randint(1, 4), 2),\n",
        "        \"n_independent\": rs.randint(1, 6),\n",
        "        \"n_shared\": rs.randint(1, 6),\n",
        "        \"momentum\": 0.4 * np.power(10, rs.uniform(-3, -1)),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax as alternative to sparsemax\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def load_openml_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Drop class labels for training\n",
        "    X = X.drop(columns=[dataset.default_target_attribute], errors='ignore')\n",
        "\n",
        "    # Identify and encode categorical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    cat_idx = []\n",
        "    cat_dims = []\n",
        "    for col in categorical_cols:\n",
        "        X[col], uniques = pd.factorize(X[col])\n",
        "        X[col] = X[col].replace(-1, len(uniques))  # Replace NaN factorized values with a valid new index\n",
        "        cat_idx.append(X.columns.get_loc(col))\n",
        "        cat_dims.append(len(uniques) + 1)  # Account for NaN as an additional category\n",
        "\n",
        "    # Scale numeric features and handle missing values\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    X.fillna(-1, inplace=True)  # Replace NaN with -1 for non-categorical columns\n",
        "    scaler = StandardScaler()\n",
        "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    y, _ = pd.factorize(y)\n",
        "\n",
        "    return X, y, cat_dims, cat_idx\n",
        "\n",
        "\n",
        "# Example configuration and arguments\n",
        "class Args:\n",
        "    objective = \"classification\"  # Change to \"regression\" if needed\n",
        "    batch_size = 64  # Larger batch size for numerical stability\n",
        "    epochs = 10\n",
        "    early_stopping_rounds = 3\n",
        "\n",
        "    def __init__(self, cat_idx, cat_dims):\n",
        "        self.cat_idx = cat_idx\n",
        "        self.cat_dims = cat_dims\n",
        "\n",
        "# Load data and set up arguments\n",
        "X, y, cat_dims, cat_idx = load_openml_data(task_id=146195)  # Example OpenML task ID\n",
        "args = Args(cat_idx, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameters and initialize the model\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "params[\"device_name\"] = device\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ced93b0-4c1b-4227-ba68-a1a2db796595",
        "id": "UKGanPggHjpN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.88735 | eval_logloss: 1.85102 |  0:01:56s\n",
            "epoch 1  | loss: 0.83127 | eval_logloss: 1.09154 |  0:03:34s\n",
            "epoch 2  | loss: 0.79927 | eval_logloss: 0.96378 |  0:05:13s\n",
            "epoch 3  | loss: 0.79278 | eval_logloss: 1.04925 |  0:06:52s\n",
            "epoch 4  | loss: 0.79345 | eval_logloss: 0.8551  |  0:08:32s\n",
            "epoch 5  | loss: 0.75154 | eval_logloss: 0.92834 |  0:10:11s\n",
            "epoch 6  | loss: 0.71754 | eval_logloss: 1.10864 |  0:11:51s\n",
            "epoch 7  | loss: 0.7025  | eval_logloss: 0.80745 |  0:13:31s\n",
            "epoch 8  | loss: 0.69249 | eval_logloss: 0.79891 |  0:15:08s\n",
            "epoch 9  | loss: 0.66537 | eval_logloss: 0.73321 |  0:16:47s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 0.73321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID76: 146800"
      ],
      "metadata": {
        "id": "djRliAGpodhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import openml\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class TabNetModel:\n",
        "    def __init__(self, params, args):\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.params[\"n_a\"] = self.params[\"n_d\"]\n",
        "        self.params[\"cat_idxs\"] = args.cat_idx\n",
        "        self.params[\"cat_dims\"] = args.cat_dims\n",
        "        self.params[\"device_name\"] = device  # Set device for TabNet\n",
        "\n",
        "        if args.objective == \"regression\":\n",
        "            self.model = TabNetRegressor(**self.params)\n",
        "            self.metric = \"rmse\"\n",
        "        elif args.objective == \"classification\":\n",
        "            self.model = TabNetClassifier(**self.params)\n",
        "            self.metric = \"logloss\"\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if self.args.objective == \"regression\":\n",
        "            y, y_val = y.reshape(-1, 1), y_val.reshape(-1, 1)\n",
        "\n",
        "        drop_last = X.shape[0] % self.args.batch_size == 1\n",
        "        self.model.fit(\n",
        "            X,\n",
        "            y,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_name=[\"eval\"],\n",
        "            eval_metric=[self.metric],\n",
        "            max_epochs=self.args.epochs,\n",
        "            patience=self.args.early_stopping_rounds,\n",
        "            batch_size=self.args.batch_size,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        history = self.model.history\n",
        "        return history[\"loss\"], history[\"eval_\" + self.metric]\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        if self.args.objective == \"regression\":\n",
        "            return self.model.predict(X)\n",
        "        else:\n",
        "            return self.model.predict_proba(X)\n",
        "\n",
        "def define_trial_parameters(cls, trial, args):\n",
        "    params = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
        "        \"cat_emb_dim\": min(trial.suggest_int(\"cat_emb_dim\", 1, 3), 2),\n",
        "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
        "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
        "        \"momentum\": trial.suggest_float(\"momentum\", 0.001, 0.4, log=True),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax to avoid sparsemax issues\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def get_random_parameters(cls, seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"n_d\": rs.randint(8, 65),\n",
        "        \"n_steps\": rs.randint(3, 11),\n",
        "        \"gamma\": 1.0 + rs.rand(),\n",
        "        \"cat_emb_dim\": min(rs.randint(1, 4), 2),\n",
        "        \"n_independent\": rs.randint(1, 6),\n",
        "        \"n_shared\": rs.randint(1, 6),\n",
        "        \"momentum\": 0.4 * np.power(10, rs.uniform(-3, -1)),\n",
        "        \"mask_type\": \"entmax\",  # Use entmax as alternative to sparsemax\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def load_openml_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Drop class labels for training\n",
        "    X = X.drop(columns=[dataset.default_target_attribute], errors='ignore')\n",
        "\n",
        "    # Identify and encode categorical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    cat_idx = []\n",
        "    cat_dims = []\n",
        "    for col in categorical_cols:\n",
        "        X[col], uniques = pd.factorize(X[col])\n",
        "        X[col] = X[col].replace(-1, len(uniques))  # Replace NaN factorized values with a valid new index\n",
        "        cat_idx.append(X.columns.get_loc(col))\n",
        "        cat_dims.append(len(uniques) + 1)  # Account for NaN as an additional category\n",
        "\n",
        "    # Scale numeric features and handle missing values\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    X.fillna(-1, inplace=True)  # Replace NaN with -1 for non-categorical columns\n",
        "    scaler = StandardScaler()\n",
        "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    y, _ = pd.factorize(y)\n",
        "\n",
        "    return X, y, cat_dims, cat_idx\n",
        "\n",
        "\n",
        "# Example configuration and arguments\n",
        "class Args:\n",
        "    objective = \"classification\"  # Change to \"regression\" if needed\n",
        "    batch_size = 16  # Larger batch size for numerical stability\n",
        "    epochs = 10\n",
        "    early_stopping_rounds = 3\n",
        "\n",
        "    def __init__(self, cat_idx, cat_dims):\n",
        "        self.cat_idx = cat_idx\n",
        "        self.cat_dims = cat_dims\n",
        "\n",
        "# Load data and set up arguments\n",
        "X, y, cat_dims, cat_idx = load_openml_data(task_id=146800)  # Example OpenML task ID\n",
        "args = Args(cat_idx, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameters and initialize the model\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "params[\"device_name\"] = device\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1fe83e6-51f6-4ed2-e0c1-8c80b0e507b9",
        "id": "nMeUCmNrHkFs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 3.64069 | eval_logloss: 13.72817|  0:00:08s\n",
            "epoch 1  | loss: 3.06553 | eval_logloss: 13.65436|  0:00:18s\n",
            "epoch 2  | loss: 2.12764 | eval_logloss: 14.17106|  0:00:25s\n",
            "epoch 3  | loss: 1.75403 | eval_logloss: 12.99573|  0:00:36s\n",
            "epoch 4  | loss: 1.51718 | eval_logloss: 13.03454|  0:00:45s\n",
            "epoch 5  | loss: 1.49023 | eval_logloss: 12.67445|  0:00:54s\n",
            "epoch 6  | loss: 1.43927 | eval_logloss: 10.23553|  0:01:02s\n",
            "epoch 7  | loss: 1.3487  | eval_logloss: 12.07331|  0:01:07s\n",
            "epoch 8  | loss: 1.31162 | eval_logloss: 10.41501|  0:01:12s\n",
            "epoch 9  | loss: 1.23692 | eval_logloss: 10.73824|  0:01:17s\n",
            "\n",
            "Early stopping occurred at epoch 9 with best_epoch = 6 and best_eval_logloss = 10.23553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID77: 146817"
      ],
      "metadata": {
        "id": "OHIi7Bmuodhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=146817)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872ffdd2-5b35-4d1a-ab50-2120cc50c7d0",
        "id": "MpvdAl4eHkjy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 2.68165 | eval_logloss: 11.99211|  0:00:08s\n",
            "epoch 1  | loss: 1.74952 | eval_logloss: 12.22625|  0:00:18s\n",
            "epoch 2  | loss: 1.43355 | eval_logloss: 11.65258|  0:00:24s\n",
            "epoch 3  | loss: 1.29062 | eval_logloss: 10.65536|  0:00:30s\n",
            "epoch 4  | loss: 1.18507 | eval_logloss: 9.73483 |  0:00:34s\n",
            "epoch 5  | loss: 1.14264 | eval_logloss: 10.2273 |  0:00:39s\n",
            "epoch 6  | loss: 1.09982 | eval_logloss: 9.07718 |  0:00:45s\n",
            "epoch 7  | loss: 1.14554 | eval_logloss: 9.05169 |  0:00:49s\n",
            "epoch 8  | loss: 1.06585 | eval_logloss: 11.81899|  0:00:55s\n",
            "epoch 9  | loss: 1.07118 | eval_logloss: 8.62176 |  0:01:00s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 8.62176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID78: 146819"
      ],
      "metadata": {
        "id": "6YAr5rloodht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=146819)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60fbe545-36c5-4d27-a96f-de4a948c7c48",
        "id": "v6ePoxNpHlBk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.78554 | eval_logloss: 2.65706 |  0:00:01s\n",
            "epoch 1  | loss: 1.10755 | eval_logloss: 7.23312 |  0:00:03s\n",
            "epoch 2  | loss: 0.82675 | eval_logloss: 1.62376 |  0:00:05s\n",
            "epoch 3  | loss: 0.47711 | eval_logloss: 2.65706 |  0:00:06s\n",
            "epoch 4  | loss: 0.60157 | eval_logloss: 2.93957 |  0:00:07s\n",
            "epoch 5  | loss: 0.36914 | eval_logloss: 1.62376 |  0:00:09s\n",
            "epoch 6  | loss: 0.36235 | eval_logloss: 2.25002 |  0:00:10s\n",
            "epoch 7  | loss: 0.27615 | eval_logloss: 4.65629 |  0:00:11s\n",
            "epoch 8  | loss: 0.29527 | eval_logloss: 7.1624  |  0:00:13s\n",
            "\n",
            "Early stopping occurred at epoch 8 with best_epoch = 5 and best_eval_logloss = 1.62376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID79: 146821"
      ],
      "metadata": {
        "id": "QXe5jv1eodhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=146821)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "335f1fe5-3735-4faf-8d5c-1dca73c34f6d",
        "id": "fFQnNqQZHlcT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.84776 | eval_logloss: 11.79552|  0:00:04s\n",
            "epoch 1  | loss: 1.05541 | eval_logloss: 8.20157 |  0:00:08s\n",
            "epoch 2  | loss: 0.85181 | eval_logloss: 5.11446 |  0:00:13s\n",
            "epoch 3  | loss: 0.75032 | eval_logloss: 5.11446 |  0:00:17s\n",
            "epoch 4  | loss: 0.80587 | eval_logloss: 5.11446 |  0:00:22s\n",
            "epoch 5  | loss: 0.79653 | eval_logloss: 5.11446 |  0:00:27s\n",
            "\n",
            "Early stopping occurred at epoch 5 with best_epoch = 2 and best_eval_logloss = 5.11446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID80: 14954"
      ],
      "metadata": {
        "id": "OVzQtBMZodhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=14954)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2ac01e-3fde-47ab-80c3-b00ebada4d94",
        "id": "9d0urgMMHl1p"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.13078 | eval_logloss: 9.44734 |  0:00:01s\n",
            "epoch 1  | loss: 1.13367 | eval_logloss: 7.97119 |  0:00:02s\n",
            "epoch 2  | loss: 1.37395 | eval_logloss: 7.4926  |  0:00:04s\n",
            "epoch 3  | loss: 0.83519 | eval_logloss: 6.79028 |  0:00:06s\n",
            "epoch 4  | loss: 0.70683 | eval_logloss: 7.0855  |  0:00:08s\n",
            "epoch 5  | loss: 0.77105 | eval_logloss: 6.64266 |  0:00:09s\n",
            "epoch 6  | loss: 0.72537 | eval_logloss: 7.0855  |  0:00:10s\n",
            "epoch 7  | loss: 0.63287 | eval_logloss: 8.41404 |  0:00:12s\n",
            "epoch 8  | loss: 0.67329 | eval_logloss: 7.23312 |  0:00:13s\n",
            "\n",
            "Early stopping occurred at epoch 8 with best_epoch = 5 and best_eval_logloss = 6.64266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID81: 167141"
      ],
      "metadata": {
        "id": "5MD4k3V5odhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=167141)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57bcf0c2-afbd-415a-e813-e70b45302a60",
        "id": "zOm2AI-4HmTp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.761   | eval_logloss: 6.77967 |  0:00:13s\n",
            "epoch 1  | loss: 0.41278 | eval_logloss: 4.91764 |  0:00:27s\n",
            "epoch 2  | loss: 0.38843 | eval_logloss: 11.72693|  0:00:43s\n",
            "epoch 3  | loss: 0.34273 | eval_logloss: 4.4863  |  0:00:56s\n",
            "epoch 4  | loss: 0.33398 | eval_logloss: 2.83512 |  0:01:10s\n",
            "epoch 5  | loss: 0.33208 | eval_logloss: 2.75758 |  0:01:23s\n",
            "epoch 6  | loss: 0.31043 | eval_logloss: 2.91404 |  0:01:36s\n",
            "epoch 7  | loss: 0.30053 | eval_logloss: 2.2786  |  0:01:49s\n",
            "epoch 8  | loss: 0.29898 | eval_logloss: 6.11773 |  0:02:03s\n",
            "epoch 9  | loss: 0.27517 | eval_logloss: 1.32208 |  0:02:17s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 1.32208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID82: 167140"
      ],
      "metadata": {
        "id": "lApN4vgIodhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=167140)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c4eac9-37fc-430f-b5dd-7a4faeb57173",
        "id": "K17thYliHmsh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.91516 | eval_logloss: 9.66    |  0:00:09s\n",
            "epoch 1  | loss: 1.24728 | eval_logloss: 8.02218 |  0:00:18s\n",
            "epoch 2  | loss: 1.03928 | eval_logloss: 8.29849 |  0:00:27s\n",
            "epoch 3  | loss: 0.89406 | eval_logloss: 7.43972 |  0:00:37s\n",
            "epoch 4  | loss: 0.72824 | eval_logloss: 8.38782 |  0:00:45s\n",
            "epoch 5  | loss: 0.62032 | eval_logloss: 4.78904 |  0:00:55s\n",
            "epoch 6  | loss: 0.55244 | eval_logloss: 2.50983 |  0:01:04s\n",
            "epoch 7  | loss: 0.46628 | eval_logloss: 3.89368 |  0:01:13s\n",
            "epoch 8  | loss: 0.35348 | eval_logloss: 2.46433 |  0:01:22s\n",
            "epoch 9  | loss: 0.30849 | eval_logloss: 2.10161 |  0:01:31s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_eval_logloss = 2.10161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID83: 167125"
      ],
      "metadata": {
        "id": "fsg-GdlAodhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, split, and remove classes\n",
        "X, y, cat_dims = load_openml_data(task_id=167125)  # Example task ID from OpenML\n",
        "args = Args(X, cat_dims)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "seed = 42\n",
        "params = get_random_parameters(TabNetModel, seed)\n",
        "\n",
        "# Initialize and train the model\n",
        "tabnet_model = TabNetModel(params, args)\n",
        "train_loss, val_loss = tabnet_model.fit(X_train.values, y_train, X_val.values, y_val)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = tabnet_model.predict(X_val.values)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted classes\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22fce31-d254-4b28-dc7c-29a4181d0496",
        "id": "KR1ssUfZHnLU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 1.42006 | eval_logloss: 6.82898 |  0:00:19s\n",
            "epoch 1  | loss: 0.63548 | eval_logloss: 2.79478 |  0:00:41s\n",
            "epoch 2  | loss: 0.37057 | eval_logloss: 2.72187 |  0:01:02s\n",
            "epoch 3  | loss: 0.30258 | eval_logloss: 1.99227 |  0:01:23s\n",
            "epoch 4  | loss: 0.20639 | eval_logloss: 2.03623 |  0:01:44s\n",
            "epoch 5  | loss: 0.1757  | eval_logloss: 1.50746 |  0:02:05s\n",
            "epoch 6  | loss: 0.1877  | eval_logloss: 2.17526 |  0:02:25s\n",
            "epoch 7  | loss: 0.16209 | eval_logloss: 0.72361 |  0:02:45s\n",
            "epoch 8  | loss: 0.14063 | eval_logloss: 1.62026 |  0:03:06s\n",
            "epoch 9  | loss: 0.12295 | eval_logloss: 2.25064 |  0:03:28s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_eval_logloss = 0.72361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID84: 167124"
      ],
      "metadata": {
        "id": "AYvLTXMLodhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID85: 167121"
      ],
      "metadata": {
        "id": "Wi8EmADDodhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAINT"
      ],
      "metadata": {
        "id": "o56SgvM60xaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Tabular Dataset class to handle data\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, features, targets, categorical_indices):\n",
        "        self.features = torch.tensor(features.values, dtype=torch.float32)  # Convert to numpy array\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)  # Convert to long for multiclass classification\n",
        "        self.categorical_indices = categorical_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "# Load the data from OpenML\n",
        "def load_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    cat_columns = [col for col in X.columns if X[col].dtype.name == 'category']\n",
        "    num_columns = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "    return X, y, cat_columns, num_columns\n",
        "\n",
        "def preprocess_data(X, cat_columns):\n",
        "    # Handle missing values in categorical columns\n",
        "    for col in cat_columns:\n",
        "        X[col] = X[col].astype('category')  # Convert the column to 'category' type if not already\n",
        "        X[col] = X[col].cat.add_categories('Unknown')  # Add 'Unknown' as a new category\n",
        "        X[col] = X[col].fillna('Unknown')  # Replace NaN with 'Unknown'\n",
        "\n",
        "    # Apply label encoding for categorical columns\n",
        "    label_encoders = {}\n",
        "    for col in cat_columns:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))  # Ensure string type before label encoding\n",
        "        label_encoders[col] = le  # Store the encoder for future use\n",
        "\n",
        "    # Convert all columns to float for PyTorch compatibility\n",
        "    for col in X.columns:\n",
        "        X[col] = X[col].astype(float)\n",
        "\n",
        "    return X, label_encoders\n",
        "\n",
        "\n",
        "# Define the SAINT model\n",
        "class SAINT(nn.Module):\n",
        "    def __init__(self, input_dim, params):\n",
        "        super(SAINT, self).__init__()\n",
        "        self.embedding_dim = params['dim']\n",
        "        self.embedding = nn.Linear(input_dim, self.embedding_dim)\n",
        "        self.fc = nn.Linear(self.embedding_dim, len(np.unique(params['target'])))  # Output for each class\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Train the model\n",
        "def train_model(X_train, y_train, X_val, y_val, params):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_dataset = TabularDataset(X_train, y_train, params['cat_idx'])\n",
        "    val_dataset = TabularDataset(X_val, y_val, params['cat_idx'])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=params['val_batch_size'], shuffle=False)\n",
        "\n",
        "    model = SAINT(input_dim=X_train.shape[1], params=params).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()  # For multiclass classification\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=params['lr'])\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(params['epochs']):\n",
        "        model.train()\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for x_batch, y_batch in val_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=1)  # Get class predictions for multiclass\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(y_batch.cpu().numpy())\n",
        "\n",
        "        # Flatten the lists to compute accuracy\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        val_accuracy = np.mean(all_preds == all_labels)  # Compute accuracy\n",
        "\n",
        "    return model, val_accuracy\n",
        "\n",
        "# Generate random hyperparameters\n",
        "def get_random_parameters(seed):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    params = {\n",
        "        \"dim\": rs.choice([32, 64, 128, 256]),\n",
        "        \"depth\": rs.choice([1, 2, 3, 6, 12]),\n",
        "        \"heads\": rs.choice([2, 4, 8]),\n",
        "        \"dropout\": rs.choice([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]),\n",
        "        \"epochs\": 100,\n",
        "        \"batch_size\": 64,\n",
        "        \"val_batch_size\": 32,\n",
        "        \"lr\": 0.00003,\n",
        "        \"cat_idx\": [],\n",
        "        \"num_idx\": [],\n",
        "    }\n",
        "    return params"
      ],
      "metadata": {
        "id": "9YgEn2WGiU2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID1: 14965"
      ],
      "metadata": {
        "id": "dmPA2RGAosKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 14965\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GZ5SUXAHL-K",
        "outputId": "62cebc92-c0b4-4563-be57-37c33bf380f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID2: 9977"
      ],
      "metadata": {
        "id": "JrhLQUd4osKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9977\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuDqRIBF1umB",
        "outputId": "925f18af-cf63-439c-8b0f-b6fb0d377400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Ww_vDQosKP"
      },
      "source": [
        "# Task ID3: 34539"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 34539\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf1WUOGTI3Xd",
        "outputId": "93ce2642-a46a-44c7-9061-268bbe63378d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEmuZe_xosKP"
      },
      "source": [
        "# Task ID4: 146606"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146606\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_KuzZvMI4eZ",
        "outputId": "e3e2a232-76b9-4ff9-c0a9-6510ecc8294c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2opLQx0osKQ"
      },
      "source": [
        "# Task ID5: 7592\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 7592\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_eeSv5IViCx",
        "outputId": "61f10b88-9eaa-4cf4-c7f2-d8dd0fa0b0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YLx3ZNGosKQ"
      },
      "source": [
        "# Task ID6: 146195\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146195\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6eFoyg_I6LT",
        "outputId": "d816fda8-dd5d-4109-c925-badca2345238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhBL32DSosKQ"
      },
      "source": [
        "# Task ID7: 167119\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 167119\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyeN2IZHtcdr",
        "outputId": "e2ae76d1-c3f7-44ba-8a19-43c006c8e53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqk7zn2eosKQ"
      },
      "source": [
        "# Task ID8: 167120\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 167120\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iCSf0exI7i2",
        "outputId": "d1f030ef-d7bd-4422-abd2-fb4529310e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8vUgc4WosKR"
      },
      "source": [
        "# Task ID11: \t168331\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 168331\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psewhcIJI8d2",
        "outputId": "207ad73c-c549-453a-9b2a-bbedafe0396c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8uNDUsxosKR"
      },
      "source": [
        "# Task ID12: \t168330\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 168330\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fub25hu5I9Xq",
        "outputId": "7548c335-cfb7-4103-b2ee-eb85fa6c0a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OoyHMMEosKR"
      },
      "source": [
        "# Task ID13: \t168335\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 168335\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TilbghI2I-RH",
        "outputId": "219464df-8ed3-4e4f-e036-767aded31c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSOxuuHPosKR"
      },
      "source": [
        "\n",
        "# Task ID16: \t146212\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146212\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWQ3L4lKI_UU",
        "outputId": "78433681-56b1-4759-d355-df4061c64145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJjHRP4qosKR"
      },
      "source": [
        "# Task ID19: \t168868\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 168868\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaxQNIFAJAOW",
        "outputId": "bf8c62c1-621c-4a2c-b5a9-bfd6a7aa5ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID20: 31"
      ],
      "metadata": {
        "id": "VXIOTWnBosKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 31\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXCuUIo4LJym",
        "outputId": "545d4b05-0058-4578-c73c-83d8a7fa9d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID21: 10101"
      ],
      "metadata": {
        "id": "PVZItPV4osKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 10101\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbbaIsSnLMD_",
        "outputId": "aa646acd-7e6e-4a63-8759-b496dc94c519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID22: \t3913"
      ],
      "metadata": {
        "id": "QNzfnhTVosKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3913\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzuyjb3qLM6G",
        "outputId": "9451d0ba-2acc-473e-b07a-d2ad71a4fd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID23: 3"
      ],
      "metadata": {
        "id": "pSugay3losKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0qefzuCLNt0",
        "outputId": "4b0de2aa-24f3-4e34-bd82-f3b315a5469c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID24: 3917"
      ],
      "metadata": {
        "id": "vKk62I4AosKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3917\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19oM3bQdLOXT",
        "outputId": "1fad75cc-8169-4a17-9768-9d292def3822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID25: 9957"
      ],
      "metadata": {
        "id": "fiD5zORKosKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9957\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_M971SBLPGx",
        "outputId": "61fb60e3-904e-40e5-a1d9-079b0e22fec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID26: 9946"
      ],
      "metadata": {
        "id": "tJzLQGmxosKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9946\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8WjMaf_LP7j",
        "outputId": "bf0b003b-2e30-49cd-e7aa-517caca8b4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID27: 3918"
      ],
      "metadata": {
        "id": "DZUWRSYgosKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3918\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd264e6-94b8-45e1-be9d-b2dd8a54a27e",
        "id": "fZ8CLopoIqw7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID28: 3903"
      ],
      "metadata": {
        "id": "OoLhAvc8osKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3903\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a2a8c3-2a33-49a8-d355-1db7a5d4cb74",
        "id": "GnYqmjQxIrLV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID29: 37"
      ],
      "metadata": {
        "id": "bct2qH7tosKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 37\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b5b9e95-ebbf-4a69-85e5-f11f89909543",
        "id": "jSWLMExqIrhg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID30: 9971"
      ],
      "metadata": {
        "id": "3ASvte7WosKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9971\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c9a23c-bb09-405c-f7e0-43de71722823",
        "id": "gzEtgwqaIr2C"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID31: 9952"
      ],
      "metadata": {
        "id": "zEgUcUuQosKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9952\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7478b59-9a80-4c96-ec8a-86d62f8c827d",
        "id": "yrUBIMq1IsMt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID32: 3902"
      ],
      "metadata": {
        "id": "VO61Ws8HosKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3902\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70875b94-2396-4b83-c811-9888e0b233eb",
        "id": "s04H4iF1Isho"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID33: 49"
      ],
      "metadata": {
        "id": "KH5Su5ijosKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 49\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4b03c5-e51a-4091-f462-c9f3804120fe",
        "id": "w8nyCks6Is9w"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID34: 43"
      ],
      "metadata": {
        "id": "gA2WLDWLosKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 43\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b5af4f0-2a13-4930-f8e4-f02bef40ea41",
        "id": "-lm51BB9Itx4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID35: 9978"
      ],
      "metadata": {
        "id": "KIQnhY_2osKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9978\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d73868-65f4-4794-f70a-30df8d01c0d7",
        "id": "OmRYuZTTIuIH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID36: 10093"
      ],
      "metadata": {
        "id": "fZyphVWFosKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 10093\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cf5db4-d04a-4243-f81a-304fc2916483",
        "id": "KbLsMz8zIugU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID37: 219"
      ],
      "metadata": {
        "id": "FeP3l1IaosKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 219\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da1b8fc-208c-4801-c614-5e3ad9af8e0e",
        "id": "tnIYsbavIu21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID38: 9976"
      ],
      "metadata": {
        "id": "jM2pbYRMosKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9976\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3656d38-b3e1-4492-f188-0e9dd1ad06e1",
        "id": "wjeVYYPqIvMm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID39: 6"
      ],
      "metadata": {
        "id": "ql54Ol8xosKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 6\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564e47d4-25ce-4ed9-82f6-6020b9c758be",
        "id": "nOhkxDPDIvhu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID40: 53"
      ],
      "metadata": {
        "id": "1Q7CpVoTosKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 53\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e8adab-3017-4b8c-98ad-d3c93820f28c",
        "id": "Yc6UncpvIv2n"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID41: 11"
      ],
      "metadata": {
        "id": "80yBIp7NosKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 11\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56359252-a457-4fe4-aef6-04081a19d17d",
        "id": "NY10ojLiIwM6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID42: 15"
      ],
      "metadata": {
        "id": "jMZIevWgosKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 15\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc365d94-afb6-4d4a-e235-7175ad25f982",
        "id": "tKjqXOs1Iwpe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID43: 16"
      ],
      "metadata": {
        "id": "pTG_OJXHosKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 16\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b07990df-170a-47e7-d052-d43efb04b308",
        "id": "M-MiOAQ_IxUc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID44: 14"
      ],
      "metadata": {
        "id": "TYkDJ9ImosKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 14\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3905d3-a7f9-4134-ad51-0a013a921597",
        "id": "Dc057xjCIxsm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID45: 32"
      ],
      "metadata": {
        "id": "y7OJS7U9osKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 32\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e46b312-8c64-4bdf-de13-d1621d436a8a",
        "id": "Uwm-rK8wIyE1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID46: 3549"
      ],
      "metadata": {
        "id": "evEUEbt7osKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3549\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b818aa40-ba21-4fad-e632-5c86601abb50",
        "id": "ONYjhgUvIyaV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID47: 12"
      ],
      "metadata": {
        "id": "tda1kl7KosKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 12\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58674ef4-e2ff-4d38-fac7-2b1ec4adccfd",
        "id": "IRDiQ2ZNIyxT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID48: 9981"
      ],
      "metadata": {
        "id": "EZets-gAosKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9981\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab508d5f-c997-4c6e-d39d-74cb97b8d4b3",
        "id": "7eB8d2yYIzJV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID49: 18"
      ],
      "metadata": {
        "id": "uixaLT7qosKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 18\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827bf604-f770-4802-beb5-96b7621ebf84",
        "id": "GM1FBEZ0IzfG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID50: 28"
      ],
      "metadata": {
        "id": "IpZ53NH7osKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 28\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6f889f-aed6-418b-cb5c-6b1d9a7b533d",
        "id": "18ERbIGYIz1W"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID51: 2074"
      ],
      "metadata": {
        "id": "PUIA5-tQosKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 2074\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d9953c-3f43-4715-abb8-7d78cd760ad4",
        "id": "NPVlQ_6jI0N-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID52: 29"
      ],
      "metadata": {
        "id": "RleJyfp6osKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 29\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868c1a84-6ec7-40ba-af49-98b7f9b47e29",
        "id": "bL7b62fDI01q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID53: 45"
      ],
      "metadata": {
        "id": "GMhCiQOgosKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 45\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f012845-2d5f-48df-ee62-4e860a014c73",
        "id": "v5AxNv7HI1NB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID54: 125922"
      ],
      "metadata": {
        "id": "H_dz6_dHosKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 125922\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f182a54e-3649-41da-d56d-2bab23ef7bbf",
        "id": "weCKM5CNI1jo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID55: 9960"
      ],
      "metadata": {
        "id": "je68gm1yosKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9960\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8593a01-f99f-4ddd-ee75-ea45c456d5f5",
        "id": "rTd7UP4fI155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID56: 9964"
      ],
      "metadata": {
        "id": "PD90LLtYosKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9964\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ab39d9-524e-4a90-d9e0-007633af9bde",
        "id": "7eEK707GI2QA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID57: 22"
      ],
      "metadata": {
        "id": "lsDGsz27osKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 22\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c40f678-7062-45ed-fe6e-f78bb99fa9e5",
        "id": "w8LxWEKII2m9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID58: 2079"
      ],
      "metadata": {
        "id": "bF54EbrkosKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 2079\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6156ef-b613-4301-d174-9a326478f6b2",
        "id": "3MDMgMPII3UR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.1486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID59: 14969"
      ],
      "metadata": {
        "id": "lHHx9_TzosKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 14969\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422d064d-33fb-4412-c2f6-2a36915da69c",
        "id": "mUodXnRYI3pL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID60: 3560"
      ],
      "metadata": {
        "id": "HaXbCdgZosKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3560\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da05fec-5e5f-4833-8487-926d6cc76fef",
        "id": "DQXloovkI3-u"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID61: 14952"
      ],
      "metadata": {
        "id": "-SYkWdw5osKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 14952\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8751b47f-2d1a-484d-96c5-1d2390593a89",
        "id": "5gXzxh3LI4VA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID62: 125920"
      ],
      "metadata": {
        "id": "1jINB0cvosKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 125920\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819f7694-083a-4289-c4ed-41c08cf1a9aa",
        "id": "30V2bz4GI4qI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID63: 23"
      ],
      "metadata": {
        "id": "gWKsCe9bosKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 23\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0f6454-d768-489f-e350-46ec445c73d1",
        "id": "7rlCmYTgI5Ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID64: 3904"
      ],
      "metadata": {
        "id": "9J-nzXiaosKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3904\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469b61e4-7e35-410f-e7a9-a6ccb81bfe8f",
        "id": "uJ02TbtiI5zb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID65: 3022"
      ],
      "metadata": {
        "id": "tpOoVbbTosKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3022\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8328b1f9-7181-4ac7-e6d1-a6512848103b",
        "id": "hRBzU89AI6KF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID66: 9985"
      ],
      "metadata": {
        "id": "38VVVryvosKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9985\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e91387-2b1c-46eb-fc71-59a77f6203f3",
        "id": "aPi45D1JI6fy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID67: 9910"
      ],
      "metadata": {
        "id": "RmXmL3ZRosKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 9910\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e618db-eb25-4293-8df8-3f29b5bffaa1",
        "id": "w4UBHL6qI63F"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID68: 14970"
      ],
      "metadata": {
        "id": "7c8Qgo0MosKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 14970\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d63baa6-3ad9-476d-9ab2-c0e67b35440c",
        "id": "z8wO4ytEI7PU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID69: 3021"
      ],
      "metadata": {
        "id": "MI2tihESosKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3021\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102aeec6-81ac-4022-b3d1-455e6e3ad010",
        "id": "WSHkoty2I7lH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID70: 3481"
      ],
      "metadata": {
        "id": "LO_u_fwXosKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3481\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83770db8-1054-4148-c88e-a22d78a024b2",
        "id": "0ktseXxFI772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID71: 3573"
      ],
      "metadata": {
        "id": "Immo9TdkosKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 3573\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb45d56c-731b-4094-9738-b19f57076079",
        "id": "Er8SOavjI8R1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID72: 146824"
      ],
      "metadata": {
        "id": "P_AxBxYdosKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146824\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0bb1f0b-6dc1-4fe4-f764-8c7b8ef58b7a",
        "id": "GppSCwvLI9Iy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID73: 146820"
      ],
      "metadata": {
        "id": "NgS8-5OZosKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146820\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a62871-faa6-42ab-d71c-b5c84a946b08",
        "id": "lMYM2WJCI9eh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID74: 146822"
      ],
      "metadata": {
        "id": "EJM49Bd4osKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146822\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b451ed63-d68a-4572-9cdb-d57379961541",
        "id": "Pw5jUnaSI91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID75: 146195"
      ],
      "metadata": {
        "id": "3Q8kVtSzosKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146195\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a0ea72-47ed-42f0-f4f9-e928a63936f6",
        "id": "0PQelWQHI-I6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID76: 146800"
      ],
      "metadata": {
        "id": "5iDsXkF1osKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146800\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc00904-c93f-4f60-c9f8-c594732f7029",
        "id": "NWvL9zl3I-dV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.1481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID77: 146817"
      ],
      "metadata": {
        "id": "6bXKuQbposKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146817\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad41cd0-a11b-4fdf-f907-e1a5ef14536f",
        "id": "ZaNHtcpHI-zd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID78: 146819"
      ],
      "metadata": {
        "id": "L5GTZGAposKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146819\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eca54a8-881b-4ffd-884d-a67d2d4abb43",
        "id": "7Y-zf5dvI_GV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID79: 146821"
      ],
      "metadata": {
        "id": "3HrHEIVoosKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 146821\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fa17f5-a280-46a6-af4c-1c1c52e55198",
        "id": "4aHQ49ZyI_mn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID80: 14954"
      ],
      "metadata": {
        "id": "6b3ZUsGSosKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 14954\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5ca63e-0019-43bd-9839-be4862f285bb",
        "id": "mp5w-6QUI_7q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID81: 167141"
      ],
      "metadata": {
        "id": "Uxh1sDlmosKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 167141\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc9c1a2-d333-4586-af6e-b2da53e078d8",
        "id": "VwneNT7wJARA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID82: 167140"
      ],
      "metadata": {
        "id": "QgRGDL4BosKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 167140\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b648d68-a027-432a-df5c-1c178dd5fd67",
        "id": "rYrj0vJDJAlA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID83: 167125"
      ],
      "metadata": {
        "id": "PVwi85x0osKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "task_id = 167125\n",
        "X, y, cat_columns, num_columns = load_data(task_id)\n",
        "X, label_encoders = preprocess_data(X, cat_columns)\n",
        "\n",
        "# Encode the target variable\n",
        "target_encoder = LabelEncoder()\n",
        "y_encoded = target_encoder.fit_transform(y)  # Encode the target labels to integers\n",
        "\n",
        "params = get_random_parameters(seed=42)\n",
        "params['cat_idx'] = [X.columns.get_loc(c) for c in cat_columns]\n",
        "params['num_idx'] = [X.columns.get_loc(c) for c in num_columns if c in X.columns]\n",
        "params['target'] = np.unique(y_encoded)  # Add the unique targets for the output layer size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model, accuracy = train_model(X_train, y_train, X_val, y_val, params)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eecf4f2-1323-44fe-91f3-eed495930ab3",
        "id": "wwzVPTKfJA7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID84: 167124"
      ],
      "metadata": {
        "id": "tWIb_jdMosKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID85: 167121"
      ],
      "metadata": {
        "id": "7kFK2mHNosKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIME"
      ],
      "metadata": {
        "id": "m595ps8LhbC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import openml\n",
        "\n",
        "class VIMESelf(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, input_dim)\n",
        "        self.mask_layer = nn.Linear(input_dim, input_dim)\n",
        "        self.feat_layer = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.input_layer(x))\n",
        "        out_mask = torch.sigmoid(self.mask_layer(x))\n",
        "        out_feat = torch.sigmoid(self.feat_layer(x))\n",
        "        return out_mask, out_feat\n",
        "\n",
        "\n",
        "class VIMESemi(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=100, n_layers=5):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(n_layers - 1)]\n",
        "        )\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.input_layer(x))\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(layer(x))\n",
        "        out = self.output_layer(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class VIME:\n",
        "    def __init__(self, params, args):\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.model_self = VIMESelf(args['num_features']).to(args['device'])\n",
        "        self.model_semi = VIMESemi(args['num_features'], args['num_classes']).to(args['device'])\n",
        "\n",
        "        if args['data_parallel']:\n",
        "            self.model_self = nn.DataParallel(self.model_self)\n",
        "            self.model_semi = nn.DataParallel(self.model_semi)\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        self.fit_self(X)\n",
        "        return self.fit_semi(X, y, X_val, y_val)\n",
        "\n",
        "    def fit_self(self, X):\n",
        "        optimizer = optim.RMSprop(self.model_self.parameters(), lr=0.001)\n",
        "        loss_func_mask = nn.BCELoss()\n",
        "        loss_func_feat = nn.MSELoss()\n",
        "\n",
        "        m_unlab = self.mask_generator(0.3, X)\n",
        "        m_label, x_tilde = self.pretext_generator(m_unlab, X)\n",
        "\n",
        "        x_tilde = torch.tensor(x_tilde).float().to(self.args['device'])\n",
        "        m_label = torch.tensor(m_label).float().to(self.args['device'])\n",
        "        X = torch.tensor(X).float().to(self.args['device'])\n",
        "        train_dataset = TensorDataset(x_tilde, m_label, X)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.args['batch_size'], shuffle=True)\n",
        "\n",
        "        for epoch in range(10):\n",
        "            for batch_X, batch_mask, batch_feat in train_loader:\n",
        "                out_mask, out_feat = self.model_self(batch_X)\n",
        "                loss_mask = loss_func_mask(out_mask, batch_mask)\n",
        "                loss_feat = loss_func_feat(out_feat, batch_feat)\n",
        "                loss = loss_mask + loss_feat * self.params['alpha']\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    def fit_semi(self, X, y, X_val=None, y_val=None):\n",
        "        optimizer = optim.AdamW(self.model_semi.parameters())\n",
        "        X = torch.tensor(X).float().to(self.args['device'])\n",
        "        y = torch.tensor(y).long().to(self.args['device'])\n",
        "        X_val = torch.tensor(X_val).float().to(self.args['device'])\n",
        "        y_val = torch.tensor(y_val).long().to(self.args['device'])\n",
        "\n",
        "        loss_func_supervised = nn.CrossEntropyLoss()\n",
        "\n",
        "        train_dataset = TensorDataset(X, y)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.args['batch_size'], shuffle=True)\n",
        "\n",
        "        for epoch in range(self.args['epochs']):\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                y_hat = self.model_semi(batch_X)\n",
        "\n",
        "                loss = loss_func_supervised(y_hat, batch_y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        with torch.no_grad():\n",
        "            val_preds = self.model_semi(X_val)\n",
        "            val_preds = torch.argmax(val_preds, dim=1)\n",
        "            accuracy = (val_preds == y_val).float().mean().item()\n",
        "            return accuracy\n",
        "\n",
        "    @staticmethod\n",
        "    def mask_generator(p_m, x):\n",
        "        mask = np.random.binomial(1, p_m, x.shape)\n",
        "        return mask\n",
        "\n",
        "    @staticmethod\n",
        "    def pretext_generator(m, x):\n",
        "        no, dim = x.shape\n",
        "        x_bar = np.zeros([no, dim])\n",
        "        for i in range(dim):\n",
        "            idx = np.random.permutation(no)\n",
        "            x_bar[:, i] = x[idx, i]\n",
        "\n",
        "        x_tilde = x * (1 - m) + x_bar * m\n",
        "        m_new = 1 * (x != x_tilde)\n",
        "        return m_new, x_tilde\n",
        "\n",
        "def load_openml_data(task_id):\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Identify categorical columns (exclude numeric columns)\n",
        "    categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    # Apply One-Hot Encoding to categorical columns\n",
        "    X = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Handle non-finite values by filling NaNs with zeros (or use an appropriate strategy for your case)\n",
        "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    X.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure all columns in X are numeric\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    X.fillna(0, inplace=True)\n",
        "\n",
        "    # Check for non-numeric columns\n",
        "    non_numeric_columns = X.select_dtypes(exclude=['number']).columns\n",
        "    if len(non_numeric_columns) > 0:\n",
        "        # Drop non-numeric columns\n",
        "        X = X.drop(columns=non_numeric_columns)\n",
        "\n",
        "    # Convert y to numeric if it's categorical\n",
        "    if y.dtype == 'object' or isinstance(y.dtype, pd.CategoricalDtype):\n",
        "        y = pd.factorize(y)[0]\n",
        "\n",
        "    # Ensure y is a numpy array and has the correct type\n",
        "    y = np.asarray(y).astype(np.int64)\n",
        "\n",
        "    # Final check for non-numeric types in X\n",
        "    if not np.issubdtype(X.values.dtype, np.number):\n",
        "        raise ValueError(\"There are still non-numeric columns in the dataset after conversion.\")\n",
        "\n",
        "    return X.to_numpy(), y\n"
      ],
      "metadata": {
        "id": "OMhytmC5K9WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID1: 14965"
      ],
      "metadata": {
        "id": "ZqMzfjUko09D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 14965  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1qxfn7NhwL5",
        "outputId": "3278b8b9-a10f-4be4-9eb6-b5214ffe336b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID2: 9977"
      ],
      "metadata": {
        "id": "XM-UwCNeo09D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9977  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_6E5gwWhzD-",
        "outputId": "15a69035-3b4c-41a9-b707-49d366a76392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSo6ojkro09D"
      },
      "source": [
        "# Task ID3: 34539"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 34539  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqj0W2aebYme",
        "outputId": "034ba4b2-038e-4410-d6a8-8d260fdd2ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNRKFQklo09D"
      },
      "source": [
        "# Task ID4: 146606"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146606  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwSQRo86h1Q8",
        "outputId": "de3d738a-19c8-421a-c450-bd5b65062e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eMHChYno09D"
      },
      "source": [
        "# Task ID5: 7592\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 7592  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbB2O5Dvh2JU",
        "outputId": "857caeb9-c4a8-4f48-e956-bbd3d7093cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wCkYQ6Wo09F"
      },
      "source": [
        "# Task ID6: 146195\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146195  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWujQFy0h3K_",
        "outputId": "7ea67791-3141-4ef2-c153-466724248c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsFMP1DRo09F"
      },
      "source": [
        "# Task ID7: 167119\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 167119 # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gd7gYOUh4PB",
        "outputId": "1cc90245-ca36-40cc-9ba6-8931039a5932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJDfeTJOo09F"
      },
      "source": [
        "# Task ID8: 167120\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 167120  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiU7hXNBh5xD",
        "outputId": "ef6478ff-27e8-4bd1-e980-65f48ba1706d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebNoiu_-o09F"
      },
      "source": [
        "# Task ID11: \t168331\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 168331  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loXw9_ayh6ag",
        "outputId": "55aed19b-4ec0-4af3-c3c5-73f6f31801cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgLLa2hlo09F"
      },
      "source": [
        "# Task ID12: \t168330\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 168330  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6NjqOC2h8dM",
        "outputId": "e6926468-968e-47f4-ab19-d12afacece87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzYQNlICo09G"
      },
      "source": [
        "# Task ID13: \t168335\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 168335  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKnbPCWPh9Sd",
        "outputId": "60434df2-8908-4944-d681-3545e1b275d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_24zEleuo09G"
      },
      "source": [
        "\n",
        "# Task ID16: \t146212\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146212  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvDPPY18h-EJ",
        "outputId": "6337abfd-4346-4fd3-f87f-47a36c0d7979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6dJRG8Go09G"
      },
      "source": [
        "# Task ID19: \t168868\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 168868  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QAJkFOch-vD",
        "outputId": "819a6eea-eea2-4fdc-d595-35c2f5034508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID20: 31"
      ],
      "metadata": {
        "id": "FhumFv_wo09G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 31  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmvVJzBIKEkC",
        "outputId": "c5bcae94-2344-4dc6-de83-36d481c2d89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID21: 10101"
      ],
      "metadata": {
        "id": "PgvUckGFo09G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 10101  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axx30Y7tKJYS",
        "outputId": "d5275552-d96d-4396-9f41-db51f2353055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID22: \t3913"
      ],
      "metadata": {
        "id": "xbnuJvCuo09G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3913  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpECdz9GKKK1",
        "outputId": "bb254158-e85f-4c8f-b2b1-536d20d03e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID23: 3"
      ],
      "metadata": {
        "id": "rceLgTQAo09H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXYjkCEhKLM6",
        "outputId": "4e192379-02ce-420f-d86c-0d4a0e3ad7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID24: 3917"
      ],
      "metadata": {
        "id": "wB8QaZc1o09H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3917  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SL5HAX9KMBn",
        "outputId": "fb89556d-8495-44b9-998a-dc9b4b1806f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID25: 9957"
      ],
      "metadata": {
        "id": "LPmuKNqro09H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9957  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GHSHiPKKMqg",
        "outputId": "c4cd7f0a-93b2-4820-a7f5-0326de7e3ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID26: 9946"
      ],
      "metadata": {
        "id": "50iiHbZJo09H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9946  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlx9eTsOKNQR",
        "outputId": "f46cb298-b14b-4097-f03b-364bed3d8d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID27: 3918"
      ],
      "metadata": {
        "id": "tQAQXCYBo09I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3918  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd19dc7c-5bd8-431a-e5b5-e4b4b70bc645",
        "id": "S59pXJwNINPT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID28: 3903"
      ],
      "metadata": {
        "id": "wP4XFP3co09I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3903  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ab88e0-1446-4811-f263-f73c592d0704",
        "id": "2UV5-C0LINoq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID29: 37"
      ],
      "metadata": {
        "id": "A_tqa3QXo09J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 37  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f55b49-6296-4fcb-b360-a55016c972c9",
        "id": "J_5PA8gSIOHV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID30: 9971"
      ],
      "metadata": {
        "id": "_MFmIjjKo09J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9971  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9883de-43bf-4719-8deb-75d112797c67",
        "id": "Dq4KgIAVIOmH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID31: 9952"
      ],
      "metadata": {
        "id": "vPdFwG7yo09J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9952  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04af55b4-4d3c-452f-8943-4163020ef23a",
        "id": "ds4K_SqrIPQm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID32: 3902"
      ],
      "metadata": {
        "id": "nXkZq5E9o09J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3902  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f5fd4a-c221-4029-d471-ec49180dd7e9",
        "id": "EZNvnyY0IP56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID33: 49"
      ],
      "metadata": {
        "id": "hN7oa-o6o09K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 49  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe2a2bc-e191-4356-fd32-e9e242ea3b7a",
        "id": "XIlSI3x9IQR_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID34: 43"
      ],
      "metadata": {
        "id": "JqDe_Sg6o09K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 43  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1396833b-48e8-46cd-eac8-6707b42970a0",
        "id": "80JzKBXvIQqd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID35: 9978"
      ],
      "metadata": {
        "id": "eNiFM0yfo09K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9978  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131daca9-eca0-4442-f2c6-45dbeb072514",
        "id": "6lOVsKAxIRAF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID36: 10093"
      ],
      "metadata": {
        "id": "tUmJwqoNo09K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 10093  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa6e6a5-06f9-409d-fd54-89fd090c928f",
        "id": "u6tAHtnFIRWC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID37: 219"
      ],
      "metadata": {
        "id": "E67mlRjio09K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 219  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df513ab-b90e-4ec7-9ec7-7149f8ca0f5a",
        "id": "fLqNJQRhIR-1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID38: 9976"
      ],
      "metadata": {
        "id": "zeHwr2C-o09K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9976  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08dc9df-3e8a-475a-8e42-05bf6138937f",
        "id": "DnKJXlphISXF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID39: 6"
      ],
      "metadata": {
        "id": "C6PA0vi3o09L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 6  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd1f691c-9f98-4e07-b20a-34306ec73024",
        "id": "J8DiKDNZIStZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID40: 53"
      ],
      "metadata": {
        "id": "2ULi26-xo09L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 53  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb77b14-883a-4596-c269-d1b6df6162ed",
        "id": "wv0hdVPxITFK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID41: 11"
      ],
      "metadata": {
        "id": "xOkiyzs3o09L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 11  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1aa2110-261e-4564-8516-d4fd196036d0",
        "id": "zPqB9lzKITr3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID42: 15"
      ],
      "metadata": {
        "id": "r5CUkPK_o09L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 15  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968d64a0-5994-4866-8e76-cc3b71b50e96",
        "id": "qKPfcYxtIUKU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID43: 16"
      ],
      "metadata": {
        "id": "NNXtQYvHo09L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 16  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc4993e2-53a7-4f3b-db05-5b9bba4a2a38",
        "id": "wDAeH9iNIUj7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID44: 14"
      ],
      "metadata": {
        "id": "t_QddLdro09M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 14  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e35193b-2006-474f-c04f-d172510b954c",
        "id": "-0JGwvmyIU78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID45: 32"
      ],
      "metadata": {
        "id": "bLUpT5bOo09M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 32  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e4cb3e-2dfc-473b-9c57-84d17748391a",
        "id": "xav-_yaCIVba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID46: 3549"
      ],
      "metadata": {
        "id": "DZSc4l8Mo09N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id =  3549  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df58673-718b-4582-e6bb-3af4188e6729",
        "id": "Mo1mi1l8IWpY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID47: 12"
      ],
      "metadata": {
        "id": "ZGVIMQb1o09N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 12  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a470d52-8ff3-40b8-8f52-8bb7b4971dd6",
        "id": "kmj_BrCdIXEl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID48: 9981"
      ],
      "metadata": {
        "id": "P95SpUUYo09N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9981  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53b2778-c885-4e8a-ea45-cc0eeaee7f5b",
        "id": "tT_jVhKtIXbI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID49: 18"
      ],
      "metadata": {
        "id": "RZZxZ2fmo09N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 18  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9353f5be-b814-4d96-a265-bc4481b3c5bf",
        "id": "KsjwcFSpIX65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID50: 28"
      ],
      "metadata": {
        "id": "pBHudYBXo09N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 28  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0c5329-0e04-4689-ffdd-7e6a777570fc",
        "id": "G1uF136dIYRb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID51: 2074"
      ],
      "metadata": {
        "id": "-wxEq7z7o09N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 2074  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4844518-5a0f-448e-9256-271d3f53a317",
        "id": "1dvlPtFCIYpQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID52: 29"
      ],
      "metadata": {
        "id": "iZ1CvrNAo09O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 29  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed666cb7-6671-4678-b1e3-a67ca7ec9166",
        "id": "ZXK3ijbzIZAm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID53: 45"
      ],
      "metadata": {
        "id": "iobFBcoqo09O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 45  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863281e5-9339-447e-9922-78a9708e1f4b",
        "id": "u3XSvZ1KIZYU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID54: 125922"
      ],
      "metadata": {
        "id": "fsfuRsSzo09O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 125922  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c08307-ab12-4dd6-9138-a43e8b3935a2",
        "id": "xUBqXIHsIaD3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID55: 9960"
      ],
      "metadata": {
        "id": "4Su86KLeo09O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9960  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b30642-bedf-404e-de4c-cd9fec528e92",
        "id": "8B1YCtcdIaoS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID56: 9964"
      ],
      "metadata": {
        "id": "Kcxw0Cwjo09O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9964  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d32e76f-dcf4-4af0-8bfd-7ac22d5895ad",
        "id": "2VeCEdc9IbNH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID57: 22"
      ],
      "metadata": {
        "id": "UYvXvbyGo09O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 22  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b6c43d-2c3a-473f-d369-adef0e75fba8",
        "id": "lp7lLLnHIcBk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID58: 2079"
      ],
      "metadata": {
        "id": "EraDKroao09O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 2079  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3867fc-e509-40de-d7ec-afe6c566299b",
        "id": "f5GlyavbIcYT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID59: 14969"
      ],
      "metadata": {
        "id": "vtB2cW95o09P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 14969  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7d6659-423d-4e38-eedf-2519962c2e7b",
        "id": "TePjFIAGIc1x"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID60: 3560"
      ],
      "metadata": {
        "id": "Cja7-iwko09P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3560  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6b3ce4-4508-4823-aa58-4266c3e10add",
        "id": "8hzrytzYIdTk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID61: 14952"
      ],
      "metadata": {
        "id": "gGqyeJ_So09Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 14952  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1745386-1191-45c7-bdbe-fd7786d1aca9",
        "id": "n7F7RKBBId0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID62: 125920"
      ],
      "metadata": {
        "id": "ILHHgHJCo09Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 125920  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edf81b2-c525-4460-e821-a708a0b84c21",
        "id": "D4xrohLTIeQR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID63: 23"
      ],
      "metadata": {
        "id": "H8-xL6nao09Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 23  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31741a28-4035-43d0-9a0c-272da34f7042",
        "id": "hU0tyjMWIerv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID64: 3904"
      ],
      "metadata": {
        "id": "nOl9Cz0Go09Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3904  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc3e029d-59ab-4721-ab89-784b8edada99",
        "id": "axI0eGdzIfOA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID65: 3022"
      ],
      "metadata": {
        "id": "mTRD8Al2o09Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3022  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f96baa7-fdd8-48b0-8a26-40ce14bba8f1",
        "id": "mtoKDp6eIfkc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID66: 9985"
      ],
      "metadata": {
        "id": "ukHeEDMRo09Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9985  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb2d5ab-e87a-4fdf-f9c1-ce6706ced13b",
        "id": "7qB3nyovIf8W"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID67: 9910"
      ],
      "metadata": {
        "id": "PGmYjDmLo09R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 9910  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e112a9-1d26-48f2-9515-1b4c7c7426c7",
        "id": "W4OESPOlIg2u"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID68: 14970"
      ],
      "metadata": {
        "id": "7H6aLvwRo09R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 14970  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82445582-6585-4a85-e626-295144e61651",
        "id": "xAzbBrzXIhfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID69: 3021"
      ],
      "metadata": {
        "id": "xfbPID5_o09R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3021  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30076fe-2a74-46a2-ed80-bed9b2a3bb23",
        "id": "Q0NxRmK7Ih2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID70: 3481"
      ],
      "metadata": {
        "id": "Qa0CZnPjo09R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3481  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4d189c-5c71-4199-e497-5db222ee53f3",
        "id": "mv0QdiVPIiMm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID71: 3573"
      ],
      "metadata": {
        "id": "ebXniS0Jo09R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 3573  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "X9EAd0wuIihs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID72: 146824"
      ],
      "metadata": {
        "id": "hLpBpEzPo09R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146824  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0608f591-96e8-4e9b-b29e-72f6e3d609a9",
        "id": "5A6azYO7Ii7J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID73: 146820"
      ],
      "metadata": {
        "id": "aVHfYZTKo09S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146820  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b67a3e9-11ac-4626-ad53-bdb8b962525d",
        "id": "B4Lre7FPIjT5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID74: 146822"
      ],
      "metadata": {
        "id": "Vq6VO6rRo09S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146822  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e29974-80ba-4063-e95e-0254518e2762",
        "id": "YV_SpgNjIjrr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID75: 146195"
      ],
      "metadata": {
        "id": "OOtqjlHso09S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146195  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5e6a41-a537-42de-9f81-38a341151837",
        "id": "8QYFR-5YIkCk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID76: 146800"
      ],
      "metadata": {
        "id": "X612y0iBo09S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146800  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07909361-3640-4047-f3d2-fa127b5a6175",
        "id": "r2BRAoIzIkbD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID77: 146817"
      ],
      "metadata": {
        "id": "vuLFkD-9o09S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146817  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13088de3-c315-47e2-ff76-709dff78d490",
        "id": "BKadBG7vIk0y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID78: 146819"
      ],
      "metadata": {
        "id": "BdQmScDto09S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146819  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78296e25-445c-4022-e3d7-bbe1cf78e3d0",
        "id": "_GmbHKFPIlp-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID79: 146821"
      ],
      "metadata": {
        "id": "YbZF8W2oo09T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 146821 # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc804be1-a024-4418-bdfd-ef1a74c8560b",
        "id": "CgT-j3tAImBx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID80: 14954"
      ],
      "metadata": {
        "id": "eIs-VgDGo09T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 14954  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a34cc2-abf1-4b47-a89d-b70d6642cbdb",
        "id": "8gazkkBgImXh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID81: 167141"
      ],
      "metadata": {
        "id": "V15LXPZGo09T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 167141 # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f73d75-a97c-40d8-ab85-eec671ef40b4",
        "id": "3B-gcquoImtE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID82: 167140"
      ],
      "metadata": {
        "id": "Sm_jJ-DEo09T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 167140  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87726457-7efa-4c91-e937-3da7b844de46",
        "id": "z0SkSujAInDd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID83: 167125"
      ],
      "metadata": {
        "id": "Gq1FLvGDo09T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify OpenML task ID\n",
        "    task_id = 167125  # Replace with your desired task ID\n",
        "    X, y = load_openml_data(task_id)\n",
        "\n",
        "    # Determine number of features and classes from the dataset\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameters and arguments\n",
        "    params = {\n",
        "        \"p_m\": 0.5,\n",
        "        \"alpha\": 3.0,\n",
        "    }\n",
        "    args = {\n",
        "        \"num_features\": num_features,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"data_parallel\": False,\n",
        "    }\n",
        "\n",
        "    # Initialize and train VIME model\n",
        "    model = VIME(params, args)\n",
        "    accuracy = model.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Print only validation accuracy\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2a9bab-ff43-4b3a-b914-5ca5c71a4c12",
        "id": "1uepyztgInc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID84: 167124"
      ],
      "metadata": {
        "id": "0ltzfX3Zo09U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task ID85: 167121"
      ],
      "metadata": {
        "id": "nneo9xU8o09U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn openml pymfe tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-HwWXxNL18R",
        "outputId": "133991a7-e0c0-464a-9cae-c0f251d24932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Collecting openml\n",
            "  Downloading openml-0.15.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting pymfe\n",
            "  Downloading pymfe-0.4.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting liac-arff>=2.4.0 (from openml)\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xmltodict (from openml)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openml) (2.32.3)\n",
            "Collecting minio (from openml)\n",
            "  Downloading minio-7.2.10-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openml) (17.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from openml) (24.1)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from pymfe) (0.5.6)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from pymfe) (0.14.4)\n",
            "Collecting texttable (from pymfe)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting igraph>=0.10.1 (from pymfe)\n",
            "  Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting gower (from pymfe)\n",
            "  Downloading gower-0.1.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from minio->openml) (2024.8.30)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from minio->openml) (2.2.3)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from minio->openml) (23.1.0)\n",
            "Collecting pycryptodome (from minio->openml)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from minio->openml) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openml) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openml) (3.10)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->minio->openml) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio->openml) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio->openml) (2.22)\n",
            "Downloading openml-0.15.0-py3-none-any.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m158.0/158.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymfe-0.4.3-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m156.0/156.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading gower-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Downloading minio-7.2.10-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11716 sha256=0befd38fe722a304338298f2b89778e1c35867b7c2c6f297f77ec8b881a29be9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/2a/9c/3895d9617f8f49a0883ba686326d598e78a1c2f54fe3cae86d\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: texttable, xmltodict, pycryptodome, liac-arff, igraph, gower, pymfe, minio, openml\n",
            "Successfully installed gower-0.1.2 igraph-0.11.8 liac-arff-2.5.0 minio-7.2.10 openml-0.15.0 pycryptodome-3.21.0 pymfe-0.4.3 texttable-1.7.0 xmltodict-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXYwJnzZLnp6",
        "outputId": "dbff2c07-15e7-4849-b33e-6b5a04e9eebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openml:No permission to create OpenML directory at /root/.config/openml! This can result in OpenML-Python not working properly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Featurizing task ID: 14965\n",
            "Processing dataset: bank-marketing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
            "  res = hypotest_fun_out(*samples, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9977\n",
            "Processing dataset: nomao\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 34539\n",
            "Processing dataset: Amazon_employee_access\n",
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146606\n",
            "Processing dataset: higgs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 7592\n",
            "Processing dataset: adult\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_entropy'. Will ignore this method. Error message:\n",
            "TypeError(\"'<' not supported between instances of 'float' and 'str'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146195\n",
            "Processing dataset: connect-4\n",
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 167119\n",
            "Processing dataset: jungle_chess_2pcs_raw_endgame_complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
            "  res = hypotest_fun_out(*samples, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 167120\n",
            "Processing dataset: numerai28.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 168331\n",
            "Processing dataset: volkert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
            "  res = hypotest_fun_out(*samples, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 168330\n",
            "Processing dataset: jannis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 168335\n",
            "Processing dataset: MiniBooNE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146212\n",
            "Processing dataset: shuttle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 168868\n",
            "Processing dataset: APSFailure\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 31\n",
            "Processing dataset: credit-g\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 10101\n",
            "Processing dataset: blood-transfusion-service-center\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3913\n",
            "Processing dataset: kc2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3917\n",
            "Processing dataset: kc1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9957\n",
            "Processing dataset: qsar-biodeg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9946\n",
            "Processing dataset: wdbc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3918\n",
            "Processing dataset: pc1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3903\n",
            "Processing dataset: pc3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 37\n",
            "Processing dataset: diabetes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9971\n",
            "Processing dataset: ilpd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9952\n",
            "Processing dataset: phoneme\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3902\n",
            "Processing dataset: pc4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 49\n",
            "Processing dataset: tic-tac-toe\n",
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 43\n",
            "Processing dataset: spambase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9978\n",
            "Processing dataset: ozone-level-8hr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 10093\n",
            "Processing dataset: banknote-authentication\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 219\n",
            "Processing dataset: electricity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
            "  res = hypotest_fun_out(*samples, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9976\n",
            "Processing dataset: madelon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 6\n",
            "Processing dataset: letter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 53\n",
            "Processing dataset: vehicle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 11\n",
            "Processing dataset: balance-scale\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 15\n",
            "Processing dataset: breast-w\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 16\n",
            "Processing dataset: mfeat-karhunen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 14\n",
            "Processing dataset: mfeat-fourier\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 32\n",
            "Processing dataset: pendigits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3549\n",
            "Processing dataset: analcatdata_authorship\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 12\n",
            "Processing dataset: mfeat-factors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9981\n",
            "Processing dataset: cnae-9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 18\n",
            "Processing dataset: mfeat-morphological\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 28\n",
            "Processing dataset: optdigits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
            "  res = hypotest_fun_out(*samples, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 2074\n",
            "Processing dataset: satimage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 29\n",
            "Processing dataset: credit-approval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_entropy'. Will ignore this method. Error message:\n",
            "TypeError(\"'<' not supported between instances of 'str' and 'float'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 45\n",
            "Processing dataset: splice\n",
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 125922\n",
            "Processing dataset: texture\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9960\n",
            "Processing dataset: wall-robot-navigation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9964\n",
            "Processing dataset: semeion\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 22\n",
            "Processing dataset: mfeat-zernike\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 2079\n",
            "Processing dataset: eucalyptus\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 14969\n",
            "Processing dataset: GesturePhaseSegmentationProcessed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3560\n",
            "Processing dataset: analcatdata_dmft\n",
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 14952\n",
            "Processing dataset: PhishingWebsites\n",
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 125920\n",
            "Processing dataset: dresses-sales\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_entropy'. Will ignore this method. Error message:\n",
            "TypeError(\"'<' not supported between instances of 'float' and 'str'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 23\n",
            "Processing dataset: cmc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3904\n",
            "Processing dataset: jm1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3022\n",
            "Processing dataset: vowel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9985\n",
            "Processing dataset: first-order-theorem-proving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 9910\n",
            "Processing dataset: Bioresponse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 14970\n",
            "Processing dataset: har\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3021\n",
            "Processing dataset: sick\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_entropy'. Will ignore this method. Error message:\n",
            "TypeError(\"'<' not supported between instances of 'float' and 'str'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 3481\n",
            "Processing dataset: isolet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146824\n",
            "Processing dataset: mfeat-pixel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146820\n",
            "Processing dataset: wilt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146822\n",
            "Processing dataset: segment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146800\n",
            "Processing dataset: MiceProtein\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146817\n",
            "Processing dataset: steel-plates-fault\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146819\n",
            "Processing dataset: climate-model-simulation-crashes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 146821\n",
            "Processing dataset: car\n",
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 14954\n",
            "Processing dataset: cylinder-bands\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_entropy'. Will ignore this method. Error message:\n",
            "TypeError(\"'<' not supported between instances of 'float' and 'str'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 167141\n",
            "Processing dataset: churn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 167140\n",
            "Processing dataset: dna\n",
            "Writing. Do not interrupt...\n",
            "Featurizing task ID: 167125\n",
            "Processing dataset: Internet-Advertisements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:1281: UserWarning:  * Something went wrong while precomputing 'precompute_can_cors'. Will ignore this method. Error message:\n",
            "TypeError(\"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\").\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing. Do not interrupt...\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import openml\n",
        "import pandas as pd\n",
        "from pymfe.mfe import MFE\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# Suppress precision and invalid value warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "\n",
        "# Meta-feature extraction settings\n",
        "groups = [\"landmarking\", \"general\", \"statistical\", \"model-based\", \"info-theory\", \"relative\"]\n",
        "summary_funcs = [\"mean\", \"sd\", \"count\", \"histogram\", \"iq_range\", \"kurtosis\", \"max\", \"median\", \"min\", \"quantiles\", \"range\", \"skewness\"]\n",
        "scoring = \"balanced-accuracy\"\n",
        "\n",
        "def featurize_dataset(task_id):\n",
        "    # Load data using OpenML task ID\n",
        "    task = openml.tasks.get_task(task_id)\n",
        "    dataset = task.get_dataset()\n",
        "    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = X.to_numpy()\n",
        "    y = y.to_numpy()\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_cols = list(dataset.get_features_by_type('nominal', [dataset.default_target_attribute]))\n",
        "\n",
        "    # Check if the dataset is classification\n",
        "    if dataset.qualities[\"NumberOfClasses\"] <= 1:\n",
        "        print(\"Unsupported target type. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Processing dataset: {dataset.name}\")\n",
        "    metafeats = []\n",
        "\n",
        "    # Extract metafeatures\n",
        "    mfe = MFE(groups=groups, summary=summary_funcs, random_state=0, score=scoring)\n",
        "    mfe.fit(X, y, cat_cols=categorical_cols, transform_num=False, transform_cat=None)\n",
        "    ft = mfe.extract()\n",
        "\n",
        "    # Consolidate results\n",
        "    fold_metafeats = {\"dataset_name\": dataset.name}\n",
        "    for group in groups:\n",
        "        ft_group = mfe.parse_by_group(group, ft)\n",
        "        fold_metafeats.update(\n",
        "            {f\"f__pymfe.{group}.{name}\": value for name, value in zip(*ft_group)}\n",
        "        )\n",
        "    metafeats.append(fold_metafeats)\n",
        "\n",
        "    return metafeats\n",
        "\n",
        "def featurize_all_datasets(task_ids):\n",
        "    output_file = Path(\"metafeatures.csv\")\n",
        "    if output_file.exists():\n",
        "        computed_features = pd.read_csv(output_file)\n",
        "        computed_features.set_index(\"dataset_name\", inplace=True)\n",
        "    else:\n",
        "        computed_features = None\n",
        "\n",
        "    for task_id in task_ids:\n",
        "        dataset_name = openml.tasks.get_task(task_id).get_dataset().name\n",
        "        if computed_features is not None and dataset_name in computed_features.index:\n",
        "            continue\n",
        "\n",
        "        print(f\"Featurizing task ID: {task_id}\")\n",
        "        dataset_metafeatures = featurize_dataset(task_id)\n",
        "        if dataset_metafeatures is None:\n",
        "            continue\n",
        "\n",
        "        dataset_metafeatures = pd.DataFrame(dataset_metafeatures)\n",
        "        dataset_metafeatures.set_index(\"dataset_name\", inplace=True)\n",
        "\n",
        "        if computed_features is None:\n",
        "            computed_features = dataset_metafeatures\n",
        "            computed_features = computed_features[sorted(computed_features.columns)]\n",
        "        else:\n",
        "            computed_features = pd.concat([dataset_metafeatures, computed_features])\n",
        "\n",
        "        print(\"Writing. Do not interrupt...\")\n",
        "        computed_features.to_csv(output_file)\n",
        "\n",
        "# Specify OpenML task IDs\n",
        "task_ids = [14965, 9977, 34539, 146606, 7592, 146195, 167119, 167120, 168331, 168330, 168335, 146212,\n",
        "            168868, 31, 10101, 3913, 3917, 9957, 9946, 3918,\n",
        "            3903, 37, 9971, 9952, 3902, 49, 43, 9978, 10093, 219, 9976, 6, 53, 11, 15, 16, 14, 32, 3549,\n",
        "            12, 9981, 18, 28, 2074, 29, 45, 125922, 9960, 9964, 22, 2079, 14969, 3560, 14952, 125920, 23,\n",
        "            3904, 3022, 9985, 9910, 14970, 3021, 3481, 146824, 146820, 146822, 146195, 146800, 146817,\n",
        "            146819, 146821, 14954, 167141, 167140, 167125]\n",
        "featurize_all_datasets(task_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list --format=freeze > requirements.txt"
      ],
      "metadata": {
        "id": "L9HozMfOmMID"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}